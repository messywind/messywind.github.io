# 线性回归


## 单变量线性回归
### 模型表示
以最经典的问题为例，假设现在我们要预测房价数据，且目前房价只跟一个因素有关。

实际上就是用一条直线拟合一些数据，等同于高中学过的回归直线方程。我们假设用 $h(x) = \theta_0 &#43; \theta_1x$ 来对数据做回归。

### 损失函数

损失函数就是误差值，假设数据集为 $(x_i,y_i)$，那么误差值就为每个点到直线的距离之和，即 $\sum\limits_{i=0} ^ {n}(h(x_i) - y_i)^2$，为了方便后续的梯度下降，这里距离选择用平方而不是绝对值。

现在把上述损失函数看作一个二元函数 (因为斜率和截距都是不确定变量)，即 $J(\theta_0, \theta_1) = \sum\limits_{i=0} ^ {n}(\theta_0 &#43; \theta_1x_i - y_i)^2$，我们需要将损失函数最小化。这个二元函数就是空间中的一个山谷，我们需要用梯度下降法来找到“谷底”，即损失最小的点。

### 梯度下降

先以一元函数为例，现假设有 $f(x) = x ^ 2$，现在有一个小球在点 $x_0 = (10, f(10))$ 处，该点的导数为 $f\&#39;(10) = 2 \times 10 = 20$，那么我们此时引入一个学习率 $\alpha$ (相当于步长)，用学习率乘以该点的负导数，再加上该点的坐标，相当于该点在函数上移动。假设 $\alpha = 0.2$，则新位置 $x_1 = x_0 - 0.2 \times 20 = 6$，即 $x_1 = (6, f(6))$，继续迭代，$x_2 = x_1 - 0.2 \times (2 \times 6) = 3.6$，如此迭代下去，发现最终新的位置会无限接近于 $0$，即函数的最小值处。

```cpp
#include &lt;bits/stdc&#43;&#43;.h&gt;
#define int long long
using namespace std;
signed main() {
    double x = 10, alpha = 0.2;
    for (int i = 1; i &lt;= 20; i &#43;&#43;) {
        x &#43;= -2 * x * alpha;
        cout &lt;&lt; x &lt;&lt; &#34;\n&#34;;
    }
}
```

用 C&#43;&#43; 代码模拟 20 次的输出结果：

```text
6
3.6
2.16
1.296
0.7776
0.46656
0.279936
0.167962
0.100777
0.0604662
0.0362797
0.0217678
0.0130607
0.00783642
0.00470185
0.00282111
0.00169267
0.0010156
0.00060936
0.000365616
```

为什么会这样？因为导数是描述函数变化率的，越靠近极值点导数越小，每次走的步长就小。

我们可以通过调整 $\alpha$ 来改变这个过程。下面是一些调参的结果：

![1](/image/ML/1.webp)

对于二元函数，同样和上述过程类似，需要把求导改为偏导，即 $\theta_i -\alpha\dfrac{\partial J}{\partial \theta_i}$

![1](/image/ML/2.webp)


## 多变量线性回归

假设现在有很多变量影响了房价因素，这些特征为 $(x_1,x_2,\cdots,x_n)$，此时模型也必须表述为一个超平面 $h(x_1,x_2,\cdots,x_n) = \theta_0 &#43; \sum\limits_{i = 1}^{n}\theta_ix_i$，由于变量太多了我们用矩阵表示 $\theta=\begin{bmatrix}
 \theta_0 &amp; \cdots &amp; \theta_n
\end{bmatrix}, X = \begin{bmatrix} 0 \\\\ x_1 \\\\ \vdots \\\\ x_n \end{bmatrix}$

则 $h(X) = \theta X$，那么损失函数就为 $J(\theta) = \sum\limits_{i = 1} ^ {n}(\theta X_i - y_i)^2$

梯度下降：$\theta_i - \alpha\sum\limits_{i = 1} ^ {n} 2x_i(h(x_i) - y_i) $

### 特征缩放

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0~2000 平方英尺，而房间数量的值则是 0~5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。我们可以将房屋大小除以 2000，将卧室个数除以 5，再画出等高线图就比较合适了（如下图右）。

![1](/image/ML/3.webp)

#### 均值归一化

将数值都缩放在 $[-1,1]$ 之间，公式：$x_i = \dfrac{x_i - \mu}{s}$，其中 $\mu$ 表示平均值，$s$ 表示标准差

### 学习率选取

梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

通常可以考虑尝试些学习率：$\alpha = 0.01, 0.03, 0.1, 0.3, 1, 3,10$

---

> 作者: [凌乱之风](https://github.com/messywind)  
> URL: https://blog.messywind.top/posts/ml%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/  


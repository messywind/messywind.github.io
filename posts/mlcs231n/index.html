<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-cn">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>CS231n - 凌乱之风的博客</title><meta name="author" content="凌乱之风">
<meta name="description" content="学了那么多理论了做一下斯坦福大学的 CS 课程：CS231n，对 CV 有一个基本的认识，同时加强一下实操能力。
课程网址
本地环境部署 由于 2024 版的没有给 jupyter 的压缩包，所以先下载 2024 colab 版本，然后在 colab 上把数据拉下来然后下载到本地，放到 /datasets 下，之后删除掉一开始的 google.colab 驱动相关。
"><meta name="keywords" content='KNN, 神经网络, SVM'>
  <meta itemprop="name" content="CS231n">
  <meta itemprop="description" content="学了那么多理论了做一下斯坦福大学的 CS 课程：CS231n，对 CV 有一个基本的认识，同时加强一下实操能力。
课程网址
本地环境部署 由于 2024 版的没有给 jupyter 的压缩包，所以先下载 2024 colab 版本，然后在 colab 上把数据拉下来然后下载到本地，放到 /datasets 下，之后删除掉一开始的 google.colab 驱动相关。">
  <meta itemprop="datePublished" content="2024-12-03T13:47:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T03:37:34+00:00">
  <meta itemprop="wordCount" content="39211">
  <meta itemprop="keywords" content="KNN,神经网络,SVM"><meta property="og:url" content="https://blog.messywind.top/posts/mlcs231n/">
  <meta property="og:site_name" content="凌乱之风的博客">
  <meta property="og:title" content="CS231n">
  <meta property="og:description" content="学了那么多理论了做一下斯坦福大学的 CS 课程：CS231n，对 CV 有一个基本的认识，同时加强一下实操能力。
课程网址
本地环境部署 由于 2024 版的没有给 jupyter 的压缩包，所以先下载 2024 colab 版本，然后在 colab 上把数据拉下来然后下载到本地，放到 /datasets 下，之后删除掉一开始的 google.colab 驱动相关。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-03T13:47:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T03:37:34+00:00">
    <meta property="article:tag" content="KNN">
    <meta property="article:tag" content="神经网络">
    <meta property="article:tag" content="SVM">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="CS231n">
  <meta name="twitter:description" content="学了那么多理论了做一下斯坦福大学的 CS 课程：CS231n，对 CV 有一个基本的认识，同时加强一下实操能力。
课程网址
本地环境部署 由于 2024 版的没有给 jupyter 的压缩包，所以先下载 2024 colab 版本，然后在 colab 上把数据拉下来然后下载到本地，放到 /datasets 下，之后删除掉一开始的 google.colab 驱动相关。">
<meta name="application-name" content="凌乱之风的博客">
<meta name="apple-mobile-web-app-title" content="凌乱之风的博客"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="images/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="https://blog.messywind.top/posts/mlcs231n/" title="CS231n - 凌乱之风的博客" /><link rel="prev" type="text/html" href="https://blog.messywind.top/posts/mlknn/" title="KNN" /><link rel="next" type="text/html" href="https://blog.messywind.top/posts/misc2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" title="2024 年度总结" /><link rel="alternate" type="text/markdown" href="https://blog.messywind.top/posts/mlcs231n/index.md" title="CS231n - 凌乱之风的博客"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "CS231n",
    "inLanguage": "zh-cn",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/blog.messywind.top\/posts\/mlcs231n\/"
    },"genre": "posts","keywords": "KNN, 神经网络, SVM","wordcount":  39211 ,
    "url": "https:\/\/blog.messywind.top\/posts\/mlcs231n\/","datePublished": "2024-12-03T13:47:00+00:00","dateModified": "2025-05-08T03:37:34+00:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "凌乱之风"
      },"description": ""
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="凌乱之风的博客"><img loading="lazy" src="/images/favicon.ico" alt="凌乱之风的博客" data-title="凌乱之风的博客" width="26" height="26" class="logo" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/><span class="header-title-text">凌乱之风的博客</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 档案</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> 目录</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a class="menu-link" href="/friends/"><i class="fa-solid fa-link fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容……" id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="凌乱之风的博客"><img loading="lazy" src="/images/favicon.ico" alt="凌乱之风的博客" data-title="凌乱之风的博客" width="26" height="26" class="logo" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/><span class="header-title-text">凌乱之风的博客</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容……" id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li class="menu-item"><a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 档案</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> 目录</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item"><a class="menu-link" href="/friends/"><i class="fa-solid fa-link fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="切换主题"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container"><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label="合集"></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span>CS231n</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><a href="https://github.com/messywind" title="作者"target="_blank" rel="external nofollow noopener noreferrer author" class="author"><img loading="lazy" src="/images/favicon.ico" alt="凌乱之风" data-title="凌乱之风" width="20" height="20" class="avatar" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/>&nbsp;凌乱之风</a></span><span class="post-included-in">&nbsp;收录于 <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-category" title="分类 - 机器学习"><i class="fa-regular fa-folder fa-fw" aria-hidden="true"></i> 机器学习</a></span></div><div class="post-meta-line"><span title="发布于 2024-12-03 13:47:00"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden="true"></i><time datetime="2024-12-03">2024-12-03</time></span>&nbsp;<span title="更新于 2025-05-08 03:37:34"><i class="fa-regular fa-calendar-check fa-fw me-1" aria-hidden="true"></i><time datetime="2025-05-08">2025-05-08</time></span>&nbsp;<span title="39211 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>约 39300 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>预计阅读 79 分钟</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#assignment-1">Assignment 1</a>
      <ul>
        <li><a href="#q1-k-nearest-neighbor-classifier">Q1: k-Nearest Neighbor classifier</a>
          <ul>
            <li><a href="#todo-两重循环计算-l2-距离">TODO: 两重循环计算 L2 距离。</a></li>
            <li><a href="#inline-question-1">Inline Question 1</a></li>
            <li><a href="#todo-predict_labels">TODO: predict_labels</a></li>
            <li><a href="#inline-question-2">Inline Question 2</a></li>
            <li><a href="#todo-一个循环求-l2-距离">TODO: 一个循环求 L2 距离：</a></li>
            <li><a href="#todo-不使用循环求-l2-距离">TODO: 不使用循环求 L2 距离</a></li>
            <li><a href="#todo-分成-folds">TODO: 分成 folds</a></li>
            <li><a href="#inline-question-3">Inline Question 3</a></li>
          </ul>
        </li>
        <li><a href="#q2-training-a-support-vector-machine">Q2: Training a Support Vector Machine</a>
          <ul>
            <li><a href="#todo-svm_loss_naive">TODO: svm_loss_naive</a></li>
            <li><a href="#inline-question-1-1">Inline Question 1</a></li>
            <li><a href="#todo-svm_loss_vectorized">TODO: svm_loss_vectorized</a></li>
            <li><a href="#todo-linearclassifier">TODO: LinearClassifier</a></li>
            <li><a href="#todo-predict">TODO: predict</a></li>
            <li><a href="#todo-使用不同学习率和正则化参数">TODO: 使用不同学习率和正则化参数</a></li>
            <li><a href="#inline-question-2-1">Inline question 2</a></li>
          </ul>
        </li>
        <li><a href="#q3-implement-a-softmax-classifier">Q3: Implement a Softmax classifier</a>
          <ul>
            <li><a href="#softmax-损失函数">SoftMax 损失函数</a></li>
            <li><a href="#softmax-梯度推导">SoftMax 梯度推导</a></li>
            <li><a href="#todo-softmax_loss_naive">TODO: softmax_loss_naive</a></li>
            <li><a href="#inline-question-1-2">Inline Question 1</a></li>
            <li><a href="#todo-softmax_loss_vectorized">TODO: softmax_loss_vectorized</a></li>
            <li><a href="#todo-交叉验证">TODO: 交叉验证</a></li>
            <li><a href="#inline-question-2---true-or-false">Inline Question 2 - <em>True or False</em></a></li>
          </ul>
        </li>
        <li><a href="#q4-two-layer-neural-network">Q4: Two-Layer Neural Network</a>
          <ul>
            <li><a href="#todo-affine_forward">TODO: affine_forward</a></li>
            <li><a href="#todo-affine_backward">TODO: affine_backward</a></li>
            <li><a href="#todo-relu_forward">TODO: relu_forward</a></li>
            <li><a href="#todo-relu_backward">TODO: relu_backward</a></li>
            <li><a href="#inline-question-1-3">Inline Question 1</a></li>
            <li><a href="#todo-svm_loss">TODO: svm_loss</a></li>
            <li><a href="#todo-softmax_loss">TODO: softmax_loss</a></li>
            <li><a href="#todo-twolayernet__init__">TODO: TwoLayerNet.<strong>init</strong></a></li>
            <li><a href="#todo-twolayernetloss">TODO: TwoLayerNet.loss</a></li>
            <li><a href="#todo-solver">TODO: Solver</a></li>
            <li><a href="#todo-hyperparameters">TODO: hyperparameters</a></li>
            <li><a href="#inline-question-2-2">Inline Question 2</a></li>
          </ul>
        </li>
        <li><a href="#q5-higher-level-representations-image-features">Q5: Higher Level Representations: Image Features</a>
          <ul>
            <li><a href="#todo-train-svm-on-features">TODO: Train SVM on features</a></li>
            <li><a href="#inline-question-1-4">Inline question 1:</a></li>
            <li><a href="#todo-neural-network-on-image-features">TODO: Neural Network on image features</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#assignment-2">Assignment 2</a>
      <ul>
        <li><a href="#q1-multi-layer-fully-connected-neural-networks">Q1: Multi-Layer Fully Connected Neural Networks</a>
          <ul>
            <li><a href="#todo-fc_net">TODO: fc_net</a>
              <ul>
                <li><a href="#__init__">__init__</a></li>
                <li><a href="#loss">loss</a></li>
              </ul>
            </li>
            <li><a href="#todo-use-a-three-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale">TODO: Use a three-layer Net to overfit 50 training examples by tweaking just the learning rate and initialization scale.</a></li>
            <li><a href="#todo-use-a-five-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale">TODO: Use a five-layer Net to overfit 50 training examples by tweaking just the learning rate and initialization scale.</a></li>
            <li><a href="#inline-question-1-5">Inline Question 1</a></li>
            <li><a href="#todo-sgd_momentum">TODO: sgd_momentum</a></li>
            <li><a href="#todo-rmsprop">TODO: RMSProp</a></li>
            <li><a href="#todo-adam">TODO: Adam</a></li>
            <li><a href="#inline-question-2-3">Inline Question 2</a></li>
            <li><a href="#todo-train-a-good-model">TODO: Train a Good Model!</a></li>
          </ul>
        </li>
        <li><a href="#q2-batch-normalization">Q2: Batch Normalization</a>
          <ul>
            <li><a href="#todo-batchnorm_forward">TODO: batchnorm_forward</a></li>
            <li><a href="#todo-batchnorm_backward">TODO: batchnorm_backward</a></li>
            <li><a href="#todo-layer_utils">TODO: layer_utils</a></li>
            <li><a href="#inline-question-1-6">Inline Question 1:</a></li>
            <li><a href="#inline-question-2-4">Inline Question 2:</a></li>
            <li><a href="#layer-normalization">layer normalization</a></li>
            <li><a href="#todo-layernorm_forward">TODO: layernorm_forward</a></li>
            <li><a href="#todo-layernorm_backward">TODO: layernorm_backward</a></li>
            <li><a href="#inline-question-3-1">Inline Question 3:</a></li>
            <li><a href="#inline-question-4">Inline Question 4:</a></li>
          </ul>
        </li>
        <li><a href="#q3-dropout">Q3: Dropout</a>
          <ul>
            <li><a href="#todo-dropout_forward">TODO: dropout_forward</a></li>
            <li><a href="#todo-dropout_backward">TODO: dropout_backward</a></li>
            <li><a href="#inline-question-1-7">Inline Question 1:</a></li>
            <li><a href="#todo-给-fc_net-加上-dropout">TODO: 给 fc_net 加上 dropout</a></li>
            <li><a href="#inline-question-2-5">Inline Question 2:</a></li>
          </ul>
        </li>
        <li><a href="#q4-convolutional-neural-networks">Q4: Convolutional Neural Networks</a>
          <ul>
            <li><a href="#todo-conv_forward_naive">TODO: conv_forward_naive</a></li>
            <li><a href="#todo-conv_backward_naive">TODO: conv_backward_naive</a></li>
            <li><a href="#todo-max_pool_forward_naive">TODO: max_pool_forward_naive</a></li>
            <li><a href="#tood-max_pool_backward_naive">TOOD: max_pool_backward_naive</a></li>
            <li><a href="#fast-layers">Fast Layers</a></li>
            <li><a href="#todo-three-layer-convolutional-network">TODO: Three-Layer Convolutional Network</a></li>
            <li><a href="#todo-spatial_batchnorm_forward">TODO: spatial_batchnorm_forward</a></li>
            <li><a href="#todo-spatial_batchnorm_backward">TODO: spatial_batchnorm_backward</a></li>
            <li><a href="#todo-spatial_groupnorm_forward">TODO: spatial_groupnorm_forward</a></li>
            <li><a href="#todo-spatial_groupnorm_backward">TODO: spatial_groupnorm_backward</a></li>
          </ul>
        </li>
        <li><a href="#q5-pytorch-on-cifar-10">Q5: PyTorch on CIFAR-10</a>
          <ul>
            <li><a href="#barebones-pytorch-two-layer-network">Barebones PyTorch: Two-Layer Network</a></li>
            <li><a href="#todo-barebones-pytorch-three-layer-convnet">TODO: Barebones PyTorch: Three-Layer ConvNet</a></li>
            <li><a href="#barebones-pytorch-initialization">Barebones PyTorch: Initialization</a></li>
            <li><a href="#todo-barebones-pytorch-training-a-convnet">TODO: BareBones PyTorch: Training a ConvNet</a>
              <ul>
                <li><a href="#barebones-pytorch-训练卷积神经网络">BareBones PyTorch: 训练卷积神经网络</a></li>
              </ul>
            </li>
            <li><a href="#todo-module-api-three-layer-convnet">TODO: Module API: Three-Layer ConvNet</a>
              <ul>
                <li><a href="#模块-api三层卷积网络">模块 API：三层卷积网络</a></li>
              </ul>
            </li>
            <li><a href="#todo-module-api-train-a-three-layer-convnet">TODO: Module API: Train a Three-Layer ConvNet</a></li>
            <li><a href="#todo-sequential-api-three-layer-convnet">TODO: Sequential API: Three-Layer ConvNet</a>
              <ul>
                <li><a href="#顺序api三层卷积神经网络">顺序API：三层卷积神经网络</a></li>
              </ul>
            </li>
            <li><a href="#todo-part-v-cifar-10-open-ended-challenge">TODO: Part V. CIFAR-10 open-ended challenge</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#assignment-3">Assignment 3</a>
      <ul>
        <li><a href="#q1-image-captioning-with-vanilla-rnns">Q1: Image Captioning with Vanilla RNNs</a>
          <ul>
            <li><a href="#todo-rnn_step_forward">TODO: rnn_step_forward</a></li>
            <li><a href="#todo-rnn_step_backward">TODO: rnn_step_backward</a></li>
            <li><a href="#todo-rnn_forward">TODO: rnn_forward</a></li>
            <li><a href="#todo-rnn_backward">TODO: rnn_backward</a></li>
            <li><a href="#todo-word_embedding_forward">TODO: word_embedding_forward</a></li>
            <li><a href="#todo-word_embedding_backward">TODO: word_embedding_backward</a></li>
            <li><a href="#todo-captioningrnnloss-captioningrnnsample">TODO: CaptioningRNN.loss CaptioningRNN.sample</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><p>学了那么多理论了做一下斯坦福大学的 CS 课程：CS231n，对 CV 有一个基本的认识，同时加强一下实操能力。</p>
<p><a href="https://cs231n.github.io/"target="_blank" rel="external nofollow noopener noreferrer">课程网址</a></p>
<h2 id="本地环境部署" class="heading-element"><span>本地环境部署</span>
  <a href="#%e6%9c%ac%e5%9c%b0%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>由于 2024 版的没有给 jupyter 的压缩包，所以先下载 <a href="https://cs231n.github.io/assignments/2024/assignment1_colab.zip"target="_blank" rel="external nofollow noopener noreferrer">2024 colab 版本</a>，然后在 colab 上把数据拉下来然后下载到本地，放到 <code>/datasets</code> 下，之后删除掉一开始的 <code>google.colab</code> 驱动相关。</p>
<p>虚拟环境的话就用 conda 创一个 python3.7 然后使用 <a href="https://cs231n.github.io/assignments/2020/assignment1_jupyter.zip"target="_blank" rel="external nofollow noopener noreferrer">2020 jupyter 版本</a>的 <code>requirements.txt</code> ，如果有漏包情况再说。</p>
<p><a href="https://github.com/messywind/CS231n"target="_blank" rel="external nofollow noopener noreferrer">个人练习 GitHub 地址</a></p>
<h2 id="assignment-1" class="heading-element"><span>Assignment 1</span>
  <a href="#assignment-1" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><h3 id="q1-k-nearest-neighbor-classifier" class="heading-element"><span>Q1: k-Nearest Neighbor classifier</span>
  <a href="#q1-k-nearest-neighbor-classifier" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>数据集是一个很多 32 * 32 * 3 (32 × 32 像素，RGB) 的图片分类。</p>
<p>为了方便处理直接压成一行，$32 \times 32 \times 3 = 3072$ 列。</p>
<p>取前 $5000$ 个作为训练集，前 $500$ 个作为测试集。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_CIFAR10</span><span class="p">(</span><span class="n">cifar10_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># As a sanity check, we print out the size of the training and test data.</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training data shape: &#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training labels shape: &#39;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test data shape: &#39;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test labels shape: &#39;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Subsample the data for more efficient code execution in this exercise</span>
</span></span><span class="line"><span class="cl"><span class="n">num_training</span> <span class="o">=</span> <span class="mi">5000</span>
</span></span><span class="line"><span class="cl"><span class="n">mask</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_training</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_test</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="n">mask</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Reshape the image data into rows</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>导入 KNN 分类器，他已经把类写好了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers</span> <span class="kn">import</span> <span class="n">KNearestNeighbor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a kNN classifier instance. </span>
</span></span><span class="line"><span class="cl"><span class="c1"># Remember that training a kNN classifier is a noop: </span>
</span></span><span class="line"><span class="cl"><span class="c1"># the Classifier simply remembers the data and does no further processing </span>
</span></span><span class="line"><span class="cl"><span class="n">classifier</span> <span class="o">=</span> <span class="n">KNearestNeighbor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接下来要计算距离。需要写一下 compute_distances_two_loops 的 TODO</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Open cs231n/classifiers/k_nearest_neighbor.py and implement</span>
</span></span><span class="line"><span class="cl"><span class="c1"># compute_distances_two_loops.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Test your implementation:</span>
</span></span><span class="line"><span class="cl"><span class="n">dists</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_two_loops</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">dists</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-两重循环计算-l2-距离" class="heading-element"><span>TODO: 两重循环计算 L2 距离。</span>
  <a href="#todo-%e4%b8%a4%e9%87%8d%e5%be%aa%e7%8e%af%e8%ae%a1%e7%ae%97-l2-%e8%b7%9d%e7%a6%bb" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><code>np.sqrt()</code> 开根，<code>np.sum()</code> 求和。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">compute_distances_two_loops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Compute the distance between each test point in X and each training point
</span></span></span><span class="line"><span class="cl"><span class="s2">        in self.X_train using a nested loop over both the training data and the
</span></span></span><span class="line"><span class="cl"><span class="s2">        test data.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - X: A numpy array of shape (num_test, D) containing test data.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
</span></span></span><span class="line"><span class="cl"><span class="s2">          is the Euclidean distance between the ith test point and the jth training
</span></span></span><span class="line"><span class="cl"><span class="s2">          point.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_test</span><span class="p">,</span> <span class="n">num_train</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1">#####################################################################</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># TODO:                                                             #</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Compute the l2 distance between the ith test point and the jth    #</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># training point, and store the result in dists[i, j]. You should   #</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># not use a loop over dimension, nor use np.linalg.norm().          #</span>
</span></span><span class="line"><span class="cl">                <span class="c1">#####################################################################</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">                <span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">dists</span></span></span></code></pre></td></tr></table>
</div>
</div><p>看一下距离的网格图。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># We can visualize the distance matrix: each row is a single test example and</span>
</span></span><span class="line"><span class="cl"><span class="c1"># its distances to training examples</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/1.png" alt="/image/ML/CS231n/1.png" srcset="/image/ML/CS231n/1.png?size=small, /image/ML/CS231n/1.png?size=medium 1.5x, /image/ML/CS231n/1.png?size=large 2x" data-title="/image/ML/CS231n/1.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-1" class="heading-element"><span>Inline Question 1</span>
  <a href="#inline-question-1" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)</p>
<ul>
<li>What in the data is the cause behind the distinctly bright rows?</li>
<li>What causes the columns?</li>
</ul>
<p>${\textit Your Answer:}$ <em>fill this in.</em></p>
<p>注意距离矩阵中的结构化模式，其中一些行或列明显更亮。（注意在默认的颜色方案中，黑色表示低距离，而白色表示高距离。）</p>
<ul>
<li>数据中是什么原因导致了这些明显更亮的行？</li>
<li>是什么导致了这些明显的列？</li>
</ul>
<p>${\textit Your Answer:}$ 行是测试数据，列是训练数据。白色的行是该测试数据远离训练数据，白色的列是该训练数据远离测试数据。</p>
<h4 id="todo-predict_labels" class="heading-element"><span>TODO: predict_labels</span>
  <a href="#todo-predict_labels" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><code>np.argsort()</code> 表示返回排序后的原数组下标。这里 <code>[0 : k]</code> 取前 $k$ 大，然后再映射到 y</p>
<p><code>np.bincount()</code> 表示将输入数据装进桶计数。</p>
<p><code>np.argmax()</code> 表示取最大值的下标。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Given a matrix of distances between test points and training points,
</span></span></span><span class="line"><span class="cl"><span class="s2">        predict a label for each test point.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
</span></span></span><span class="line"><span class="cl"><span class="s2">          gives the distance betwen the ith test point and the jth training point.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - y: A numpy array of shape (num_test,) containing predicted labels for the
</span></span></span><span class="line"><span class="cl"><span class="s2">          test data, where y[i] is the predicted label for the test point X[i].
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_test</span> <span class="o">=</span> <span class="n">dists</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># A list of length k storing the labels of the k nearest neighbors to</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># the ith test point.</span>
</span></span><span class="line"><span class="cl">            <span class="n">closest_y</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># TODO:                                                                 #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Use the distance matrix to find the k nearest neighbors of the ith    #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># testing point, and use self.y_train to find the labels of these       #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># neighbors. Store these labels in closest_y.                           #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Hint: Look up the function numpy.argsort.                             #</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">closest_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span> <span class="p">:</span> <span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">closest_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">closest_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># TODO:                                                                 #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Now that you have found the labels of the k nearest neighbors, you    #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># need to find the most common label in the list closest_y of labels.   #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># label.                                                                #</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">closest_y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">y_pred</span></span></span></code></pre></td></tr></table>
</div>
</div><p>开始 $k = 1$ 的预测。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Now implement the function predict_labels and run the code below:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># We use k = 1 (which is Nearest Neighbor).</span>
</span></span><span class="line"><span class="cl"><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict_labels</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and print the fraction of correctly predicted examples</span>
</span></span><span class="line"><span class="cl"><span class="n">num_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_test</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Got </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1"> correct =&gt; accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_test</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>Got 137 / 500 correct =&gt; accuracy: 0.274000</code></p>
<p>$k = 5$ 的预测。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict_labels</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">num_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_test</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Got </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1"> correct =&gt; accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_test</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>Got 139 / 500 correct =&gt; accuracy: 0.278000</code></p>
<h4 id="inline-question-2" class="heading-element"><span>Inline Question 2</span>
  <a href="#inline-question-2" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>We can also use other distance metrics such as L1 distance.
For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,</p>
<p>the mean $\mu$ across all pixels over all images is $$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$
And the pixel-wise mean $\mu_{ij}$ across all images is
$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$
The general standard deviation $\sigma$ and pixel-wise standard deviation $\sigma_{ij}$ is defined similarly.</p>
<p>Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply. To clarify, both training and test examples are preprocessed in the same way.</p>
<ol>
<li>Subtracting the mean $\mu$ ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu$.)</li>
<li>Subtracting the per pixel mean $\mu_{ij}$  ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}$.)</li>
<li>Subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$.</li>
<li>Subtracting the pixel-wise mean $\mu_{ij}$ and dividing by the pixel-wise standard deviation $\sigma_{ij}$.</li>
<li>Rotating the coordinate axes of the data, which means rotating all the images by the same angle. Empty regions in the image caused by rotation are padded with a same pixel value and no interpolation is performed.</li>
</ol>
<p>${\textit Your Answer:}$</p>
<p>${\textit Your Explanation:}$</p>
<p>我们也可以使用其他距离度量方法，比如 L1 距离。
对于某个图像 $I_k$ 中位置 $(i,j)$ 的像素值 $p_{ij}^{(k)}$，</p>
<p>所有图像中所有像素的均值 $\mu$ 为：
$$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$</p>
<p>所有图像中每个像素位置的均值 $\mu_{ij}$ 为：
$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$</p>
<p>总体标准差 $\sigma$ 和每个像素位置的标准差 $\sigma_{ij}$ 的定义类似。</p>
<p>以下哪种预处理步骤不会改变使用 L1 距离的最近邻分类器的性能？选择所有适用的选项。为明确起见，训练和测试样本都以相同的方式进行预处理。</p>
<ol>
<li>减去均值 $\mu$ ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu$.)</li>
<li>减去每个像素的均值 $\mu_{ij}$  ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}$.)</li>
<li>减去均值 $\mu$ 并除以标准差 $\sigma$。</li>
<li>减去每个像素的均值 $\mu_{ij}$ 并除以每个像素的标准差 $\sigma_{ij}$。</li>
<li>旋转数据的坐标轴，这意味着将所有图像旋转相同的角度。旋转导致的图像空白区域用相同的像素值填充，不进行插值。</li>
</ol>
<p>${\textit Your Answer:}$ 除了 4 都不影响。</p>
<p>${\textit Your Explanation:}$</p>
<h4 id="todo-一个循环求-l2-距离" class="heading-element"><span>TODO: 一个循环求 L2 距离：</span>
  <a href="#todo-%e4%b8%80%e4%b8%aa%e5%be%aa%e7%8e%af%e6%b1%82-l2-%e8%b7%9d%e7%a6%bb" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><code>np.sum(..., axis=1)</code> 表示在第一维求和，例如</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p>得到的结果是 <code>[3 12]</code></p>
<p>这里由于 Numpy 的广播机制，<code>self.X_train</code> 的每一行会减掉 <code>X[i, :]</code></p>
<p>例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p>得到的结果是 <code>[[0 0 0] [1 1 1]]</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_distances_one_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Compute the distance between each test point in X and each training point
</span></span></span><span class="line"><span class="cl"><span class="s2">        in self.X_train using a single loop over the test data.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Input / Output: Same as compute_distances_two_loops
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_test</span><span class="p">,</span> <span class="n">num_train</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># TODO:                                                               #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Compute the l2 distance between the ith test point and all training #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># points, and store the result in dists[i, :].                        #</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Do not use np.linalg.norm().                                        #</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">dists</span></span></span></code></pre></td></tr></table>
</div>
</div><p>正确性检测：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Now lets speed up distance matrix computation by using partial vectorization</span>
</span></span><span class="line"><span class="cl"><span class="c1"># with one loop. Implement the function compute_distances_one_loop and run the</span>
</span></span><span class="line"><span class="cl"><span class="c1"># code below:</span>
</span></span><span class="line"><span class="cl"><span class="n">dists_one</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_one_loop</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># To ensure that our vectorized implementation is correct, we make sure that it</span>
</span></span><span class="line"><span class="cl"><span class="c1"># agrees with the naive implementation. There are many ways to decide whether</span>
</span></span><span class="line"><span class="cl"><span class="c1"># two matrices are similar; one of the simplest is the Frobenius norm. In case</span>
</span></span><span class="line"><span class="cl"><span class="c1"># you haven&#39;t seen it before, the Frobenius norm of two matrices is the square</span>
</span></span><span class="line"><span class="cl"><span class="c1"># root of the squared sum of differences of all elements; in other words, reshape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the matrices into vectors and compute the Euclidean distance between them.</span>
</span></span><span class="line"><span class="cl"><span class="n">difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dists</span> <span class="o">-</span> <span class="n">dists_one</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;One loop difference was: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">difference</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Good! The distance matrices are the same&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uh-oh! The distance matrices are different&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>One loop difference was: 0.000000 Good! The distance matrices are the same</code></p>
<h4 id="todo-不使用循环求-l2-距离" class="heading-element"><span>TODO: 不使用循环求 L2 距离</span>
  <a href="#todo-%e4%b8%8d%e4%bd%bf%e7%94%a8%e5%be%aa%e7%8e%af%e6%b1%82-l2-%e8%b7%9d%e7%a6%bb" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>展开平方。$(x_i - x_j)^ 2 = x_i ^ 2 + x_j ^ 2 - 2x_ix_j$</p>
<p>测试集这里要用 <code>.reshape((num_test, 1))</code>，利用广播机制把 <code>test_squared + train_squared</code> 弄成 <code>(num_test, num_train)</code> 大小，然后注意矩阵乘法的时候要满足中间的矩阵大小是相等的，所以训练集要转置一下，即 <code>self.X_train.T</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">compute_distances_no_loops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Compute the distance between each test point in X and each training point
</span></span></span><span class="line"><span class="cl"><span class="s2">        in self.X_train using no explicit loops.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Input / Output: Same as compute_distances_two_loops
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_test</span><span class="p">,</span> <span class="n">num_train</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO:                                                                 #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute the l2 distance between all test points and all training      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># points without using any explicit loops, and store the result in      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># dists.                                                                #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># You should implement this function using only basic array operations; #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in particular you should not use functions from scipy,                #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># nor use np.linalg.norm().                                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#       and two broadcast sums.                                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">test_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">train_squared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">cross_term</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">test_squared</span> <span class="o">+</span> <span class="n">train_squared</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cross_term</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">dists</span></span></span></code></pre></td></tr></table>
</div>
</div><p>正确性检测：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Now implement the fully vectorized version inside compute_distances_no_loops</span>
</span></span><span class="line"><span class="cl"><span class="c1"># and run the code</span>
</span></span><span class="line"><span class="cl"><span class="n">dists_two</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_no_loops</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># check that the distance matrix agrees with the one we computed before:</span>
</span></span><span class="line"><span class="cl"><span class="n">difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dists</span> <span class="o">-</span> <span class="n">dists_two</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No loop difference was: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">difference</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Good! The distance matrices are the same&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Uh-oh! The distance matrices are different&#39;</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>No loop difference was: 0.000000 Good! The distance matrices are the same</code></p>
<p>时间对比：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Let&#39;s compare how fast the implementations are</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">time_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Call a function f with args and return the time (in seconds) that it took to execute.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">two_loop_time</span> <span class="o">=</span> <span class="n">time_function</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_two_loops</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Two loop version took </span><span class="si">%f</span><span class="s1"> seconds&#39;</span> <span class="o">%</span> <span class="n">two_loop_time</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">one_loop_time</span> <span class="o">=</span> <span class="n">time_function</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_one_loop</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;One loop version took </span><span class="si">%f</span><span class="s1"> seconds&#39;</span> <span class="o">%</span> <span class="n">one_loop_time</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">no_loop_time</span> <span class="o">=</span> <span class="n">time_function</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">compute_distances_no_loops</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No loop version took </span><span class="si">%f</span><span class="s1"> seconds&#39;</span> <span class="o">%</span> <span class="n">no_loop_time</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># You should see significantly faster performance with the fully vectorized implementation!</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># NOTE: depending on what machine you&#39;re using, </span>
</span></span><span class="line"><span class="cl"><span class="c1"># you might not see a speedup when you go from two loops to one loop, </span>
</span></span><span class="line"><span class="cl"><span class="c1"># and might even see a slow-down.</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Two loop version took 27.218139 seconds
</span></span><span class="line"><span class="cl">One loop version took 40.968490 seconds
</span></span><span class="line"><span class="cl">No loop version took 0.350827 seconds</span></span></code></pre></td></tr></table>
</div>
</div><p>交叉验证 (Cross-validation)：</p>
<h4 id="todo-分成-folds" class="heading-element"><span>TODO: 分成 folds</span>
  <a href="#todo-%e5%88%86%e6%88%90-folds" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>大概就是把数据集切分成若干个部分，然后遍历 folds，把当前作为测试集其他作为训练集，再逐个枚举 $k$，得到一个平均值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">k_choices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_train_folds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train_folds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Split up the training data into folds. After splitting, X_train_folds and    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y_train_folds should each be lists of length num_folds, where                #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Hint: Look up the numpy array_split function.                                #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X_train_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">num_folds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_folds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># A dictionary holding the accuracies for different values of k that we find</span>
</span></span><span class="line"><span class="cl"><span class="c1"># when running cross-validation. After running cross-validation,</span>
</span></span><span class="line"><span class="cl"><span class="c1"># k_to_accuracies[k] should be a list of length num_folds giving the different</span>
</span></span><span class="line"><span class="cl"><span class="c1"># accuracy values that we found when using that value of k.</span>
</span></span><span class="line"><span class="cl"><span class="n">k_to_accuracies</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Perform k-fold cross validation to find the best value of k. For each        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># where in each case you use all but one of the folds as training data and the #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># last fold as a validation set. Store the accuracies for all fold and all     #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># values of k in the k_to_accuracies dictionary.                               #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_choices</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_folds</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 使用除当前 fold 以外的所有 fold 作为训练数据</span>
</span></span><span class="line"><span class="cl">        <span class="n">X_train_fold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X_train_folds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_folds</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">fold</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train_fold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_train_folds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_folds</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">fold</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 当前 fold 作为验证集</span>
</span></span><span class="line"><span class="cl">        <span class="n">X_val_fold</span> <span class="o">=</span> <span class="n">X_train_folds</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_val_fold</span> <span class="o">=</span> <span class="n">y_train_folds</span><span class="p">[</span><span class="n">fold</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 训练 k-NN 分类器</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span> <span class="o">=</span> <span class="n">KNearestNeighbor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train_fold</span><span class="p">,</span> <span class="n">y_train_fold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 在验证集上进行预测</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_fold</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">num_loops</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_val_pred</span> <span class="o">==</span> <span class="n">y_val_fold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">num_correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_val_fold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print out the computed accuracies</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">k_to_accuracies</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">accuracy</span> <span class="ow">in</span> <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;k = </span><span class="si">%d</span><span class="s1">, accuracy = </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">k = 1, accuracy = 0.263000
</span></span><span class="line"><span class="cl">k = 1, accuracy = 0.257000
</span></span><span class="line"><span class="cl">k = 1, accuracy = 0.264000
</span></span><span class="line"><span class="cl">k = 1, accuracy = 0.278000
</span></span><span class="line"><span class="cl">k = 1, accuracy = 0.266000
</span></span><span class="line"><span class="cl">k = 3, accuracy = 0.239000
</span></span><span class="line"><span class="cl">k = 3, accuracy = 0.249000
</span></span><span class="line"><span class="cl">k = 3, accuracy = 0.240000
</span></span><span class="line"><span class="cl">k = 3, accuracy = 0.266000
</span></span><span class="line"><span class="cl">k = 3, accuracy = 0.254000
</span></span><span class="line"><span class="cl">k = 5, accuracy = 0.248000
</span></span><span class="line"><span class="cl">k = 5, accuracy = 0.266000
</span></span><span class="line"><span class="cl">k = 5, accuracy = 0.280000
</span></span><span class="line"><span class="cl">k = 5, accuracy = 0.292000
</span></span><span class="line"><span class="cl">k = 5, accuracy = 0.280000
</span></span><span class="line"><span class="cl">k = 8, accuracy = 0.262000
</span></span><span class="line"><span class="cl">k = 8, accuracy = 0.282000
</span></span><span class="line"><span class="cl">k = 8, accuracy = 0.273000
</span></span><span class="line"><span class="cl">k = 8, accuracy = 0.290000
</span></span><span class="line"><span class="cl">k = 8, accuracy = 0.273000
</span></span><span class="line"><span class="cl">k = 10, accuracy = 0.265000
</span></span><span class="line"><span class="cl">k = 10, accuracy = 0.296000
</span></span><span class="line"><span class="cl">k = 10, accuracy = 0.276000
</span></span><span class="line"><span class="cl">k = 10, accuracy = 0.284000
</span></span><span class="line"><span class="cl">k = 10, accuracy = 0.280000
</span></span><span class="line"><span class="cl">k = 12, accuracy = 0.260000
</span></span><span class="line"><span class="cl">k = 12, accuracy = 0.295000
</span></span><span class="line"><span class="cl">k = 12, accuracy = 0.279000
</span></span><span class="line"><span class="cl">k = 12, accuracy = 0.283000
</span></span><span class="line"><span class="cl">k = 12, accuracy = 0.280000
</span></span><span class="line"><span class="cl">k = 15, accuracy = 0.252000
</span></span><span class="line"><span class="cl">k = 15, accuracy = 0.289000
</span></span><span class="line"><span class="cl">k = 15, accuracy = 0.278000
</span></span><span class="line"><span class="cl">k = 15, accuracy = 0.282000
</span></span><span class="line"><span class="cl">k = 15, accuracy = 0.274000
</span></span><span class="line"><span class="cl">k = 20, accuracy = 0.270000
</span></span><span class="line"><span class="cl">k = 20, accuracy = 0.279000
</span></span><span class="line"><span class="cl">k = 20, accuracy = 0.279000
</span></span><span class="line"><span class="cl">k = 20, accuracy = 0.282000
</span></span><span class="line"><span class="cl">k = 20, accuracy = 0.285000
</span></span><span class="line"><span class="cl">k = 50, accuracy = 0.271000
</span></span><span class="line"><span class="cl">k = 50, accuracy = 0.288000
</span></span><span class="line"><span class="cl">k = 50, accuracy = 0.278000
</span></span><span class="line"><span class="cl">k = 50, accuracy = 0.269000
</span></span><span class="line"><span class="cl">k = 50, accuracy = 0.266000
</span></span><span class="line"><span class="cl">k = 100, accuracy = 0.256000
</span></span><span class="line"><span class="cl">k = 100, accuracy = 0.270000
</span></span><span class="line"><span class="cl">k = 100, accuracy = 0.263000
</span></span><span class="line"><span class="cl">k = 100, accuracy = 0.256000
</span></span><span class="line"><span class="cl">k = 100, accuracy = 0.263000</span></span></code></pre></td></tr></table>
</div>
</div><p>画出不同 k 的准确率图。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># plot the raw observations</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_choices</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">accuracies</span> <span class="o">=</span> <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">accuracies</span><span class="p">),</span> <span class="n">accuracies</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># plot the trend line with error bars that correspond to standard deviation</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracies_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">k_to_accuracies</span><span class="o">.</span><span class="n">items</span><span class="p">())])</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracies_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">k_to_accuracies</span><span class="o">.</span><span class="n">items</span><span class="p">())])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">k_choices</span><span class="p">,</span> <span class="n">accuracies_mean</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">accuracies_std</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cross-validation on k&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">k_choices</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">k_choices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-validation accuracy&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/2.png" alt="/image/ML/CS231n/2.png" srcset="/image/ML/CS231n/2.png?size=small, /image/ML/CS231n/2.png?size=medium 1.5x, /image/ML/CS231n/2.png?size=large 2x" data-title="/image/ML/CS231n/2.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>选一个最佳 $k$ 来预测数据，这里选择 $k = 10$，要求准确率应该在 28% 以上。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Based on the cross-validation results above, choose the best value for k,   </span>
</span></span><span class="line"><span class="cl"><span class="c1"># retrain the classifier using all the training data, and test it on the test</span>
</span></span><span class="line"><span class="cl"><span class="c1"># data. You should be able to get above 28% accuracy on the test data.</span>
</span></span><span class="line"><span class="cl"><span class="n">best_k</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">classifier</span> <span class="o">=</span> <span class="n">KNearestNeighbor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">classifier</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">best_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute and display the accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">num_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_test</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Got </span><span class="si">%d</span><span class="s1"> / </span><span class="si">%d</span><span class="s1"> correct =&gt; accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_correct</span><span class="p">,</span> <span class="n">num_test</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>Got 141 / 500 correct =&gt; accuracy: 0.282000</code> 预测准确率为 28.2%</p>
<h4 id="inline-question-3" class="heading-element"><span>Inline Question 3</span>
  <a href="#inline-question-3" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.</p>
<ol>
<li>The decision boundary of the k-NN classifier is linear.</li>
<li>The training error of a 1-NN will always be lower than or equal to that of 5-NN.</li>
<li>The test error of a 1-NN will always be lower than that of a 5-NN.</li>
<li>The time needed to classify a test example with the k-NN classifier grows with the size of the training set.</li>
<li>None of the above.</li>
</ol>
<p>${\textit Your Answer:}$</p>
<p>${\textit Your Explanation:}$</p>
<p><strong>内嵌问题 3</strong></p>
<p>关于 $k$-最近邻（$k$-NN）在分类设置中的以下哪些陈述是正确的，并且适用于所有 $k$？选择所有适用的选项。</p>
<ol>
<li>$k$-NN 分类器的决策边界是线性的。</li>
<li>1-NN 的训练误差总是小于或等于 5-NN 的训练误差。</li>
<li>1-NN 的测试误差总是小于 5-NN 的测试误差。</li>
<li>使用 $k$-NN 分类器对测试样本进行分类所需的时间随着训练集的大小而增加。</li>
<li>以上都不正确。</li>
</ol>
<p>${\textit Your Answer:}$ 4</p>
<p>${\textit Your Explanation:}$ 1 显然不对，2、3 直接看结果，4 确实是因为距离是要遍历数据集。</p>
<h3 id="q2-training-a-support-vector-machine" class="heading-element"><span>Q2: Training a Support Vector Machine</span>
  <a href="#q2-training-a-support-vector-machine" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>还是那个数据集，这次先减去一个图像像素的平均值。然后因为是 SVM，所以 <code>np.ones</code> 加上一个 $1$ 的偏置。</p>
<p><img loading="lazy" src="/image/ML/CS231n/wb.jpeg" alt="1" srcset="/image/ML/CS231n/wb.jpeg?size=small, /image/ML/CS231n/wb.jpeg?size=medium 1.5x, /image/ML/CS231n/wb.jpeg?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Preprocessing: subtract the mean image</span>
</span></span><span class="line"><span class="cl"><span class="c1"># first: compute the image mean based on the training data</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">mean_image</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span> <span class="c1"># print a few of the elements</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mean_image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">))</span> <span class="c1"># visualize the mean image</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># second: subtract the mean image from train and test data</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">-=</span> <span class="n">mean_image</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val</span> <span class="o">-=</span> <span class="n">mean_image</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">-=</span> <span class="n">mean_image</span>
</span></span><span class="line"><span class="cl"><span class="n">X_dev</span> <span class="o">-=</span> <span class="n">mean_image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># third: append the bias dimension of ones (i.e. bias trick) so that our SVM</span>
</span></span><span class="line"><span class="cl"><span class="c1"># only has to worry about optimizing a single weight matrix W.</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_val</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_dev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_dev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_dev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-svm_loss_naive" class="heading-element"><span>TODO: svm_loss_naive</span>
  <a href="#todo-svm_loss_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>计算损失的时候，用 <code>X[i].dot(W)</code> 表示该条数据在 $10$ 个分类下的表现得分 (W 是 3073 * 10 的矩阵)，<code>correct_class_score = scores[y[i]]</code> 表示从得分向量中提取出第 $i$ 个样本的正确类别的得分。根据官网讲义(<a href="https://cs231n.github.io/linear-classify/"target="_blank" rel="external nofollow noopener noreferrer">https://cs231n.github.io/linear-classify/</a>)，定义损失为 $L_i = \sum\limits_{j \ne y_i}\max(0, s_j - s_{y_i} + \Delta)$
为什么这么定义呢？根据讲义</p>
<blockquote>
<p>The Multiclass Support Vector Machine &ldquo;wants&rdquo; the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible.</p>
</blockquote>
<p>多类别支持向量机 &ldquo;希望 &ldquo;正确类别的得分至少比所有其他类别的得分高出 delta 值。 如果任何一个类别的得分在红色区域内（或更高），那么就会有累计损失。 否则，损失为零。 我们的目标是为训练数据中的所有示例找到同时满足这一约束条件的权重，并尽可能降低总损失。</p>
<p><img loading="lazy" src="/image/ML/CS231n/margin.jpg" alt="1" srcset="/image/ML/CS231n/margin.jpg?size=small, /image/ML/CS231n/margin.jpg?size=medium 1.5x, /image/ML/CS231n/margin.jpg?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>除此之外，还要加上一个正则化损失，一般是 L2：</p>
<p>$$
R(W) = \sum_{k}\sum_{l}W_{k, l} ^ 2
$$</p>
<p>然后乘以一个参数(函数传入的 reg)，最后和平均累计损失相加得到最终损失：</p>
<p>$$
L = \frac{1}{N}\sum_i L_i + \lambda R(W)
$$</p>
<p>那么梯度就是对损失函数对 W 求导，第一部分可以在算损失的时候计算出来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Structured SVM loss function, naive implementation (with loops).
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs have dimension D, there are C classes, and we operate on minibatches
</span></span></span><span class="line"><span class="cl"><span class="s2">    of N examples.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - W: A numpy array of shape (D, C) containing weights.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - X: A numpy array of shape (N, D) containing a minibatch of data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means
</span></span></span><span class="line"><span class="cl"><span class="s2">      that X[i] has label c, where 0 &lt;= c &lt; C.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - reg: (float) regularization strength
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - loss as single float
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gradient with respect to weights W; an array of same shape as W
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># initialize the gradient as zero</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># compute the loss and the gradient</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">correct_class_score</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">            <span class="n">margin</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">correct_class_score</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># note delta = 1</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span> <span class="o">+=</span> <span class="n">margin</span>
</span></span><span class="line"><span class="cl">                <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">dW</span><span class="p">[:,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Right now the loss is a sum over all training examples, but we want it</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># to be an average instead so we divide by num_train.</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Add regularization to the loss.</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO:                                                                     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Compute the gradient of the loss function and store it dW.                #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Rather that first computing the loss and then computing the derivative,   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># it may be simpler to compute the derivative at the same time that the     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># loss is being computed. As a result you may need to modify some of the    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># code above to compute the gradient.                                       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">/=</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span></span></span></code></pre></td></tr></table>
</div>
</div><p>检验一下梯度是否对</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Once you&#39;ve implemented the gradient, recompute it with the code below</span>
</span></span><span class="line"><span class="cl"><span class="c1"># and gradient check it with the function we provided for you</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute the loss and its gradient at W.</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Numerically compute the gradient along several randomly chosen dimensions, and</span>
</span></span><span class="line"><span class="cl"><span class="c1"># compare them with your analytically computed gradient. The numbers should match</span>
</span></span><span class="line"><span class="cl"><span class="c1"># almost exactly along all dimensions.</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.gradient_check</span> <span class="kn">import</span> <span class="n">grad_check_sparse</span>
</span></span><span class="line"><span class="cl"><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">grad_numerical</span> <span class="o">=</span> <span class="n">grad_check_sparse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># do the gradient check once again with regularization turned on</span>
</span></span><span class="line"><span class="cl"><span class="c1"># you didn&#39;t forget the regularization gradient did you?</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">5e1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">5e1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">grad_numerical</span> <span class="o">=</span> <span class="n">grad_check_sparse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">numerical: 10.450954 analytic: 10.450954, relative error: 2.255776e-11
</span></span><span class="line"><span class="cl">numerical: 7.036361 analytic: 7.125610, relative error: 6.301992e-03
</span></span><span class="line"><span class="cl">numerical: -33.423497 analytic: -33.423497, relative error: 9.550081e-12
</span></span><span class="line"><span class="cl">numerical: 22.845277 analytic: 22.830022, relative error: 3.340013e-04
</span></span><span class="line"><span class="cl">numerical: 11.480153 analytic: 11.564991, relative error: 3.681392e-03
</span></span><span class="line"><span class="cl">numerical: -26.401444 analytic: -26.401444, relative error: 1.537030e-11
</span></span><span class="line"><span class="cl">numerical: 8.954193 analytic: 8.954193, relative error: 2.288146e-11
</span></span><span class="line"><span class="cl">numerical: -3.249659 analytic: -3.249659, relative error: 7.932744e-12
</span></span><span class="line"><span class="cl">numerical: 9.067911 analytic: 9.075238, relative error: 4.038672e-04
</span></span><span class="line"><span class="cl">numerical: -12.493792 analytic: -12.493792, relative error: 1.033714e-11
</span></span><span class="line"><span class="cl">numerical: -14.040295 analytic: -14.040295, relative error: 2.451599e-11
</span></span><span class="line"><span class="cl">numerical: -17.850408 analytic: -17.850408, relative error: 4.971397e-12
</span></span><span class="line"><span class="cl">numerical: 4.775439 analytic: 4.811624, relative error: 3.774404e-03
</span></span><span class="line"><span class="cl">numerical: 1.002399 analytic: 1.002399, relative error: 1.362218e-10
</span></span><span class="line"><span class="cl">numerical: -19.384504 analytic: -19.382918, relative error: 4.091049e-05
</span></span><span class="line"><span class="cl">numerical: 23.624824 analytic: 23.624824, relative error: 1.588455e-11
</span></span><span class="line"><span class="cl">numerical: -26.578911 analytic: -26.578911, relative error: 3.778756e-12
</span></span><span class="line"><span class="cl">numerical: -9.700067 analytic: -9.700067, relative error: 3.511119e-12
</span></span><span class="line"><span class="cl">numerical: 2.004068 analytic: 2.004068, relative error: 1.303094e-11
</span></span><span class="line"><span class="cl">numerical: -12.635156 analytic: -12.635156, relative error: 1.036973e-11</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到相对误差都比较小。</p>
<h4 id="inline-question-1-1" class="heading-element"><span>Inline Question 1</span>
  <a href="#inline-question-1-1" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? How would change the margin affect of the frequency of this happening? <em>Hint: the SVM loss function is not strictly speaking differentiable</em></p>
<p>${\textit Your Answer:}$ <em>fill this in.</em></p>
<p>有时候，梯度检查中的某个维度可能不会完全匹配。这种差异可能是由什么引起的？这是否是一个值得担心的问题？在一维中，梯度检查可能失败的一个简单例子是什么？改变边距会如何影响这种情况发生的频率？<em>提示：SVM损失函数严格来说并不是可微的</em></p>
<p>${\textit Your Answer:}$ 在梯度检查中，某个维度不完全匹配的差异可能是由于数值计算的精度限制或损失函数的不可微性引起的。SVM损失函数在某些点上是不可微的，例如在边界条件下（即损失函数的“铰链”部分），这可能导致梯度检查不精确。</p>
<p>这种差异通常不是一个严重的问题，因为数值梯度计算本身就有一定的误差。一个简单的例子是在一维中，考虑一个绝对值函数 $( f(x) = |x| )$，在 $( x = 0 )$ 处，梯度是不可定义的，这可能导致梯度检查失败。</p>
<p>改变边距（margin）可能会影响这种情况发生的频率。较大的边距可能会减少不可微点的数量，从而减少梯度检查失败的可能性。然而，边距的改变也会影响模型的性能，因此需要在准确性和稳定性之间进行权衡。</p>
<h4 id="todo-svm_loss_vectorized" class="heading-element"><span>TODO: svm_loss_vectorized</span>
  <a href="#todo-svm_loss_vectorized" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>损失计算：</p>
<p><code>scores = X.dot(W)</code>，得到一个 $N \times 10$ 的每个样本每个分类评分数组。</p>
<p><code>correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1)</code> 将每个样本的正确分类评分拿出来，并转成 $N \times 1$ 的列向量方便广播。</p>
<p><code>margins = np.maximum(0, scores - correct_class_scores + 1)</code> <code>margins[np.arange(num_train), y] = 0</code> 将每个分数减去正确分数，然后把正确分数的那一列变成 $0$</p>
<p>梯度计算：</p>
<p><code>binary = margins &gt; 0</code> <code>binary = binary.astype(float)</code> 得到一个每个元素是否大于 $0$ 的矩阵并转浮点数。</p>
<p><code>row_sum = np.sum(binary, axis=1)</code> ，每一行有多少元素大于 $0$</p>
<p><code>binary[np.arange(num_train), y] = -row_sum</code>，将每个正确位置的地方置为负的 <code>row_sum</code>，因为大于零的都会产生负贡献。</p>
<p><code>dW = X.T.dot(binary)</code>，相当于给每个特征都做 naive 版本的这个操作：<code>dW[:, j] += X[i]</code> <code>dW[:, y[i]] -= X[i]</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Structured SVM loss function, vectorized implementation.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs and outputs are the same as svm_loss_naive.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># initialize the gradient as zero</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO:                                                                     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Implement a vectorized version of the structured SVM loss, storing the    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># result in loss.                                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct_class_scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">margins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">correct_class_scores</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">margins</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">margins</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO:                                                                     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Implement a vectorized version of the gradient for the structured SVM     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># loss, storing the result in dW.                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># to reuse some of the intermediate values that you used to compute the     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># loss.                                                                     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">binary</span> <span class="o">=</span> <span class="n">margins</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">binary</span> <span class="o">=</span> <span class="n">binary</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">row_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">binary</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">binary</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">row_sum</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">binary</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对比一下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Next implement the function svm_loss_vectorized; for now only compute the loss;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># we will implement the gradient in a moment.</span>
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_naive</span><span class="p">,</span> <span class="n">grad_naive</span> <span class="o">=</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Naive loss: </span><span class="si">%e</span><span class="s1"> computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_naive</span><span class="p">,</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers.linear_svm</span> <span class="kn">import</span> <span class="n">svm_loss_vectorized</span>
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_vectorized</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vectorized loss: </span><span class="si">%e</span><span class="s1"> computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_vectorized</span><span class="p">,</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The losses should match but your vectorized implementation should be much faster.</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;difference: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_naive</span> <span class="o">-</span> <span class="n">loss_vectorized</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Naive loss: 9.399452e+00 computed in 0.112582s
</span></span><span class="line"><span class="cl">Vectorized loss: 9.399452e+00 computed in 0.008816s
</span></span><span class="line"><span class="cl">difference: -0.000000</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Complete the implementation of svm_loss_vectorized, and compute the gradient</span>
</span></span><span class="line"><span class="cl"><span class="c1"># of the loss function in a vectorized way.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The naive implementation and the vectorized implementation should match, but</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the vectorized version should still be much faster.</span>
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">grad_naive</span> <span class="o">=</span> <span class="n">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Naive loss and gradient: computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">grad_vectorized</span> <span class="o">=</span> <span class="n">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vectorized loss and gradient: computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The loss is a single number, so it is easy to compare the values computed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># by the two implementations. The gradient on the other hand is a matrix, so</span>
</span></span><span class="line"><span class="cl"><span class="c1"># we use the Frobenius norm to compare them.</span>
</span></span><span class="line"><span class="cl"><span class="n">difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_naive</span> <span class="o">-</span> <span class="n">grad_vectorized</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;difference: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">difference</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Naive loss and gradient: computed in 0.105182s
</span></span><span class="line"><span class="cl">Vectorized loss and gradient: computed in 0.002997s
</span></span><span class="line"><span class="cl">difference: 0.000000</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-linearclassifier" class="heading-element"><span>TODO: LinearClassifier</span>
  <a href="#todo-linearclassifier" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>每次随机抽 <code>batch_size</code> 个样本计算梯度，迭代更新 W</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                 #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Sample batch_size elements from the training data and their           #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># corresponding labels to use in this round of gradient descent.        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Store the data in X_batch and their corresponding labels in           #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># and y_batch should have shape (batch_size,)                           #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                       #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Hint: Use np.random.choice to generate indices. Sampling with         #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># replacement is faster than sampling without replacement.              #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_batch</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate loss and gradient</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># perform parameter update</span>
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                 #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Update the weights using the gradient and the learning rate.          #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#########################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-predict" class="heading-element"><span>TODO: predict</span>
  <a href="#todo-predict" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>直接用数据乘一下已经训练好了的 W 矩阵，argmax 取出最高分数的类。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Use the trained weights of this linear classifier to predict labels for
</span></span></span><span class="line"><span class="cl"><span class="s2">        data points.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - X: A numpy array of shape (N, D) containing training data; there are N
</span></span></span><span class="line"><span class="cl"><span class="s2">          training samples each of dimension D.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional
</span></span></span><span class="line"><span class="cl"><span class="s2">          array of length N, and each element is an integer giving the predicted
</span></span></span><span class="line"><span class="cl"><span class="s2">          class.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO:                                                                   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Implement this method. Store the predicted labels in y_pred.            #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">y_pred</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">iteration 0 / 1500: loss 791.498502
</span></span><span class="line"><span class="cl">iteration 100 / 1500: loss 289.450806
</span></span><span class="line"><span class="cl">iteration 200 / 1500: loss 107.822187
</span></span><span class="line"><span class="cl">iteration 300 / 1500: loss 42.558331
</span></span><span class="line"><span class="cl">iteration 400 / 1500: loss 18.847986
</span></span><span class="line"><span class="cl">iteration 500 / 1500: loss 10.427407
</span></span><span class="line"><span class="cl">iteration 600 / 1500: loss 6.668877
</span></span><span class="line"><span class="cl">iteration 700 / 1500: loss 5.704408
</span></span><span class="line"><span class="cl">iteration 800 / 1500: loss 5.485960
</span></span><span class="line"><span class="cl">iteration 900 / 1500: loss 5.553929
</span></span><span class="line"><span class="cl">iteration 1000 / 1500: loss 6.068083
</span></span><span class="line"><span class="cl">iteration 1100 / 1500: loss 5.816354
</span></span><span class="line"><span class="cl">iteration 1200 / 1500: loss 5.403992
</span></span><span class="line"><span class="cl">iteration 1300 / 1500: loss 5.660007
</span></span><span class="line"><span class="cl">iteration 1400 / 1500: loss 5.307236
</span></span><span class="line"><span class="cl">That took 7.161625s</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/3.png" alt="1" srcset="/image/ML/CS231n/3.png?size=small, /image/ML/CS231n/3.png?size=medium 1.5x, /image/ML/CS231n/3.png?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-使用不同学习率和正则化参数" class="heading-element"><span>TODO: 使用不同学习率和正则化参数</span>
  <a href="#todo-%e4%bd%bf%e7%94%a8%e4%b8%8d%e5%90%8c%e5%ad%a6%e4%b9%a0%e7%8e%87%e5%92%8c%e6%ad%a3%e5%88%99%e5%8c%96%e5%8f%82%e6%95%b0" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Use the validation set to tune hyperparameters (regularization strength and</span>
</span></span><span class="line"><span class="cl"><span class="c1"># learning rate). You should experiment with different ranges for the learning</span>
</span></span><span class="line"><span class="cl"><span class="c1"># rates and regularization strengths; if you are careful you should be able to</span>
</span></span><span class="line"><span class="cl"><span class="c1"># get a classification accuracy of about 0.39 (&gt; 0.385) on the validation set.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Note: you may see runtime/overflow warnings during hyper-parameter search. </span>
</span></span><span class="line"><span class="cl"><span class="c1"># This may be caused by extreme values, and is not a bug.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># results is dictionary mapping tuples of the form</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (learning_rate, regularization_strength) to tuples of the form</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span>
</span></span><span class="line"><span class="cl"><span class="c1"># of data points that are correctly classified.</span>
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>   <span class="c1"># The highest validation accuracy that we have seen so far.</span>
</span></span><span class="line"><span class="cl"><span class="n">best_svm</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># The LinearSVM object that achieved the highest validation rate.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Write code that chooses the best hyperparameters by tuning on the validation #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># set. For each combination of hyperparameters, train a linear SVM on the      #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># training set, compute its accuracy on the training and validation sets, and  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># store these numbers in the results dictionary. In addition, store the best   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># accuracy in best_svm.                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                              #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Hint: You should use a small value for num_iters as you develop your         #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># validation code so that the SVMs don&#39;t take much time to train; once you are #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># confident that your validation code works, you should rerun the validation   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># code with a larger value for num_iters.                                      #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Provided as a reference. You may or may not want to change these hyperparameters</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-7</span><span class="p">,</span> <span class="mf">5e-5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">regularization_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5e4</span><span class="p">,</span> <span class="mf">5e4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">regularization_strengths</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">svm</span> <span class="o">=</span> <span class="n">LinearSVM</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="n">svm</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算训练集上的准确率</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">y_train_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算验证集上的准确率</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_val</span> <span class="o">==</span> <span class="n">y_val_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 将结果存储在字典中</span>
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果当前验证准确率是最高的，则更新 best_val 和 best_svm</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">val_accuracy</span> <span class="o">&gt;</span> <span class="n">best_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_val</span> <span class="o">=</span> <span class="n">val_accuracy</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_svm</span> <span class="o">=</span> <span class="n">svm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1"># Print out results.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lr </span><span class="si">%e</span><span class="s1"> reg </span><span class="si">%e</span><span class="s1"> train accuracy: </span><span class="si">%f</span><span class="s1"> val accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best validation accuracy achieved during cross-validation: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">best_val</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.369000 val accuracy: 0.379000
</span></span><span class="line"><span class="cl">lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.349898 val accuracy: 0.364000
</span></span><span class="line"><span class="cl">lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.075143 val accuracy: 0.094000
</span></span><span class="line"><span class="cl">lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000
</span></span><span class="line"><span class="cl">best validation accuracy achieved during cross-validation: 0.379000</span></span></code></pre></td></tr></table>
</div>
</div><p>可视化权重：</p>
<p><img loading="lazy" src="/image/ML/CS231n/4.png" alt="1" srcset="/image/ML/CS231n/4.png?size=small, /image/ML/CS231n/4.png?size=medium 1.5x, /image/ML/CS231n/4.png?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-2-1" class="heading-element"><span>Inline question 2</span>
  <a href="#inline-question-2-1" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Describe what your visualized SVM weights look like, and offer a brief explanation for why they look the way they do.</p>
<p>${\textit Your Answer:}$ <em>fill this in</em></p>
<p>当然，这里是翻译：</p>
<p><strong>内联问题 2</strong></p>
<p>描述你可视化的 SVM 权重是什么样的，并简要解释它们为什么会是这样的。</p>
<p>${\textit Your Answer:}$</p>
<ol>
<li>
<p><strong>权重图像的外观</strong>：</p>
<ul>
<li>每个类别的权重图像可能看起来像该类别的典型代表。例如，飞机类别的权重图像可能会显示出机翼的形状，汽车类别可能会显示出车轮的形状。</li>
<li>这些图像通常是模糊的，因为 SVM 是线性分类器，它试图通过线性组合输入特征来区分类别。</li>
</ul>
</li>
<li>
<p><strong>为什么权重看起来是这样的</strong>：</p>
<ul>
<li>SVM 权重反映了模型在训练过程中学到的特征，这些特征有助于区分不同的类别。</li>
<li>权重的正值区域表示该区域的像素对该类别的正贡献，而负值区域表示对该类别的负贡献。</li>
<li>由于 SVM 是线性模型，它只能捕捉到线性可分的特征，因此权重图像可能无法捕捉到复杂的非线性特征。</li>
</ul>
</li>
<li>
<p><strong>权重的可解释性</strong>：</p>
<ul>
<li>通过观察这些权重图像，可以直观地理解模型在做出分类决策时关注的图像区域。</li>
<li>这有助于调试和改进模型，例如通过数据增强或特征提取来提高模型的性能。</li>
</ul>
</li>
</ol>
<h3 id="q3-implement-a-softmax-classifier" class="heading-element"><span>Q3: Implement a Softmax classifier</span>
  <a href="#q3-implement-a-softmax-classifier" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p><img loading="lazy" src="/image/ML/CS231n/softmax.webp" alt="1" srcset="/image/ML/CS231n/softmax.webp?size=small, /image/ML/CS231n/softmax.webp?size=medium 1.5x, /image/ML/CS231n/softmax.webp?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="softmax-损失函数" class="heading-element"><span>SoftMax 损失函数</span>
  <a href="#softmax-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>SoftMax 是把得分转换成了概率。公式如下：</p>
<p>$$
S(y_i) = \dfrac{e ^ {y_i}}{\sum\limits_{j} e ^ {y_j}}
$$</p>
<p>损失函数就是根据交叉熵套了个 $-\log(x)$：</p>
<p>$$
L_i = -\log\left(\dfrac{e ^ {y_i}}{\sum\limits_{j} e ^ {y_j}}\right)
$$</p>
<h4 id="softmax-梯度推导" class="heading-element"><span>SoftMax 梯度推导</span>
  <a href="#softmax-%e6%a2%af%e5%ba%a6%e6%8e%a8%e5%af%bc" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>首先样本 $i$ 的得分为：</p>
<p>$$
s_i = x_i \cdot W
$$</p>
<p>$s_{i, j}$ 表示样本 $i$ 在类别 $j$ 上的得分。</p>
<p>$$
p_i = \text{softmax}(s_i) = \dfrac{e ^ {s_i}}{\sum\limits_{k} e ^ {s_{i, k}}}
$$</p>
<p>$p_{i, j}$ 表示样本 $i$ 被预测为类别 $j$ 的概率。</p>
<p>假设一共有 $C$ 类，$p_i$ 是长这样子的：</p>
<p>$$
p_i = \left[ \frac{e^{s_{i, 1}}}{\sum\limits_{k=1}^{C} e^{s_{i, k}}}, \frac{e^{s_{i, 2}}}{\sum\limits_{k=1}^{C} e^{s_{i, k}}}, \cdots, \frac{e^{s_{i, C}}}{\sum\limits_{k=1}^{C} e^{s_{i, k}}} \right]
$$</p>
<p>那么它的损失函数为：</p>
<p>$$
L_i = -\log(p_i) = -\log\left(\dfrac{e ^ {s_i}}{\sum\limits_{k} e ^ {s_{i, k}}}\right)
$$</p>
<p>(以下公式为了形式美观将 $s_{i, j}$ 令成 $s_j$，$p_{i, j}$ 令成 $p_j$，意思是都是样本 $i$ 的)</p>
<p>损失函数对 $W$ 求导，并使用链式法则：</p>
<p>$$
\dfrac{\partial L_i}{\partial W} = \dfrac{\partial L_i}{\partial s_j} \times \dfrac{\partial s_j}{\partial W}
$$</p>
<p>显然有 $\dfrac{\partial s_j}{\partial W} = x_i$，重点讨论 $\dfrac{\partial L_i}{\partial s_j}$：</p>
<div class="details admonition tip open">
  <div class="details-summary admonition-title"><i class="icon fa-fw fa-regular fa-lightbulb" aria-hidden="true"></i>$\frac{\partial L_i}{\partial s_j}$<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></div>
  <div class="details-content">
    <div class="admonition-content"><p>对于每个类别 $j$：</p>
<ul>
<li>如果 $j$ 为正确类别 ($y_i = j$)：</li>
</ul>
<p>$$
\frac{\partial L_i}{\partial s_j} = \frac{\partial (-\log(p_j))}{\partial s_j} = -\frac{1}{p_j} \times \frac{\partial p_j}{\partial s_j}
$$</p>
<p>接下来 $\dfrac{\partial p_j}{\partial s_j}$ 是：</p>
<p>$$
\frac{\partial p_j}{\partial s_j} = \frac{\partial}{\partial s_j} \left( \frac{e^{s_j}}{\sum\limits_{k} e^{s_{j, k}}} \right)
$$</p>
<p>求导：</p>
<p>$$
\frac{\partial}{\partial s_j} \left( \frac{e^{s_j}}{\sum\limits_{k} e^{s_{j, k}}} \right) = \frac{e^{s_j} \sum\limits_{k} e^{s_{j, k}} - e^{s_j} \cdot e^{s_j}}{\left(\sum\limits_{k} e^{s_{j, k}}\right)^2} = \frac{e^{s_j} \left(\sum\limits_{k} e^{s_{j, k}} - e^{s_j}\right)}{\left(\sum\limits_{k} e^{s_{j, k}}\right)^2} = \dfrac{e ^ {s_j}}{\sum\limits_{k} e^{s_{j, k}}} \times \left(1 - \dfrac{e ^ {s_j}}{\sum\limits_{k} e^{s_{j, k}}}\right) = p_j (1 - p_j)
$$</p>
<p>于是 $\dfrac{\partial L_i}{\partial s_j} = -\dfrac{1}{p_j} \times p_j(1 - p_j) = (p_j - 1)$</p>
<p>则 $\dfrac{\partial L_i}{\partial W} = (p_j - 1) x_i$</p>
<ul>
<li>如果 $j$ 为不正确类别 ($y_i \ne j$)：</li>
</ul>
<p>$$
\frac{\partial L_i}{\partial s_{y_j}} = \frac{\partial (-\log(p_{y_i}))}{\partial s_{y_j}} = -\frac{1}{p_{y_i}} \times \frac{\partial p_{y_i}}{\partial s_{y_j}}
$$</p>
<p>接下来 $\dfrac{\partial p_{y_j}}{\partial s_{y_j}}$ 是：</p>
<p>$$
\dfrac{\partial p_{y_j}}{\partial s_{y_j}} = \frac{\partial}{\partial s_{y_j}} \left( \frac{e^{s_j}}{\sum\limits_{k} e^{s_{j, k}}} \right)
$$</p>
<p>求导：</p>
<p>$$
\frac{\partial}{\partial s_{y_j}} \left( \frac{e^{s_j}}{\sum\limits_{k} e^{s_{j, k}}} \right) = -\frac{e^{s_j} e ^ {s_{y_j}}}{\left(\sum\limits_{k} e^{s_{j, k}}\right) ^ 2} = -p_j p_{y_j}
$$</p>
<p>于是 $\dfrac{\partial L_i}{\partial s_{y_j}} = -\dfrac{1}{p_{y_i}} \times \dfrac{\partial p_{y_i}}{\partial s_{y_j}} = p_j$</p>
<p>则 $\dfrac{\partial L_i}{\partial W} = p_j x_i$</p>
</div>
  </div>
</div>
<h4 id="todo-softmax_loss_naive" class="heading-element"><span>TODO: softmax_loss_naive</span>
  <a href="#todo-softmax_loss_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Softmax loss function, naive implementation (with loops)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs have dimension D, there are C classes, and we operate on minibatches
</span></span></span><span class="line"><span class="cl"><span class="s2">    of N examples.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - W: A numpy array of shape (D, C) containing weights.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - X: A numpy array of shape (N, D) containing a minibatch of data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means
</span></span></span><span class="line"><span class="cl"><span class="s2">      that X[i] has label c, where 0 &lt;= c &lt; C.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - reg: (float) regularization strength
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - loss as single float
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gradient with respect to weights W; an array of same shape as W
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize the loss and gradient to zero.</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Compute the softmax loss and its gradient using explicit loops.     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># regularization!                                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取样本数量和类别数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 遍历每个样本</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算得分</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 数值稳定性处理</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算softmax概率</span>
</span></span><span class="line"><span class="cl">        <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">exp_scores</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算损失</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算梯度</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dW</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">probs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 平均损失</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加上正则化损失</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 平均梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">/=</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加上正则化梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="inline-question-1-2" class="heading-element"><span>Inline Question 1</span>
  <a href="#inline-question-1-2" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Why do we expect our loss to be close to -log(0.1)? Explain briefly.</p>
<p>${\textit Your Answer:}$ <em>Fill this in</em></p>
<p><strong>内嵌问题 1</strong></p>
<p>为什么我们期望损失接近于 $-\log(0.1)$？请简要解释。</p>
<p>${\textit Your Answer:}$</p>
<p>在 Softmax 分类器中，损失函数的计算是基于预测概率的对数损失。假设我们有 10 个类别，并且权重矩阵是随机初始化的，那么每个类别的预测概率大约是均匀分布的，即每个类别的概率约为 $0.1$</p>
<p>因此，损失函数的期望值为 $-\log(0.1)$，因为这是对数损失在预测概率为 $0.1$ 时的值。</p>
<h4 id="todo-softmax_loss_vectorized" class="heading-element"><span>TODO: softmax_loss_vectorized</span>
  <a href="#todo-softmax_loss_vectorized" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">softmax_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Softmax loss function, vectorized version.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs and outputs are the same as softmax_loss_naive.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize the loss and gradient to zero.</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Compute the softmax loss and its gradient using no explicit loops.  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># regularization!                                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取样本数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算得分矩阵</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 数值稳定性处理</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算softmax概率</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">probs</span> <span class="o">=</span> <span class="n">exp_scores</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算损失</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct_log_probs</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_log_probs</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dscores</span> <span class="o">=</span> <span class="n">probs</span>
</span></span><span class="line"><span class="cl">    <span class="n">dscores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dscores</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span></span></span></code></pre></td></tr></table>
</div>
</div><p>对比：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Now that we have a naive implementation of the softmax loss function and its gradient,</span>
</span></span><span class="line"><span class="cl"><span class="c1"># implement a vectorized version in softmax_loss_vectorized.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># The two versions should compute the same results, but the vectorized version should be</span>
</span></span><span class="line"><span class="cl"><span class="c1"># much faster.</span>
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_naive</span><span class="p">,</span> <span class="n">grad_naive</span> <span class="o">=</span> <span class="n">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;naive loss: </span><span class="si">%e</span><span class="s1"> computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_naive</span><span class="p">,</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers.softmax</span> <span class="kn">import</span> <span class="n">softmax_loss_vectorized</span>
</span></span><span class="line"><span class="cl"><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_vectorized</span><span class="p">,</span> <span class="n">grad_vectorized</span> <span class="o">=</span> <span class="n">softmax_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="mf">0.000005</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;vectorized loss: </span><span class="si">%e</span><span class="s1"> computed in </span><span class="si">%f</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">loss_vectorized</span><span class="p">,</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># As we did for the SVM, we use the Frobenius norm to compare the two versions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># of the gradient.</span>
</span></span><span class="line"><span class="cl"><span class="n">grad_difference</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_naive</span> <span class="o">-</span> <span class="n">grad_vectorized</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss difference: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">loss_naive</span> <span class="o">-</span> <span class="n">loss_vectorized</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient difference: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">grad_difference</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">naive loss: 2.304545e+00 computed in 0.112797s
</span></span><span class="line"><span class="cl">vectorized loss: 2.304545e+00 computed in 0.007444s
</span></span><span class="line"><span class="cl">Loss difference: 0.000000
</span></span><span class="line"><span class="cl">Gradient difference: 0.000000</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-交叉验证" class="heading-element"><span>TODO: 交叉验证</span>
  <a href="#todo-%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Use the validation set to tune hyperparameters (regularization strength and</span>
</span></span><span class="line"><span class="cl"><span class="c1"># learning rate). You should experiment with different ranges for the learning</span>
</span></span><span class="line"><span class="cl"><span class="c1"># rates and regularization strengths; if you are careful you should be able to</span>
</span></span><span class="line"><span class="cl"><span class="c1"># get a classification accuracy of over 0.35 on the validation set.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers</span> <span class="kn">import</span> <span class="n">Softmax</span>
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">best_softmax</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Use the validation set to set the learning rate and regularization strength. #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This should be identical to the validation that you did for the SVM; save    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the best trained softmax classifer in best_softmax.                          #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Provided as a reference. You may or may not want to change these hyperparameters</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-7</span><span class="p">,</span> <span class="mf">5e-7</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">regularization_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5e4</span><span class="p">,</span> <span class="mf">5e4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">regularization_strengths</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">y_train_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_val</span> <span class="o">==</span> <span class="n">y_val_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">val_accuracy</span> <span class="o">&gt;</span> <span class="n">best_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_val</span> <span class="o">=</span> <span class="n">val_accuracy</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_softmax</span> <span class="o">=</span> <span class="n">softmax</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="c1"># Print out results.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lr </span><span class="si">%e</span><span class="s1"> reg </span><span class="si">%e</span><span class="s1"> train accuracy: </span><span class="si">%f</span><span class="s1"> val accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best validation accuracy achieved during cross-validation: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">best_val</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.345571 val accuracy: 0.366000
</span></span><span class="line"><span class="cl">lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.327163 val accuracy: 0.345000
</span></span><span class="line"><span class="cl">lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.340449 val accuracy: 0.352000
</span></span><span class="line"><span class="cl">lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.329878 val accuracy: 0.330000
</span></span><span class="line"><span class="cl">best validation accuracy achieved during cross-validation: 0.366000</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="inline-question-2---true-or-false" class="heading-element"><span>Inline Question 2 - <em>True or False</em></span>
  <a href="#inline-question-2---true-or-false" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.</p>
<p>${\textit Your Answer:}$</p>
<p>${\textit Your Explanation:}$</p>
<p><strong>内嵌问题 2</strong> - <em>对或错</em></p>
<p>假设整体训练损失定义为所有训练样本的每个数据点损失之和。可以添加一个新的数据点到训练集中，使得 SVM 损失保持不变，但对于 Softmax 分类器损失来说，这种情况不会发生。</p>
<p>${\textit Your Answer:}$ True</p>
<p>${\textit Your Explanation:}$</p>
<p>在 SVM 中，损失函数是基于边界的。对于一个新的数据点，如果它位于正确的边界一侧并且离边界足够远，那么它对损失的贡献为零，因此不会改变整体损失。</p>
<p>然而，在 Softmax 分类器中，损失函数是基于概率分布的对数损失。每个数据点都会对损失产生影响，因为即使是一个新的数据点也会改变概率分布，从而影响损失。因此，添加一个新的数据点总是会改变 Softmax 分类器的损失。</p>
<p>可视化权重：</p>
<p><img loading="lazy" src="/image/ML/CS231n/5.png" alt="1" srcset="/image/ML/CS231n/5.png?size=small, /image/ML/CS231n/5.png?size=medium 1.5x, /image/ML/CS231n/5.png?size=large 2x" data-title="1" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h3 id="q4-two-layer-neural-network" class="heading-element"><span>Q4: Two-Layer Neural Network</span>
  <a href="#q4-two-layer-neural-network" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>首先检验一下前向传播，数据生成一个两组大小为 $4 \times 5 \times 6$ 的数据，也就是说输入层是 $120$ 个节点，然后直接连接输出层，设输出层有 $3$ 个节点。
所以说 $w$ 就是 $120 \times 3$ 的，$b$ 就是 $3$ 的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Test the affine_forward function</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">output_dim</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">input_size</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">weight_size</span> <span class="o">=</span> <span class="n">output_dim</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">input_shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">weight_size</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="n">output_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">correct_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">1.49834967</span><span class="p">,</span>  <span class="mf">1.70660132</span><span class="p">,</span>  <span class="mf">1.91485297</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span> <span class="mf">3.25553199</span><span class="p">,</span>  <span class="mf">3.5141327</span><span class="p">,</span>   <span class="mf">3.77273342</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compare your output with ours. The error should be around e-9 or less.</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing affine_forward function:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;difference: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">correct_out</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-affine_forward" class="heading-element"><span>TODO: affine_forward</span>
  <a href="#todo-affine_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>把 $x$ 压成 $120$ 的第二维。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the forward pass for an affine (fully-connected) layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
</span></span></span><span class="line"><span class="cl"><span class="s2">    examples, where each example x[i] has shape (d_1, ..., d_k). We will
</span></span></span><span class="line"><span class="cl"><span class="s2">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
</span></span></span><span class="line"><span class="cl"><span class="s2">    then transform it to an output vector of dimension M.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - w: A numpy array of weights, of shape (D, M)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - b: A numpy array of biases, of shape (M,)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: output, of shape (N, M)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: (x, w, b)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the affine forward pass. Store the result in out. You   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># will need to reshape the input into rows.                               #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><p>接下来检验反向传播。</p>
<h4 id="todo-affine_backward" class="heading-element"><span>TODO: affine_backward</span>
  <a href="#todo-affine_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>我们知道这是批量梯度下降，也就是说输入数据 $x$ 被我们处理成了 $n \times k$ 维的，每一行代表一个数据，列就是特征。那么输出层定义为 $n \times m$ 维的，$n$ 是数据批量个数，$m$ 是输出层节点个数，但是 $w$ 和 $b$ 分别是 $k \times m$ 与 $1 \times m$ 维的，也就是说这 $n$ 个数据共用 $w$ 和 $b$，然后每个数据对应一组输出。</p>
<p>那么在反向传播时，我们需要计算 $x, w, b$ 的偏导，首先他给了一个 <code>dout</code> 数组，这代表输出层的导数，他也是一个 $n \times m$ 的。</p>
<p>单看 $\text{out}_{i, j}$：</p>
<p>$$
\text{out}_{i, j} = \sum_k x_{i, k} w_{k, j} + b_j
$$</p>
<p>$\text{out}_{i, j}$ 对 $w_{k, j}$ 求偏导：</p>
<p>$$
\dfrac{\partial \text{out}_{i, j}}{\partial w_{k, j}} = x_{i, k}
$$</p>
<p>$\text{out}_{i, j}$ 对 $x_{i, k}$ 求偏导：</p>
<p>$$
\dfrac{\partial \text{out}_{i, j}}{\partial x_{i, k}} = w_{k, j}
$$</p>
<p>$\text{out}_{i, j}$ 对 $b_j$ 求偏导：</p>
<p>$$
\dfrac{\partial \text{out}_{i, j}}{\partial b_j} = 1
$$</p>
<p>那么损失函数 $L$ 对 $w_{k, j}$ 求偏导，根据链式法则有：</p>
<p>$$
\dfrac{\partial L}{\partial w_{k, j}} = \sum_i \dfrac{\partial L}{\partial \text{out}_{i, j}} \dfrac{\partial \text{out}_{i, j}}{\partial w_{k, j}}
$$</p>
<p>其中 $\dfrac{\partial L}{\partial \text{out}_{i, j}}$ 这个东西就是 <code>dout[i, j]</code> (上游传来的导数)，而 $\dfrac{\partial \text{out}_{i, j}}{\partial w_{k, j}} = x_{i, k}$，所以</p>
<p>$$
\dfrac{\partial L}{\partial w_{k, j}} = \sum_i \text{dout}_{i, j}x_{i, k}
$$</p>
<p>再来看损失函数 $L_i$ 对 $x_{i, k}$ 求偏导，根据链式法则有：</p>
<p>$$
\dfrac{\partial L}{\partial x_{i, k}} = \sum_j \dfrac{\partial L}{\partial \text{out}_{i, j}} \dfrac{\partial \text{out}_{i, j}}{\partial x_{i, k}}
$$</p>
<p>继续带入 $\dfrac{\partial L}{\partial \text{out}_{i, j}} = \text{dout}_{i, j}$，$\dfrac{\partial \text{out}_{i, j}}{\partial x_{i, k}} = w_{k, j}$：</p>
<p>$$
\dfrac{\partial L}{\partial x_{i, k}} = \sum_j \text{dout}_{i, j}w_{k, j}
$$</p>
<p>继续看损失函数 $L_i$ 对 $b_{j}$ 求偏导，根据链式法则有：</p>
<p>$$
\dfrac{\partial L}{\partial b_{j}} = \sum_i \dfrac{\partial L}{\partial \text{out}_{i, j}} \dfrac{\partial \text{out}_{i, j}}{\partial b_{j}}
$$</p>
<p>还是继续带入 $\dfrac{\partial L}{\partial \text{out}_{i, j}} = \text{dout}_{i, j}$，$\dfrac{\partial \text{out}_{i, j}}{\partial b_j} = 1$，那么：</p>
<p>$$
\dfrac{\partial L}{\partial b_{j}} = \sum_i \text{dout}_{i, j}
$$</p>
<p>代码直接三行结束。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the backward pass for an affine layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivative, of shape (N, M)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - x: Input data, of shape (N, d_1, ... d_k)
</span></span></span><span class="line"><span class="cl"><span class="s2">      - w: Weights, of shape (D, M)
</span></span></span><span class="line"><span class="cl"><span class="s2">      - b: Biases, of shape (M,)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dw: Gradient with respect to w, of shape (D, M)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - db: Gradient with respect to b, of shape (M,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the affine backward pass.                               #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dw</span> <span class="o">=</span> <span class="n">x_reshaped</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span></span></span></code></pre></td></tr></table>
</div>
</div><p>验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Test the affine_backward function</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dx_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dw_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">db_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">_</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The error should be around e-10 or less</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing affine_backward function:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dx error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dx_num</span><span class="p">,</span> <span class="n">dx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dw error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dw_num</span><span class="p">,</span> <span class="n">dw</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;db error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">db_num</span><span class="p">,</span> <span class="n">db</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing affine_backward function:
</span></span><span class="line"><span class="cl">dx error:  1.0908199508708189e-10
</span></span><span class="line"><span class="cl">dw error:  2.1752635504596857e-10
</span></span><span class="line"><span class="cl">db error:  7.736978834487815e-12</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-relu_forward" class="heading-element"><span>TODO: relu_forward</span>
  <a href="#todo-relu_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>这个就是直接套公式
$$
\text{ReLU}(x) = \max(0, x)
$$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the forward pass for a layer of rectified linear units (ReLUs).
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Input:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Inputs, of any shape
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Output, of the same shape as x
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the ReLU forward pass.                                  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-relu_backward" class="heading-element"><span>TODO: relu_backward</span>
  <a href="#todo-relu_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>梯度也是显然的，如果 $x &gt; 0$，导数为 $1$，否则为 $0$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the backward pass for a layer of rectified linear units (ReLUs).
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Input:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of any shape
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Input x, of same shape as dout
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the ReLU backward pass.                                 #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="inline-question-1-3" class="heading-element"><span>Inline Question 1</span>
  <a href="#inline-question-1-3" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>We&rsquo;ve only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?</p>
<ol>
<li>Sigmoid</li>
<li>ReLU</li>
<li>Leaky ReLU</li>
</ol>
<p>Answer:</p>
<p><strong>内联问题 1:</strong></p>
<p>我们只要求你实现 ReLU，但在神经网络中可以使用许多不同的激活函数，每种都有其优缺点。特别是，激活函数中常见的一个问题是在反向传播过程中获得零（或接近零）的梯度流。以下哪些激活函数存在这个问题？如果你在一维情况下考虑这些函数，什么类型的输入会导致这种行为？</p>
<ol>
<li>Sigmoid</li>
<li>ReLU</li>
<li>Leaky ReLU</li>
</ol>
<p><strong>答案:</strong></p>
<p>在反向传播过程中获得零梯度流的问题通常被称为“梯度消失”问题。以下是对每个激活函数的分析：</p>
<ol>
<li>
<p><strong>Sigmoid</strong>: Sigmoid 函数在输入值非常大或非常小时会趋向于 0 或 1，这导致其导数接近于零。因此，Sigmoid 函数在输入值非常大或非常小时会出现梯度消失问题。</p>
</li>
<li>
<p><strong>ReLU</strong>: ReLU 函数在输入值小于或等于零时，其导数为零。因此，当输入值为负数时，ReLU 会出现梯度消失问题。</p>
</li>
<li>
<p><strong>Leaky ReLU</strong>: Leaky ReLU 是 ReLU 的一种变体，它在输入值小于零时，导数为一个很小的常数（而不是零）。因此，Leaky ReLU 减少了梯度消失问题，因为即使在输入值为负数时，梯度也不会完全消失。</p>
</li>
</ol>
<p>因此，Sigmoid 和 ReLU 都可能出现梯度消失问题，而 Leaky ReLU 通过在负输入时保持一个小的梯度来缓解这个问题。</p>
<ol>
<li>Sigmoid: 输入值非常大或非常小时。</li>
<li>ReLU: 输入值小于或等于零时。</li>
</ol>
<p>检验 ReLU：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.layer_utils</span> <span class="kn">import</span> <span class="n">affine_relu_forward</span><span class="p">,</span> <span class="n">affine_relu_backward</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">affine_relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dx_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dw_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">db_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient_array</span><span class="p">(</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Relative error should be around e-10 or less</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing affine_relu_forward and affine_relu_backward:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dx error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dx_num</span><span class="p">,</span> <span class="n">dx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dw error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dw_num</span><span class="p">,</span> <span class="n">dw</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;db error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">db_num</span><span class="p">,</span> <span class="n">db</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing affine_relu_forward and affine_relu_backward:
</span></span><span class="line"><span class="cl">dx error:  6.395535042049294e-11
</span></span><span class="line"><span class="cl">dw error:  8.162015570444288e-11
</span></span><span class="line"><span class="cl">db error:  7.826724021458994e-12</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-svm_loss" class="heading-element"><span>TODO: svm_loss</span>
  <a href="#todo-svm_loss" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>和之前写过的差不多。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">svm_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the loss and gradient using for multiclass SVM classification.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
</span></span></span><span class="line"><span class="cl"><span class="s2">      class for the ith input.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
</span></span></span><span class="line"><span class="cl"><span class="s2">      0 &lt;= y[i] &lt; C
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - loss: Scalar giving the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient of the loss with respect to x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Copy over your solution from A1.</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct_class_scores</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">margins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">correct_class_scores</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">margins</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">margins</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">binary</span> <span class="o">=</span> <span class="n">margins</span>
</span></span><span class="line"><span class="cl">    <span class="n">binary</span><span class="p">[</span><span class="n">margins</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">row_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">binary</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">binary</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">row_sum</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">binary</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-softmax_loss" class="heading-element"><span>TODO: softmax_loss</span>
  <a href="#todo-softmax_loss" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>同样和之前写过的差不多。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the loss and gradient for softmax classification.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
</span></span></span><span class="line"><span class="cl"><span class="s2">      class for the ith input.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
</span></span></span><span class="line"><span class="cl"><span class="s2">      0 &lt;= y[i] &lt; C
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - loss: Scalar giving the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient of the loss with respect to x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Copy over your solution from A1.</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取样本数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_train</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算softmax</span>
</span></span><span class="line"><span class="cl">    <span class="n">shifted_logits</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 数值稳定性</span>
</span></span><span class="line"><span class="cl">    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shifted_logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">shifted_logits</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算损失</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span> <span class="o">/</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">/=</span> <span class="n">num_train</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dx</span></span></span></code></pre></td></tr></table>
</div>
</div><p>误差检验：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">num_classes</span><span class="p">,</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dx_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">svm_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">svm_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing svm_loss:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss: &#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dx error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dx_num</span><span class="p">,</span> <span class="n">dx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dx_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">dx</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Testing softmax_loss:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss: &#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;dx error: &#39;</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">dx_num</span><span class="p">,</span> <span class="n">dx</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing svm_loss:
</span></span><span class="line"><span class="cl">loss:  8.999602749096233
</span></span><span class="line"><span class="cl">dx error:  1.4021566006651672e-09
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Testing softmax_loss:
</span></span><span class="line"><span class="cl">loss:  2.302545844500738
</span></span><span class="line"><span class="cl">dx error:  9.384673161989355e-09</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-twolayernet__init__" class="heading-element"><span>TODO: TwoLayerNet.<strong>init</strong></span>
  <a href="#todo-twolayernet__init__" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>要我们训练一个 <code>affine - relu - affine - softmax</code> 的两层神经网络。<code>np.random.normal</code> 随机初始化一个高斯分布的概率密度随机数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_dim</span><span class="o">=</span><span class="mi">3</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Initialize a new network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - input_dim: An integer giving the size of the input
</span></span></span><span class="line"><span class="cl"><span class="s2">        - hidden_dim: An integer giving the size of the hidden layer
</span></span></span><span class="line"><span class="cl"><span class="s2">        - num_classes: An integer giving the number of classes to classify
</span></span></span><span class="line"><span class="cl"><span class="s2">        - weight_scale: Scalar giving the standard deviation for random
</span></span></span><span class="line"><span class="cl"><span class="s2">          initialization of the weights.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - reg: Scalar giving L2 regularization strength.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Initialize the weights and biases of the two-layer net. Weights    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># should be initialized from a Gaussian centered at 0.0 with               #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># standard deviation equal to weight_scale, and biases should be           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># initialized to zero. All weights and biases should be stored in the      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># dictionary self.params, with first layer weights                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># and biases using the keys &#39;W1&#39; and &#39;b1&#39; and second layer                 #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># weights and biases using the keys &#39;W2&#39; and &#39;b2&#39;.                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">num_classes</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-twolayernetloss" class="heading-element"><span>TODO: TwoLayerNet.loss</span>
  <a href="#todo-twolayernetloss" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Compute loss and gradient for a minibatch of data.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - X: Array of input data of shape (N, d_1, ..., d_k)
</span></span></span><span class="line"><span class="cl"><span class="s2">        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        If y is None, then run a test-time forward pass of the model and return:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - scores: Array of shape (N, C) giving classification scores, where
</span></span></span><span class="line"><span class="cl"><span class="s2">          scores[i, c] is the classification score for X[i] and class c.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        If y is not None, then run a training-time forward and backward pass and
</span></span></span><span class="line"><span class="cl"><span class="s2">        return a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - loss: Scalar value giving the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        - grads: Dictionary with the same keys as self.params, mapping parameter
</span></span></span><span class="line"><span class="cl"><span class="s2">          names to gradients of the loss with respect to those parameters.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the forward pass for the two-layer net, computing the    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># class scores for X and storing them in the scores variable.              #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h1</span><span class="p">,</span> <span class="n">cache1</span> <span class="o">=</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span><span class="p">,</span> <span class="n">cache2</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># If y is None then we are in test mode so just return scores</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">scores</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the backward pass for the two-layer net. Store the loss  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in the loss variable and gradients in the grads dictionary. Compute data #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># loss using softmax, and make sure that grads[k] holds the gradients for  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># self.params[k]. Don&#39;t forget to add L2 regularization!                   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算损失和梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">dscores</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 添加L2正则化</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">*</span> <span class="n">W2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 反向传播</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第二层的反向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">dh1</span><span class="p">,</span> <span class="n">dW2</span><span class="p">,</span> <span class="n">db2</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">cache2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 第一层的反向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">dx</span><span class="p">,</span> <span class="n">dW1</span><span class="p">,</span> <span class="n">db1</span> <span class="o">=</span> <span class="n">affine_relu_backward</span><span class="p">(</span><span class="n">dh1</span><span class="p">,</span> <span class="n">cache1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 添加正则化梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">dW2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
</span></span><span class="line"><span class="cl">        <span class="n">dW1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="s1">&#39;W1&#39;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s1">&#39;b1&#39;</span><span class="p">:</span> <span class="n">db1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s1">&#39;W2&#39;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="s1">&#39;b2&#39;</span><span class="p">:</span> <span class="n">db2</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span></span></span></code></pre></td></tr></table>
</div>
</div><p>检验梯度：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">7</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="n">std</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing initialization ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">W1_std</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">-</span> <span class="n">std</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">W2_std</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">-</span> <span class="n">std</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">W1_std</span> <span class="o">&lt;</span> <span class="n">std</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;First layer weights do not seem right&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">b1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;First layer biases do not seem right&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">W2_std</span> <span class="o">&lt;</span> <span class="n">std</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;Second layer weights do not seem right&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">b2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Second layer biases do not seem right&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing test-time forward pass ... &#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">D</span><span class="o">*</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">H</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">H</span><span class="o">*</span><span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">N</span><span class="o">*</span><span class="n">D</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">correct_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="p">[[</span><span class="mf">11.53165108</span><span class="p">,</span>  <span class="mf">12.2917344</span><span class="p">,</span>   <span class="mf">13.05181771</span><span class="p">,</span>  <span class="mf">13.81190102</span><span class="p">,</span>  <span class="mf">14.57198434</span><span class="p">,</span> <span class="mf">15.33206765</span><span class="p">,</span>  <span class="mf">16.09215096</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">   <span class="p">[</span><span class="mf">12.05769098</span><span class="p">,</span>  <span class="mf">12.74614105</span><span class="p">,</span>  <span class="mf">13.43459113</span><span class="p">,</span>  <span class="mf">14.1230412</span><span class="p">,</span>   <span class="mf">14.81149128</span><span class="p">,</span> <span class="mf">15.49994135</span><span class="p">,</span>  <span class="mf">16.18839143</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">   <span class="p">[</span><span class="mf">12.58373087</span><span class="p">,</span>  <span class="mf">13.20054771</span><span class="p">,</span>  <span class="mf">13.81736455</span><span class="p">,</span>  <span class="mf">14.43418138</span><span class="p">,</span>  <span class="mf">15.05099822</span><span class="p">,</span> <span class="mf">15.66781506</span><span class="p">,</span>  <span class="mf">16.2846319</span> <span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">scores_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">correct_scores</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">scores_diff</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s1">&#39;Problem with test-time forward pass&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Testing training loss (no regularization)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">correct_loss</span> <span class="o">=</span> <span class="mf">3.4702243556</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">loss</span> <span class="o">-</span> <span class="n">correct_loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="s1">&#39;Problem with training-time loss&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">correct_loss</span> <span class="o">=</span> <span class="mf">26.5948426952</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">loss</span> <span class="o">-</span> <span class="n">correct_loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="s1">&#39;Problem with regularization loss&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Errors should be around e-7 or less</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running numeric gradient check with reg = &#39;</span><span class="p">,</span> <span class="n">reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
</span></span><span class="line"><span class="cl">  <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">grad_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> relative error: </span><span class="si">%.2e</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="n">name</span><span class="p">])))</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing initialization ... 
</span></span><span class="line"><span class="cl">Testing test-time forward pass ... 
</span></span><span class="line"><span class="cl">Testing training loss (no regularization)
</span></span><span class="line"><span class="cl">Running numeric gradient check with reg =  0.0
</span></span><span class="line"><span class="cl">W1 relative error: 1.53e-08
</span></span><span class="line"><span class="cl">W2 relative error: 3.37e-10
</span></span><span class="line"><span class="cl">b1 relative error: 8.01e-09
</span></span><span class="line"><span class="cl">b2 relative error: 4.33e-10
</span></span><span class="line"><span class="cl">Running numeric gradient check with reg =  0.7
</span></span><span class="line"><span class="cl">W1 relative error: 2.53e-07
</span></span><span class="line"><span class="cl">W2 relative error: 2.85e-08
</span></span><span class="line"><span class="cl">b1 relative error: 1.35e-08
</span></span><span class="line"><span class="cl">b2 relative error: 1.97e-09</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-solver" class="heading-element"><span>TODO: Solver</span>
  <a href="#todo-solver" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>构造 solver 训练。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">input_size</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl"><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Use a Solver instance to train a TwoLayerNet that achieves about 36% #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># accuracy on the validation set.                                            #</span>
</span></span><span class="line"><span class="cl"><span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                             END OF YOUR CODE                               #</span>
</span></span><span class="line"><span class="cl"><span class="c1">##############################################################################</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-hyperparameters" class="heading-element"><span>TODO: hyperparameters</span>
  <a href="#todo-hyperparameters" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Tune hyperparameters using the validation set. Store your best trained  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># model in best_model.                                                          #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                               #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># To help debug your network, it may help to use visualizations similar to the  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ones we used above; these visualizations will have significant qualitative    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># differences from the ones we saw above for the poorly tuned network.          #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                               #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Tweaking hyperparameters by hand can be fun, but you might find it useful to  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># write code to sweep through possible combinations of hyperparameters          #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># automatically like we did on thexs previous exercises.                          #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">regularization_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">regularization_strengths</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">train_acc_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">val_acc_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">val_accuracy</span> <span class="o">&gt;</span> <span class="n">best_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_val</span> <span class="o">=</span> <span class="n">val_accuracy</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_model</span> <span class="o">=</span> <span class="n">model</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lr </span><span class="si">%e</span><span class="s1"> reg </span><span class="si">%e</span><span class="s1"> train accuracy: </span><span class="si">%f</span><span class="s1"> val accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                              END OF YOUR CODE                                #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span></span></span></code></pre></td></tr></table>
</div>
</div><p>跑了 11 分钟。</p>
<p>测试模型准确率：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;X_val&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Validation set accuracy: &#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">y_val_pred</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y_val&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">best_model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;X_test&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: &#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y_test&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span></span></span></code></pre></td></tr></table>
</div>
</div><p>超过了 48%</p>
<div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Validation set accuracy:  0.509
</span></span><span class="line"><span class="cl">Test set accuracy:  0.488</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="inline-question-2-2" class="heading-element"><span>Inline Question 2</span>
  <a href="#inline-question-2-2" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.</p>
<ol>
<li>Train on a larger dataset.</li>
<li>Add more hidden units.</li>
<li>Increase the regularization strength.</li>
<li>None of the above.</li>
</ol>
<p>${\textit Your Answer:}$</p>
<p>${\textit Your Explanation:}$</p>
<p><strong>内联问题 2:</strong></p>
<p>现在你已经训练了一个神经网络分类器，你可能发现测试准确率远低于训练准确率。我们可以通过哪些方法来减小这个差距？选择所有适用的选项。</p>
<ol>
<li>在更大的数据集上训练</li>
<li>增加隐藏单元数量</li>
<li>增加正则化强度</li>
<li>以上都不是</li>
</ol>
<p>${\textit Your Answer:}$ 1, 3</p>
<p>${\textit Your Explanation:}$</p>
<ol>
<li>
<p><strong>在更大的数据集上训练</strong>:</p>
<ul>
<li>更大的训练数据集可以帮助模型学习更通用的特征</li>
<li>减少过拟合的风险，因为模型需要适应更多样的数据</li>
</ul>
</li>
<li>
<p><strong>增加隐藏单元数量</strong>:</p>
<ul>
<li>增加模型复杂度实际上会加大过拟合的风险</li>
<li>可能会使训练和测试准确率的差距更大</li>
</ul>
</li>
<li>
<p><strong>增加正则化强度</strong>:</p>
<ul>
<li>正则化是专门用来减少过拟合的技术</li>
<li>通过限制权重的大小，迫使模型学习更简单的特征表示</li>
<li>有助于提高模型的泛化能力</li>
</ul>
</li>
</ol>
<h3 id="q5-higher-level-representations-image-features" class="heading-element"><span>Q5: Higher Level Representations: Image Features</span>
  <a href="#q5-higher-level-representations-image-features" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>使用 HOG 和 color histogram 特征提取一下图像的信息，简单来说就是将图片的特征表现得更明显。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.features</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_color_bins</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Number of bins in the color histogram</span>
</span></span><span class="line"><span class="cl"><span class="n">feature_fns</span> <span class="o">=</span> <span class="p">[</span><span class="n">hog_feature</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">img</span><span class="p">:</span> <span class="n">color_histogram_hsv</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">nbin</span><span class="o">=</span><span class="n">num_color_bins</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train_feats</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">feature_fns</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val_feats</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">feature_fns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test_feats</span> <span class="o">=</span> <span class="n">extract_features</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">feature_fns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Preprocessing: Subtract the mean feature</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_feat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train_feats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train_feats</span> <span class="o">-=</span> <span class="n">mean_feat</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val_feats</span> <span class="o">-=</span> <span class="n">mean_feat</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test_feats</span> <span class="o">-=</span> <span class="n">mean_feat</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Preprocessing: Divide by standard deviation. This ensures that each feature</span>
</span></span><span class="line"><span class="cl"><span class="c1"># has roughly the same scale.</span>
</span></span><span class="line"><span class="cl"><span class="n">std_feat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_train_feats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train_feats</span> <span class="o">/=</span> <span class="n">std_feat</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val_feats</span> <span class="o">/=</span> <span class="n">std_feat</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test_feats</span> <span class="o">/=</span> <span class="n">std_feat</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Preprocessing: Add a bias dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train_feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_train_feats</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train_feats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_val_feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_val_feats</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_val_feats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test_feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X_test_feats</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_test_feats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-train-svm-on-features" class="heading-element"><span>TODO: Train SVM on features</span>
  <a href="#todo-train-svm-on-features" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>用新数据训练，代码和之前差不多</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Use the validation set to tune the learning rate and regularization strength</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers.linear_classifier</span> <span class="kn">import</span> <span class="n">LinearSVM</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-9</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">regularization_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5e4</span><span class="p">,</span> <span class="mf">5e5</span><span class="p">,</span> <span class="mf">5e6</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">best_svm</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Use the validation set to set the learning rate and regularization strength. #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This should be identical to the validation that you did for the SVM; save    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the best trained classifer in best_svm. You might also want to play          #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># with different numbers of bins in the color histogram. If you are careful    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># you should be able to get accuracy of near 0.44 on the validation set.       #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">regularization_strengths</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">svm</span> <span class="o">=</span> <span class="n">LinearSVM</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">svm</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train_feats</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">num_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_feats</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">y_train_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_feats</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_val</span> <span class="o">==</span> <span class="n">y_val_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">val_accuracy</span> <span class="o">&gt;</span> <span class="n">best_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_val</span> <span class="o">=</span> <span class="n">val_accuracy</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_svm</span> <span class="o">=</span> <span class="n">svm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print out results.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lr </span><span class="si">%e</span><span class="s1"> reg </span><span class="si">%e</span><span class="s1"> train accuracy: </span><span class="si">%f</span><span class="s1"> val accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best validation accuracy achieved: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">best_val</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">lr 1.000000e-09 reg 5.000000e+04 train accuracy: 0.099714 val accuracy: 0.093000
</span></span><span class="line"><span class="cl">lr 1.000000e-09 reg 5.000000e+05 train accuracy: 0.093898 val accuracy: 0.078000
</span></span><span class="line"><span class="cl">lr 1.000000e-09 reg 5.000000e+06 train accuracy: 0.414571 val accuracy: 0.413000
</span></span><span class="line"><span class="cl">lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.092082 val accuracy: 0.077000
</span></span><span class="line"><span class="cl">lr 1.000000e-08 reg 5.000000e+05 train accuracy: 0.413224 val accuracy: 0.422000
</span></span><span class="line"><span class="cl">lr 1.000000e-08 reg 5.000000e+06 train accuracy: 0.409714 val accuracy: 0.393000
</span></span><span class="line"><span class="cl">lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.417327 val accuracy: 0.421000
</span></span><span class="line"><span class="cl">lr 1.000000e-07 reg 5.000000e+05 train accuracy: 0.407857 val accuracy: 0.396000
</span></span><span class="line"><span class="cl">lr 1.000000e-07 reg 5.000000e+06 train accuracy: 0.321898 val accuracy: 0.309000
</span></span><span class="line"><span class="cl">best validation accuracy achieved: 0.422000</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="c1"># Evaluate your trained SVM on the test set: you should be able to get at least 0.40</span>
</span></span><span class="line"><span class="cl"><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">best_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_feats</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_test_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>0.424</code></p>
<h4 id="inline-question-1-4" class="heading-element"><span>Inline question 1:</span>
  <a href="#inline-question-1-4" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Describe the misclassification results that you see. Do they make sense?</p>
<p>${\textit Your Answer:}$ 可以理解，因为有些太相似了。</p>
<h4 id="todo-neural-network-on-image-features" class="heading-element"><span>TODO: Neural Network on image features</span>
  <a href="#todo-neural-network-on-image-features" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>代码和之前差不多。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.classifiers.fc_net</span> <span class="kn">import</span> <span class="n">TwoLayerNet</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cs231n.solver</span> <span class="kn">import</span> <span class="n">Solver</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">input_dim</span> <span class="o">=</span> <span class="n">X_train_feats</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;X_train&#39;</span><span class="p">:</span> <span class="n">X_train_feats</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;y_train&#39;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;X_val&#39;</span><span class="p">:</span> <span class="n">X_val_feats</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;y_val&#39;</span><span class="p">:</span> <span class="n">y_val</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;X_test&#39;</span><span class="p">:</span> <span class="n">X_test_feats</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;y_test&#39;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">net</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">best_net</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Train a two-layer neural network on image features. You may want to    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># cross-validate various parameters as in previous sections. Store your best   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># model in the best_net variable.                                              #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">learning_rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">2.75e-2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">regularization_strengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl"><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">itertools</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span> <span class="n">regularization_strengths</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">},</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">best_val_acc</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span> <span class="o">&gt;</span> <span class="n">best_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_val</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_net</span> <span class="o">=</span> <span class="n">model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print out results.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lr </span><span class="si">%e</span><span class="s1"> reg </span><span class="si">%e</span><span class="s1"> val accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best validation accuracy achieved during cross-validation: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">best_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">lr 1.000000e-02 reg 1.000000e-06 val accuracy: 0.517000
</span></span><span class="line"><span class="cl">lr 1.000000e-02 reg 1.000000e-05 val accuracy: 0.516000
</span></span><span class="line"><span class="cl">lr 1.000000e-02 reg 1.000000e-04 val accuracy: 0.516000
</span></span><span class="line"><span class="cl">lr 1.583333e-02 reg 1.000000e-06 val accuracy: 0.534000
</span></span><span class="line"><span class="cl">lr 1.583333e-02 reg 1.000000e-05 val accuracy: 0.528000
</span></span><span class="line"><span class="cl">lr 1.583333e-02 reg 1.000000e-04 val accuracy: 0.532000
</span></span><span class="line"><span class="cl">lr 2.166667e-02 reg 1.000000e-06 val accuracy: 0.555000
</span></span><span class="line"><span class="cl">lr 2.166667e-02 reg 1.000000e-05 val accuracy: 0.557000
</span></span><span class="line"><span class="cl">lr 2.166667e-02 reg 1.000000e-04 val accuracy: 0.543000
</span></span><span class="line"><span class="cl">lr 2.750000e-02 reg 1.000000e-06 val accuracy: 0.570000
</span></span><span class="line"><span class="cl">lr 2.750000e-02 reg 1.000000e-05 val accuracy: 0.566000
</span></span><span class="line"><span class="cl">lr 2.750000e-02 reg 1.000000e-04 val accuracy: 0.557000
</span></span><span class="line"><span class="cl">best validation accuracy achieved during cross-validation: 0.570000</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="assignment-2" class="heading-element"><span>Assignment 2</span>
  <a href="#assignment-2" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><h3 id="q1-multi-layer-fully-connected-neural-networks" class="heading-element"><span>Q1: Multi-Layer Fully Connected Neural Networks</span>
  <a href="#q1-multi-layer-fully-connected-neural-networks" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><h4 id="todo-fc_net" class="heading-element"><span>TODO: fc_net</span>
  <a href="#todo-fc_net" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><h5 id="__init__" class="heading-element"><span>__init__</span>
  <a href="#__init__" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h5><p>先解释一下 __init__ 的参数。</p>
<p>参数翻译：</p>
<ul>
<li><code>hidden_dims</code>: 一个整数列表，指定每个隐藏层的大小（神经元数量）</li>
<li><code>input_dim</code>: 一个整数，指定输入层的维度大小（默认值为3x32x32，适用于32x32的RGB图像）</li>
<li><code>num_classes</code>: 一个整数，指定需要分类的类别数量（默认为10类）</li>
<li><code>dropout_keep_ratio</code>: 丢弃强度，一个0到1之间的标量，表示dropout保留神经元的比例。如果等于1则表示不使用dropout</li>
<li><code>normalization</code>: 指定网络使用的归一化类型，可选值包括：
<ul>
<li>batchnorm: 批量归一化</li>
<li>layernorm: 层归一化</li>
<li>None: 不使用归一化（默认值）</li>
</ul>
</li>
<li><code>reg</code>: 一个标量，表示L2正则化的强度</li>
<li><code>weight_scale</code>: 一个标量，表示权重初始化时使用的正态分布标准差</li>
<li><code>dtype</code>: numpy数据类型对象。所有计算都将使用此数据类型：</li>
<li>float32: 运算更快但精度较低</li>
<li>float64: 适用于数值梯度检查，精度更高</li>
<li><code>seed</code>: 随机种子。如果不为None，则传递给dropout层使其具有确定性，便于进行梯度检查</li>
</ul>
<p>然后开始初始化参数，</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">        <span class="c1"># 获取所有层的维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 初始化每一层的参数</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 初始化权重矩阵,使用正态分布</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">weight_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 初始化偏置向量为0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果使用批归一化且不是最后一层，最后一层不需要正则化参数</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># gamma初始化为1</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># beta初始化为0 </span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span></span></span></code></pre></td></tr></table>
</div>
</div><p>注意最后一层不需要正则化参数，因为模型里最后一层是 softmax，本身就会归一化到 $0 \sim 1$</p>
<h5 id="loss" class="heading-element"><span>loss</span>
  <a href="#loss" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h5><p>把 assignment1 的 layers.py 先抄过来，然后写一下前向传播和反向传播。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Compute loss and gradient for the fully connected net.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - X: Array of input data of shape (N, d_1, ..., d_k)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    If y is None, then run a test-time forward pass of the model and return:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - scores: Array of shape (N, C) giving classification scores, where
</span></span></span><span class="line"><span class="cl"><span class="s2">        scores[i, c] is the classification score for X[i] and class c.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    If y is not None, then run a training-time forward and backward pass and
</span></span></span><span class="line"><span class="cl"><span class="s2">    return a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - loss: Scalar value giving the loss
</span></span></span><span class="line"><span class="cl"><span class="s2">    - grads: Dictionary with the same keys as self.params, mapping parameter
</span></span></span><span class="line"><span class="cl"><span class="s2">        names to gradients of the loss with respect to those parameters.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="s2">&#34;test&#34;</span> <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&#34;train&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Set train/test mode for batchnorm params and dropout param since they</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># behave differently during training and testing.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dropout</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_param</span><span class="p">[</span><span class="s2">&#34;mode&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">==</span> <span class="s2">&#34;batchnorm&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">bn_param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_params</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn_param</span><span class="p">[</span><span class="s2">&#34;mode&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the forward pass for the fully connected net, computing  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the class scores for X and storing them in the scores variable.          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># When using dropout, you&#39;ll need to pass self.dropout_param to each       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># dropout forward pass.                                                    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># When using batch normalization, you&#39;ll need to pass self.bn_params[0] to #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the forward pass for the first batch normalization layer, pass           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># self.bn_params[1] to the forward pass for the second batch normalization #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># layer, etc.                                                              #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 用一个变量保存上一层的输出</span>
</span></span><span class="line"><span class="cl">    <span class="n">layer_input</span> <span class="o">=</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">    <span class="n">caches</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 对前面 L - 1 层进行操作，因为最后一层的操作和前面的不一样</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算affine层的输出</span>
</span></span><span class="line"><span class="cl">        <span class="n">affine_out</span><span class="p">,</span> <span class="n">affine_cache</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算relu层的输出</span>
</span></span><span class="line"><span class="cl">        <span class="n">relu_out</span><span class="p">,</span> <span class="n">relu_cache</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">affine_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 保存cache</span>
</span></span><span class="line"><span class="cl">        <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;affine_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">affine_cache</span>
</span></span><span class="line"><span class="cl">        <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;relu_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">relu_cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新layer_input</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer_input</span> <span class="o">=</span> <span class="n">relu_out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 最后一层的操作</span>
</span></span><span class="line"><span class="cl">    <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">scores</span><span class="p">,</span> <span class="n">affine_cache</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">layer_input</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;affine_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">affine_cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># If test mode return early.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;test&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">scores</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for the fully connected net. Store the #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># loss in the loss variable and gradients in the grads dictionary. Compute #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># data loss using softmax, and make sure that grads[k] holds the gradients #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># for self.params[k]. Don&#39;t forget to add L2 regularization!               #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># When using batch/layer normalization, you don&#39;t need to regularize the   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># scale and shift parameters.                                              #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="p">,</span> <span class="n">dscores</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 先计算最后一层的梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="n">affine_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;affine_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="n">d_relu_out</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">affine_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">    <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算前面的梯度</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">affine_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;affine_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">relu_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="s1">&#39;relu_cache&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 先计算relu层的梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_affine_out</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">d_relu_out</span><span class="p">,</span> <span class="n">relu_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 再计算affine层的梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_relu_out</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">d_affine_out</span><span class="p">,</span> <span class="n">affine_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 保存梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dW</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="c1"># 加上正则化项</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span></span></span></code></pre></td></tr></table>
</div>
</div><p>初始化误差检查：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Running check with reg =  0
</span></span><span class="line"><span class="cl">Initial loss:  2.3004790897684924
</span></span><span class="line"><span class="cl">W1 relative error: 1.4839895075340334e-07
</span></span><span class="line"><span class="cl">W2 relative error: 2.212047929031316e-05
</span></span><span class="line"><span class="cl">W3 relative error: 3.5272528081494203e-07
</span></span><span class="line"><span class="cl">b1 relative error: 5.376386325179258e-09
</span></span><span class="line"><span class="cl">b2 relative error: 2.085654276112763e-09
</span></span><span class="line"><span class="cl">b3 relative error: 5.7957243458479405e-11
</span></span><span class="line"><span class="cl">Running check with reg =  3.14
</span></span><span class="line"><span class="cl">Initial loss:  7.052114776533016
</span></span><span class="line"><span class="cl">W1 relative error: 3.904542008453064e-09
</span></span><span class="line"><span class="cl">W2 relative error: 6.86942277940646e-08
</span></span><span class="line"><span class="cl">W3 relative error: 2.131129859578198e-08
</span></span><span class="line"><span class="cl">b1 relative error: 1.475242847895799e-08
</span></span><span class="line"><span class="cl">b2 relative error: 1.7223751746766738e-09
</span></span><span class="line"><span class="cl">b3 relative error: 1.5702714832602802e-10</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-use-a-three-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale" class="heading-element"><span>TODO: Use a three-layer Net to overfit 50 training examples by tweaking just the learning rate and initialization scale.</span>
  <a href="#todo-use-a-three-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>使用一个三层神经网络，仅通过调整学习率和初始化规模，对 50 个训练样本进行过拟合。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># TODO: Use a three-layer Net to overfit 50 training examples by </span>
</span></span><span class="line"><span class="cl"><span class="c1"># tweaking just the learning rate and initialization scale.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_train</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl"><span class="n">small_data</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;X_train&#34;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&#34;X_train&#34;</span><span class="p">][:</span><span class="n">num_train</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;y_train&#34;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&#34;y_train&#34;</span><span class="p">][:</span><span class="n">num_train</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;X_val&#34;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&#34;X_val&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;y_val&#34;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s2">&#34;y_val&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># weight_scale = 1e-2   # Experiment with this!</span>
</span></span><span class="line"><span class="cl"><span class="c1"># learning_rate = 1e-4  # Experiment with this!</span>
</span></span><span class="line"><span class="cl"><span class="n">weight_scale</span> <span class="o">=</span> <span class="mf">5e-2</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-3</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">small_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">print_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">update_rule</span><span class="o">=</span><span class="s2">&#34;sgd&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">loss_history</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Training loss history&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Iteration&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Training loss&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(Iteration 1 / 40) loss: 28.158169
</span></span><span class="line"><span class="cl">(Epoch 0 / 20) train acc: 0.280000; val_acc: 0.113000
</span></span><span class="line"><span class="cl">(Epoch 1 / 20) train acc: 0.220000; val_acc: 0.124000
</span></span><span class="line"><span class="cl">(Epoch 2 / 20) train acc: 0.320000; val_acc: 0.103000
</span></span><span class="line"><span class="cl">(Epoch 3 / 20) train acc: 0.620000; val_acc: 0.143000
</span></span><span class="line"><span class="cl">(Epoch 4 / 20) train acc: 0.740000; val_acc: 0.136000
</span></span><span class="line"><span class="cl">(Epoch 5 / 20) train acc: 0.800000; val_acc: 0.133000
</span></span><span class="line"><span class="cl">(Iteration 11 / 40) loss: 0.497961
</span></span><span class="line"><span class="cl">(Epoch 6 / 20) train acc: 0.960000; val_acc: 0.129000
</span></span><span class="line"><span class="cl">(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.130000
</span></span><span class="line"><span class="cl">(Epoch 8 / 20) train acc: 0.920000; val_acc: 0.106000
</span></span><span class="line"><span class="cl">(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.117000
</span></span><span class="line"><span class="cl">(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Iteration 21 / 40) loss: 0.045432
</span></span><span class="line"><span class="cl">(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.119000
</span></span><span class="line"><span class="cl">(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.119000
</span></span><span class="line"><span class="cl">(Iteration 31 / 40) loss: 0.015703
</span></span><span class="line"><span class="cl">(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.119000
</span></span><span class="line"><span class="cl">(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.120000
</span></span><span class="line"><span class="cl">(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.120000</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/6.png" alt="/image/ML/CS231n/6.png" srcset="/image/ML/CS231n/6.png?size=small, /image/ML/CS231n/6.png?size=medium 1.5x, /image/ML/CS231n/6.png?size=large 2x" data-title="/image/ML/CS231n/6.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-use-a-five-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale" class="heading-element"><span>TODO: Use a five-layer Net to overfit 50 training examples by tweaking just the learning rate and initialization scale.</span>
  <a href="#todo-use-a-five-layer-net-to-overfit-50-training-examples-by-tweaking-just-the-learning-rate-and-initialization-scale" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>使用一个五层神经网络，仅通过调整学习率和初始化规模，对 50 个训练样本进行过拟合。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># TODO: Use a five-layer Net to overfit 50 training examples by </span>
</span></span><span class="line"><span class="cl"><span class="c1"># tweaking just the learning rate and initialization scale.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_train</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl"><span class="n">small_data</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;X_train&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;X_train&#39;</span><span class="p">][:</span><span class="n">num_train</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;y_train&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y_train&#39;</span><span class="p">][:</span><span class="n">num_train</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;X_val&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;X_val&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;y_val&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;y_val&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># learning_rate = 2e-3  # Experiment with this!</span>
</span></span><span class="line"><span class="cl"><span class="c1"># weight_scale = 1e-5   # Experiment with this!</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
</span></span><span class="line"><span class="cl"><span class="n">weight_scale</span> <span class="o">=</span> <span class="mf">1e-1</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">small_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">print_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">update_rule</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">loss_history</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training loss history&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(Iteration 1 / 40) loss: 146.090563
</span></span><span class="line"><span class="cl">(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.109000
</span></span><span class="line"><span class="cl">(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.107000
</span></span><span class="line"><span class="cl">(Epoch 2 / 20) train acc: 0.320000; val_acc: 0.121000
</span></span><span class="line"><span class="cl">(Epoch 3 / 20) train acc: 0.680000; val_acc: 0.109000
</span></span><span class="line"><span class="cl">(Epoch 4 / 20) train acc: 0.920000; val_acc: 0.130000
</span></span><span class="line"><span class="cl">(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.138000
</span></span><span class="line"><span class="cl">(Iteration 11 / 40) loss: 0.118771
</span></span><span class="line"><span class="cl">(Epoch 6 / 20) train acc: 0.980000; val_acc: 0.129000
</span></span><span class="line"><span class="cl">(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.135000
</span></span><span class="line"><span class="cl">(Epoch 8 / 20) train acc: 1.000000; val_acc: 0.130000
</span></span><span class="line"><span class="cl">(Epoch 9 / 20) train acc: 1.000000; val_acc: 0.130000
</span></span><span class="line"><span class="cl">(Epoch 10 / 20) train acc: 1.000000; val_acc: 0.130000
</span></span><span class="line"><span class="cl">(Iteration 21 / 40) loss: 0.000431
</span></span><span class="line"><span class="cl">(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Iteration 31 / 40) loss: 0.000366
</span></span><span class="line"><span class="cl">(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.131000
</span></span><span class="line"><span class="cl">(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.131000</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/7.png" alt="/image/ML/CS231n/7.png" srcset="/image/ML/CS231n/7.png?size=small, /image/ML/CS231n/7.png?size=medium 1.5x, /image/ML/CS231n/7.png?size=large 2x" data-title="/image/ML/CS231n/7.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-1-5" class="heading-element"><span>Inline Question 1</span>
  <a href="#inline-question-1-5" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Did you notice anything about the comparative difficulty of training the three-layer network vs. training the five-layer network? In particular, based on your experience, which network seemed more sensitive to the initialization scale? Why do you think that is the case?</p>
<p>Answer:
[FILL THIS IN]</p>
<p>你注意到训练三层网络与训练五层网络在难度上的比较了吗？具体来说，根据你的经验，哪个网络对初始化规模更敏感？你认为为什么会这样？</p>
<p>答案：五层的更难。原因如下：</p>
<ol>
<li>
<p>梯度消失/爆炸问题: 由于五层网络更深,信号需要传播更多层,使得梯度在反向传播时更容易出现消失或爆炸。如果初始化尺度不合适,这个问题会更加严重。</p>
</li>
<li>
<p>参数规模: 五层网络的参数数量更多,需要一个更合适的初始化尺度来保持各层激活值在合理范围内。初始化尺度过大或过小都会导致训练困难。</p>
</li>
<li>
<p>优化难度: 更深的网络意味着更复杂的损失曲面,对初始点的选择(由初始化决定)更加敏感。不恰当的初始化可能使网络陷入不良的局部最优。</p>
</li>
</ol>
<p>到目前为止,我们一直使用的是普通的随机梯度下降(SGD)作为更新规则。更复杂的更新规则可以使深度网络的训练变得更容易。我们将实现几个最常用的更新规则,并将它们与普通的 SGD 进行比较。
具体可以看<a href="https://cs231n.github.io/neural-networks-3/#sgd"target="_blank" rel="external nofollow noopener noreferrer">官方讲义</a></p>
<h4 id="todo-sgd_momentum" class="heading-element"><span>TODO: sgd_momentum</span>
  <a href="#todo-sgd_momentum" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>根据讲义公式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="c1"># integrate velocity</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">+=</span> <span class="n">v</span> <span class="c1"># integrate position</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sgd_momentum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Performs stochastic gradient descent with momentum.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    config format:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - learning_rate: Scalar learning rate.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - momentum: Scalar between 0 and 1 giving the momentum value.
</span></span></span><span class="line"><span class="cl"><span class="s2">      Setting momentum = 0 reduces to sgd.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - velocity: A numpy array of the same shape as w and dw used to store a
</span></span></span><span class="line"><span class="cl"><span class="s2">      moving average of the gradients.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;momentum&#34;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;velocity&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the momentum update formula. Store the updated value in #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the next_w variable. You should also use and update the velocity v.     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;momentum&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">dw</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="p">[</span><span class="s2">&#34;velocity&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span></span></span></code></pre></td></tr></table>
</div>
</div><p>误差</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">next_w error:  8.882347033505819e-09
</span></span><span class="line"><span class="cl">velocity error:  4.269287743278663e-09</span></span></code></pre></td></tr></table>
</div>
</div><p>对比</p>
<p><img loading="lazy" src="/image/ML/CS231n/8.png" alt="/image/ML/CS231n/8.png" srcset="/image/ML/CS231n/8.png?size=small, /image/ML/CS231n/8.png?size=medium 1.5x, /image/ML/CS231n/8.png?size=large 2x" data-title="/image/ML/CS231n/8.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-rmsprop" class="heading-element"><span>TODO: RMSProp</span>
  <a href="#todo-rmsprop" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>根据讲义公式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">cache</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">cache</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rmsprop</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Uses the RMSProp update rule, which uses a moving average of squared
</span></span></span><span class="line"><span class="cl"><span class="s2">    gradient values to set adaptive per-parameter learning rates.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    config format:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - learning_rate: Scalar learning rate.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared
</span></span></span><span class="line"><span class="cl"><span class="s2">      gradient cache.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Moving average of second moments of gradients.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;decay_rate&#34;</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;epsilon&#34;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;cache&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the RMSprop update formula, storing the next value of w #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># in the next_w variable. Don&#39;t forget to update cache value stored in    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># config[&#39;cache&#39;].                                                        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;cache&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;decay_rate&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">cache</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;decay_rate&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;epsilon&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="p">[</span><span class="s2">&#34;cache&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span></span></span></code></pre></td></tr></table>
</div>
</div><p>误差</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">next_w error:  9.524687511038133e-08
</span></span><span class="line"><span class="cl">cache error:  2.6477955807156126e-09</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-adam" class="heading-element"><span>TODO: Adam</span>
  <a href="#todo-adam" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>根据讲义公式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># t is your iteration counter going from 1 to infinity</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>
</span></span><span class="line"><span class="cl"><span class="n">mt</span> <span class="o">=</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vt</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">mt</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vt</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">adam</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Uses the Adam update rule, which incorporates moving averages of both the
</span></span></span><span class="line"><span class="cl"><span class="s2">    gradient and its square and a bias correction term.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    config format:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - learning_rate: Scalar learning rate.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta1: Decay rate for moving average of first moment of gradient.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta2: Decay rate for moving average of second moment of gradient.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - m: Moving average of gradient.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - v: Moving average of squared gradient.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - t: Iteration number.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;beta1&#34;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;beta2&#34;</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;epsilon&#34;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;m&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;v&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&#34;t&#34;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the Adam update formula, storing the next value of w in #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the next_w variable. Don&#39;t forget to update the m, v, and t variables   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># stored in config.                                                       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># NOTE: In order to match the reference output, please modify t _before_  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># using it in any calculations.                                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;t&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta1&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;m&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta1&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">dw</span>
</span></span><span class="line"><span class="cl">    <span class="n">mt</span> <span class="o">=</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta1&#34;</span><span class="p">]</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">v</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta2&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;v&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta2&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">vt</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;beta2&#34;</span><span class="p">]</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">mt</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vt</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s2">&#34;epsilon&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="p">[</span><span class="s2">&#34;t&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="p">[</span><span class="s2">&#34;m&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="p">[</span><span class="s2">&#34;v&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span></span></span></code></pre></td></tr></table>
</div>
</div><p>误差</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">next_w error:  1.1395691798535431e-07
</span></span><span class="line"><span class="cl">v error:  4.208314038113071e-09
</span></span><span class="line"><span class="cl">m error:  4.214963193114416e-09</span></span></code></pre></td></tr></table>
</div>
</div><p>整体对比：</p>
<p><img loading="lazy" src="/image/ML/CS231n/9.png" alt="/image/ML/CS231n/9.png" srcset="/image/ML/CS231n/9.png?size=small, /image/ML/CS231n/9.png?size=medium 1.5x, /image/ML/CS231n/9.png?size=large 2x" data-title="/image/ML/CS231n/9.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-2-3" class="heading-element"><span>Inline Question 2</span>
  <a href="#inline-question-2-3" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cache += dw**2
</span></span><span class="line"><span class="cl">w += - learning_rate * dw / (np.sqrt(cache) + eps)</span></span></code></pre></td></tr></table>
</div>
</div><p>John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?</p>
<p>Answer:
[FILL THIS IN]</p>
<p>AdaGrad和Adam一样,是一种基于每个参数的优化方法,它使用以下更新规则:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cache += dw**2
</span></span><span class="line"><span class="cl">w += - learning_rate * dw / (np.sqrt(cache) + eps)</span></span></code></pre></td></tr></table>
</div>
</div><p>John注意到当他使用AdaGrad训练网络时,更新变得非常小,他的网络学习速度变慢。根据你对AdaGrad更新规则的理解,你认为为什么更新会变得很小?Adam会有同样的问题吗?</p>
<p>Answer:</p>
<p>AdaGrad的更新会变得很小的原因是:</p>
<ol>
<li>cache是单调递增的 - 因为它不断累加平方梯度(dw**2),这些都是非负值</li>
<li>随着训练的进行,cache会越来越大</li>
<li>由于更新规则中cache在分母位置(w += -lr * dw / sqrt(cache)),cache的增大会导致更新步长不断减小</li>
<li>最终会导致参数更新几乎停滞,模型难以继续学习</li>
</ol>
<p>Adam不会有这个问题,因为:</p>
<ol>
<li>Adam使用动量和RMSprop的思想,对梯度的一阶矩和二阶矩都采用指数移动平均</li>
<li>这意味着旧的梯度信息会逐渐&quot;衰减&rdquo;,而不是像AdaGrad那样永久累积</li>
<li>因此Adam能够保持相对稳定的更新步长,避免了学习完全停滞的问题</li>
</ol>
<p>这就是为什么Adam通常比AdaGrad表现更好,特别是在训练深度神经网络时。</p>
<h4 id="todo-train-a-good-model" class="heading-element"><span>TODO: Train a Good Model!</span>
  <a href="#todo-train-a-good-model" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>建一个四层每层 $100$ 的神经网络。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># find batch/layer normalization and dropout useful. Store your best model in  #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the best_model variable.                                                     #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight_scale</span><span class="o">=</span><span class="mf">5e-2</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">update_rule</span><span class="o">=</span><span class="s2">&#34;adam&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">optim_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;learning_rate&#34;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">solver</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">best_model</span> <span class="o">=</span> <span class="n">model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                              END OF YOUR CODE                                #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span></span></span></code></pre></td></tr></table>
</div>
</div><p>输出</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(Iteration 1 / 4900) loss: 12.396409
</span></span><span class="line"><span class="cl">(Epoch 0 / 10) train acc: 0.112000; val_acc: 0.093000
</span></span><span class="line"><span class="cl">(Iteration 11 / 4900) loss: 3.950364
</span></span><span class="line"><span class="cl">(Iteration 21 / 4900) loss: 2.959177
</span></span><span class="line"><span class="cl">(Iteration 31 / 4900) loss: 2.403476
</span></span><span class="line"><span class="cl">(Iteration 41 / 4900) loss: 2.503231
</span></span><span class="line"><span class="cl">(Iteration 51 / 4900) loss: 2.397866
</span></span><span class="line"><span class="cl">(Iteration 61 / 4900) loss: 2.213649
</span></span><span class="line"><span class="cl">(Iteration 71 / 4900) loss: 2.026688
</span></span><span class="line"><span class="cl">(Iteration 81 / 4900) loss: 1.767392
</span></span><span class="line"><span class="cl">(Iteration 91 / 4900) loss: 2.077030
</span></span><span class="line"><span class="cl">(Iteration 101 / 4900) loss: 2.052979
</span></span><span class="line"><span class="cl">(Iteration 111 / 4900) loss: 1.921003
</span></span><span class="line"><span class="cl">(Iteration 121 / 4900) loss: 1.927804
</span></span><span class="line"><span class="cl">(Iteration 131 / 4900) loss: 1.933639
</span></span><span class="line"><span class="cl">(Iteration 141 / 4900) loss: 1.899896
</span></span><span class="line"><span class="cl">(Iteration 151 / 4900) loss: 1.943097
</span></span><span class="line"><span class="cl">(Iteration 161 / 4900) loss: 1.765048
</span></span><span class="line"><span class="cl">(Iteration 171 / 4900) loss: 1.771318
</span></span><span class="line"><span class="cl">(Iteration 181 / 4900) loss: 1.850234
</span></span><span class="line"><span class="cl">(Iteration 191 / 4900) loss: 1.610974
</span></span><span class="line"><span class="cl">(Iteration 201 / 4900) loss: 1.875304
</span></span><span class="line"><span class="cl">(Iteration 211 / 4900) loss: 1.746618
</span></span><span class="line"><span class="cl">(Iteration 221 / 4900) loss: 1.655409
</span></span><span class="line"><span class="cl">(Iteration 231 / 4900) loss: 1.810486
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">(Iteration 4871 / 4900) loss: 1.150006
</span></span><span class="line"><span class="cl">(Iteration 4881 / 4900) loss: 1.142224
</span></span><span class="line"><span class="cl">(Iteration 4891 / 4900) loss: 1.431774
</span></span><span class="line"><span class="cl">(Epoch 10 / 10) train acc: 0.545000; val_acc: 0.495000</span></span></code></pre></td></tr></table>
</div>
</div><p>准确率，验证集 50% 达标了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Validation set accuracy:  0.502
</span></span><span class="line"><span class="cl">Test set accuracy:  0.483</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="q2-batch-normalization" class="heading-element"><span>Q2: Batch Normalization</span>
  <a href="#q2-batch-normalization" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p><a href="https://arxiv.org/pdf/1502.03167"target="_blank" rel="external nofollow noopener noreferrer">参考论文</a></p>
<p>核心公式：</p>
<p><img loading="lazy" src="/image/ML/CS231n/bn1.png" alt="/image/ML/CS231n/bn1.png" srcset="/image/ML/CS231n/bn1.png?size=small, /image/ML/CS231n/bn1.png?size=medium 1.5x, /image/ML/CS231n/bn1.png?size=large 2x" data-title="/image/ML/CS231n/bn1.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p><img loading="lazy" src="/image/ML/CS231n/bn2.png" alt="/image/ML/CS231n/bn2.png" srcset="/image/ML/CS231n/bn2.png?size=small, /image/ML/CS231n/bn2.png?size=medium 1.5x, /image/ML/CS231n/bn2.png?size=large 2x" data-title="/image/ML/CS231n/bn2.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-batchnorm_forward" class="heading-element"><span>TODO: batchnorm_forward</span>
  <a href="#todo-batchnorm_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Forward pass for batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    During training the sample mean and (uncorrected) sample variance are
</span></span></span><span class="line"><span class="cl"><span class="s2">    computed from minibatch statistics and used to normalize the incoming data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    During training we also keep an exponentially decaying running mean of the
</span></span></span><span class="line"><span class="cl"><span class="s2">    mean and variance of each feature, and these averages are used to normalize
</span></span></span><span class="line"><span class="cl"><span class="s2">    data at test-time.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    At each timestep we update the running averages for mean and variance using
</span></span></span><span class="line"><span class="cl"><span class="s2">    an exponential decay based on the momentum parameter:
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
</span></span></span><span class="line"><span class="cl"><span class="s2">    running_var = momentum * running_var + (1 - momentum) * sample_var
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note that the batch normalization paper suggests a different test-time
</span></span></span><span class="line"><span class="cl"><span class="s2">    behavior: they compute sample mean and variance for each feature using a
</span></span></span><span class="line"><span class="cl"><span class="s2">    large number of training images rather than using a running average. For
</span></span></span><span class="line"><span class="cl"><span class="s2">    this implementation we have chosen to use running averages instead since
</span></span></span><span class="line"><span class="cl"><span class="s2">    they do not require an additional estimation step; the torch7
</span></span></span><span class="line"><span class="cl"><span class="s2">    implementation of batch normalization also uses running averages.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Input:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Data of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gamma: Scale parameter of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta: Shift paremeter of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - bn_param: Dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - mode: &#39;train&#39; or &#39;test&#39;; required
</span></span></span><span class="line"><span class="cl"><span class="s2">      - eps: Constant for numeric stability
</span></span></span><span class="line"><span class="cl"><span class="s2">      - momentum: Constant for running mean / variance.
</span></span></span><span class="line"><span class="cl"><span class="s2">      - running_mean: Array of shape (D,) giving running mean of features
</span></span></span><span class="line"><span class="cl"><span class="s2">      - running_var Array of shape (D,) giving running variance of features
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: A tuple of values needed in the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">[</span><span class="s2">&#34;mode&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">eps</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;eps&#34;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">momentum</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;momentum&#34;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;running_mean&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">running_var</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;running_var&#34;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;train&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the training-time forward pass for batch norm.      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Use minibatch statistics to compute the mean and variance, use      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># these statistics to normalize the incoming data, and scale and      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># shift the normalized data using gamma and beta.                     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># You should store the output in the variable out. Any intermediates  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># that you need for the backward pass should be stored in the cache   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variable.                                                           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># You should also use your computed sample mean and variance together #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with the momentum variable to update the running mean and running   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variance, storing your result in the running_mean and running_var   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variables.                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Note that though you should be keeping track of the running         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variance, you should normalize the data based on the standard       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># deviation (square root of variance) instead!                        #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># might prove to be helpful.                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">running_mean</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">mean</span>
</span></span><span class="line"><span class="cl">        <span class="n">running_var</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">var</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                           END OF YOUR CODE                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;test&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the test-time forward pass for batch normalization. #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Use the running mean and variance to normalize the incoming data,   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># then scale and shift the normalized data using gamma and beta.      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Store the result in the out variable.                               #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># 归一化</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>  <span class="c1"># 计算输出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                          END OF YOUR CODE                           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid forward batchnorm mode &#34;</span><span class="si">%s</span><span class="s1">&#34;&#39;</span> <span class="o">%</span> <span class="n">mode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Store the updated running means back into bn_param</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_param</span><span class="p">[</span><span class="s2">&#34;running_mean&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_mean</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_param</span><span class="p">[</span><span class="s2">&#34;running_var&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_var</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><p>验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Before batch normalization:
</span></span><span class="line"><span class="cl">  means: [ -2.3814598  -13.18038246   1.91780462]
</span></span><span class="line"><span class="cl">  stds:  [27.18502186 34.21455511 37.68611762]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">After batch normalization (gamma=1, beta=0)
</span></span><span class="line"><span class="cl">  means: [ 1.33226763e-17 -3.94129174e-17  3.29597460e-17]
</span></span><span class="line"><span class="cl">  stds:  [0.99999999 1.         1.        ]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">After batch normalization (gamma= [1. 2. 3.] , beta= [11. 12. 13.] )
</span></span><span class="line"><span class="cl">  means: [11. 12. 13.]
</span></span><span class="line"><span class="cl">  stds:  [0.99999999 1.99999999 2.99999999]</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">After batch normalization (test-time):
</span></span><span class="line"><span class="cl">  means: [-0.03927354 -0.04349152 -0.10452688]
</span></span><span class="line"><span class="cl">  stds:  [1.01531428 1.01238373 0.97819988]</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-batchnorm_backward" class="heading-element"><span>TODO: batchnorm_backward</span>
  <a href="#todo-batchnorm_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>计算图太麻烦了，直接用链式求导。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">batchnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Backward pass for batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    For this implementation, you should write out a computation graph for
</span></span></span><span class="line"><span class="cl"><span class="s2">    batch normalization on paper and propagate gradients backward through
</span></span></span><span class="line"><span class="cl"><span class="s2">    intermediate nodes.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Variable of intermediates from batchnorm_forward.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to inputs x, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for batch normalization. Store the    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># results in the dx, dgamma, and dbeta variables.                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Referencing the original paper (https://arxiv.org/abs/1502.03167)       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># might prove to be helpful.                                              #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_norm</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">gamma</span>
</span></span><span class="line"><span class="cl">    <span class="n">dvar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dmean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">dvar</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx_norm</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">dvar</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">dmean</span> <span class="o">/</span> <span class="n">N</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">dx error:  1.7029261167605239e-09
</span></span><span class="line"><span class="cl">dgamma error:  7.420414216247087e-13
</span></span><span class="line"><span class="cl">dbeta error:  2.8795057655839487e-12</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-layer_utils" class="heading-element"><span>TODO: layer_utils</span>
  <a href="#todo-layer_utils" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>在每个 ReLU 激活函数前添加批量归一化层。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">affine_bn_relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">affine_out</span><span class="p">,</span><span class="n">affine_cache</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bn_out</span><span class="p">,</span><span class="n">bn_cache</span> <span class="o">=</span> <span class="n">batchnorm_forward</span><span class="p">(</span><span class="n">affine_out</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">relu_out</span><span class="p">,</span><span class="n">relu_cache</span> <span class="o">=</span> <span class="n">relu_forward</span><span class="p">(</span><span class="n">bn_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">affine_cache</span><span class="p">,</span> <span class="n">bn_cache</span><span class="p">,</span> <span class="n">relu_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">relu_out</span><span class="p">,</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">affine_bn_relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">affine_cache</span><span class="p">,</span> <span class="n">bn_cache</span><span class="p">,</span> <span class="n">relu_cache</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">drelu_out</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">relu_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dbn_out</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="n">batchnorm_backward</span><span class="p">(</span><span class="n">drelu_out</span><span class="p">,</span> <span class="n">bn_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dbn_out</span><span class="p">,</span> <span class="n">affine_cache</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span></span></code></pre></td></tr></table>
</div>
</div><p>验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Running check with reg =  0
</span></span><span class="line"><span class="cl">Initial loss:  2.2611955101340957
</span></span><span class="line"><span class="cl">W1 relative error: 1.10e-04
</span></span><span class="line"><span class="cl">W2 relative error: 2.85e-06
</span></span><span class="line"><span class="cl">W3 relative error: 3.92e-10
</span></span><span class="line"><span class="cl">b1 relative error: 2.22e-03
</span></span><span class="line"><span class="cl">b2 relative error: 2.22e-08
</span></span><span class="line"><span class="cl">b3 relative error: 4.78e-11
</span></span><span class="line"><span class="cl">beta1 relative error: 7.33e-09
</span></span><span class="line"><span class="cl">beta2 relative error: 1.07e-09
</span></span><span class="line"><span class="cl">gamma1 relative error: 7.47e-09
</span></span><span class="line"><span class="cl">gamma2 relative error: 2.41e-09
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Running check with reg =  3.14
</span></span><span class="line"><span class="cl">Initial loss:  6.996533220108303
</span></span><span class="line"><span class="cl">W1 relative error: 1.98e-06
</span></span><span class="line"><span class="cl">W2 relative error: 2.28e-06
</span></span><span class="line"><span class="cl">W3 relative error: 1.11e-08
</span></span><span class="line"><span class="cl">b1 relative error: 2.78e-09
</span></span><span class="line"><span class="cl">b2 relative error: 2.22e-08
</span></span><span class="line"><span class="cl">b3 relative error: 2.23e-10
</span></span><span class="line"><span class="cl">beta1 relative error: 6.32e-09
</span></span><span class="line"><span class="cl">beta2 relative error: 5.69e-09
</span></span><span class="line"><span class="cl">gamma1 relative error: 5.94e-09
</span></span><span class="line"><span class="cl">gamma2 relative error: 4.14e-09</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/10.png" alt="/image/ML/CS231n/10.png" srcset="/image/ML/CS231n/10.png?size=small, /image/ML/CS231n/10.png?size=medium 1.5x, /image/ML/CS231n/10.png?size=large 2x" data-title="/image/ML/CS231n/10.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p><img loading="lazy" src="/image/ML/CS231n/11.png" alt="/image/ML/CS231n/11.png" srcset="/image/ML/CS231n/11.png?size=small, /image/ML/CS231n/11.png?size=medium 1.5x, /image/ML/CS231n/11.png?size=large 2x" data-title="/image/ML/CS231n/11.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p><img loading="lazy" src="/image/ML/CS231n/12.png" alt="/image/ML/CS231n/12.png" srcset="/image/ML/CS231n/12.png?size=small, /image/ML/CS231n/12.png?size=medium 1.5x, /image/ML/CS231n/12.png?size=large 2x" data-title="/image/ML/CS231n/12.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-1-6" class="heading-element"><span>Inline Question 1:</span>
  <a href="#inline-question-1-6" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Describe the results of this experiment. How does the weight initialization scale affect models with/without batch normalization differently, and why?</p>
<p>根据图表结果，我可以帮你分析批量归一化(Batch Normalization)对权重初始化尺度的影响:</p>
<p>从实验结果可以观察到以下几点:</p>
<ol>
<li><strong>无批量归一化的网络</strong>:</li>
</ol>
<ul>
<li>对权重初始化尺度非常敏感</li>
<li>当初始化尺度过大或过小时,性能都会显著下降</li>
<li>只在一个很窄的权重初始化范围内表现良好</li>
</ul>
<ol start="2">
<li><strong>有批量归一化的网络</strong>:</li>
</ol>
<ul>
<li>对权重初始化尺度的依赖性明显降低</li>
<li>在很宽的权重初始化范围内都能保持稳定的性能</li>
<li>即使在较大的初始化尺度下也能达到较好的训练和验证准确率</li>
</ul>
<ol start="3">
<li><strong>原因分析</strong>:</li>
</ol>
<ul>
<li>批量归一化通过标准化每一层的输出,使得网络层之间的数据分布保持稳定</li>
<li>这种标准化效果减弱了初始权重带来的影响,因为无论初始权重如何,经过批量归一化后的输出都会被调整到类似的分布</li>
<li>这使得网络训练更加稳定,不容易受到权重初始化的影响</li>
</ul>
<p>这个实验很好地展示了批量归一化的一个重要优势:它能够降低网络对权重初始化的敏感度,使得训练更加稳定和鲁棒。</p>
<h4 id="inline-question-2-4" class="heading-element"><span>Inline Question 2:</span>
  <a href="#inline-question-2-4" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Describe the results of this experiment. What does this imply about the relationship between batch normalization and batch size? Why is this relationship observed?</p>
<p>描述本次实验的结果。这对批量归一化和批量大小之间的关系有何启示？为何会观察到这种关系？</p>
<p>从实验结果图中我们可以观察到以下几点:</p>
<ol>
<li>
<p>当使用较小的批量大小(batch_size=5,10)时,批量归一化的性能明显下降,训练和验证准确率都较低且波动较大。</p>
</li>
<li>
<p>当使用较大的批量大小(batch_size=50)时,批量归一化表现最好,训练更稳定,准确率更高。</p>
</li>
</ol>
<p>这说明批量归一化的效果与批量大小有很强的相关性,原因是:</p>
<ol>
<li>
<p>批量归一化依赖于每个mini-batch内的统计量(均值和方差)来进行归一化。当批量太小时:</p>
<ul>
<li>计算的统计量波动较大,不能很好地代表整体数据分布</li>
<li>这种不稳定的归一化会影响网络训练</li>
</ul>
</li>
<li>
<p>较大的批量大小可以:</p>
<ul>
<li>提供更稳定可靠的统计估计</li>
<li>使归一化效果更接近整体数据分布</li>
<li>减少训练过程中的噪声</li>
</ul>
</li>
<li>
<p>这也解释了为什么在实际应用中,批量归一化通常需要相对较大的batch size(如32或64)才能发挥最佳效果。</p>
</li>
</ol>
<p>这个实验结果强调了在使用批量归一化时需要合理选择批量大小,以在计算效率和归一化效果之间取得平衡。</p>
<h4 id="layer-normalization" class="heading-element"><span>layer normalization</span>
  <a href="#layer-normalization" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><a href="https://arxiv.org/pdf/1607.06450"target="_blank" rel="external nofollow noopener noreferrer">参考论文</a></p>
<p>层归一化，简单来说，就是不受 batch_size 的影响。</p>
<h4 id="todo-layernorm_forward" class="heading-element"><span>TODO: layernorm_forward</span>
  <a href="#todo-layernorm_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">layernorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">ln_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Forward pass for layer normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    During both training and test-time, the incoming data is normalized per data-point,
</span></span></span><span class="line"><span class="cl"><span class="s2">    before being scaled by gamma and beta parameters identical to that of batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note that in contrast to batch normalization, the behavior during train and test-time for
</span></span></span><span class="line"><span class="cl"><span class="s2">    layer normalization are identical, and we do not need to keep track of running averages
</span></span></span><span class="line"><span class="cl"><span class="s2">    of any sort.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Input:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Data of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gamma: Scale parameter of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta: Shift paremeter of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - ln_param: Dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - eps: Constant for numeric stability
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: A tuple of values needed in the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">eps</span> <span class="o">=</span> <span class="n">ln_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;eps&#34;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the training-time forward pass for layer norm.          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Normalize the incoming data, and scale and  shift the normalized data   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#  using gamma and beta.                                                  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: this can be done by slightly modifying your training-time         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># implementation of  batch normalization, and inserting a line or two of  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># well-placed code. In particular, can you think of any matrix            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># transformations you could perform, that would enable you to copy over   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the batch norm code and leave it almost unchanged?                      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 沿特征维度计算均值和方差（axis=1）</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 归一化处理</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 缩放和平移</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 缓存反向传播需要的中间变量</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><p>验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Before layer normalization:
</span></span><span class="line"><span class="cl">  means: [-59.06673243 -47.60782686 -43.31137368 -26.40991744]
</span></span><span class="line"><span class="cl">  stds:  [10.07429373 28.39478981 35.28360729  4.01831507]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">After layer normalization (gamma=1, beta=0)
</span></span><span class="line"><span class="cl">  means: [ 4.81096644e-16  0.00000000e+00  0.00000000e+00 -2.96059473e-16]
</span></span><span class="line"><span class="cl">  stds:  [0.99999995 0.99999999 1.         0.99999969]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">After layer normalization (gamma= [3. 3. 3.] , beta= [5. 5. 5.] )
</span></span><span class="line"><span class="cl">  means: [5. 5. 5. 5.]
</span></span><span class="line"><span class="cl">  stds:  [2.99999985 2.99999998 2.99999999 2.99999907]</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-layernorm_backward" class="heading-element"><span>TODO: layernorm_backward</span>
  <a href="#todo-layernorm_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">layernorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Backward pass for layer normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    For this implementation, you can heavily rely on the work you&#39;ve done already
</span></span></span><span class="line"><span class="cl"><span class="s2">    for batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Variable of intermediates from layernorm_forward.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to inputs x, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for layer norm.                       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: this can be done by slightly modifying your training-time         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># implementation of batch normalization. The hints to the forward pass    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># still apply!                                                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算dgamma和dbeta</span>
</span></span><span class="line"><span class="cl">    <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算dx_norm</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_norm</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">gamma</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算方差梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dvar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span><span class="o">**-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算均值梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dmean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> \
</span></span><span class="line"><span class="cl">            <span class="n">dvar</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算最终输入梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">dx_norm</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="n">dvar</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">D</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">dmean</span> <span class="o">/</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span></span></span></code></pre></td></tr></table>
</div>
</div><p>验证：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">dx error:  1.433615657860454e-09
</span></span><span class="line"><span class="cl">dgamma error:  4.519489546032799e-12
</span></span><span class="line"><span class="cl">dbeta error:  2.276445013433725e-12</span></span></code></pre></td></tr></table>
</div>
</div><p>对比：</p>
<p><img loading="lazy" src="/image/ML/CS231n/13.png" alt="/image/ML/CS231n/13.png" srcset="/image/ML/CS231n/13.png?size=small, /image/ML/CS231n/13.png?size=medium 1.5x, /image/ML/CS231n/13.png?size=large 2x" data-title="/image/ML/CS231n/13.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-3-1" class="heading-element"><span>Inline Question 3:</span>
  <a href="#inline-question-3-1" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Which of these data preprocessing steps is analogous to batch normalization, and which is analogous to layer normalization?</p>
<ol>
<li>Scaling each image in the dataset, so that the RGB channels for each row of pixels within an image sums up to 1.</li>
<li>Scaling each image in the dataset, so that the RGB channels for all pixels within an image sums up to 1.</li>
<li>Subtracting the mean image of the dataset from each image in the dataset.</li>
<li>Setting all RGB values to either 0 or 1 depending on a given threshold.</li>
</ol>
<p>这些数据预处理步骤中，哪一步与批量归一化类似，哪一步与层归一化类似？</p>
<ol>
<li>对数据集中的每张图像进行缩放，确保图像中每行像素的RGB通道之和为1。</li>
<li>对数据集中的每幅图像进行缩放，使图像中所有像素的RGB通道之和为1。</li>
<li>从数据集中的每幅图像中减去数据集的平均图像。</li>
<li>根据给定的阈值，将所有RGB值设置为0或1。</li>
</ol>
<p>3对应批量归一化，2对应层归一化。批量归一化通过减去均值进行中心化（如选项3），而层归一化在样本内所有特征上归一化（如选项2对整个图像做缩放）。</p>
<h4 id="inline-question-4" class="heading-element"><span>Inline Question 4:</span>
  <a href="#inline-question-4" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>When is layer normalization likely to not work well, and why?</p>
<ol>
<li>Using it in a very deep network</li>
<li>Having a very small dimension of features</li>
<li>Having a high regularization term</li>
</ol>
<p>层归一化在什么时候可能效果不佳，原因是什么？</p>
<ol>
<li>在非常深的网络中使用它</li>
<li>特征维度非常小</li>
<li>正则化项取值很高</li>
</ol>
<p>是 2.当特征维度非常小时，层归一化可能效果不佳。因为层归一化需要在单个样本的所有特征维度上计算统计量（均值和方差），当特征维度很小时：</p>
<ul>
<li>统计量的估计会变得不稳定</li>
<li>归一化操作可能过度缩放特征，导致信息丢失</li>
<li>特别是当特征维度为1时，归一化后所有特征值会变为0，完全破坏原始数据
相比之下，在特征维度较大的情况下，统计量的估计更可靠，归一化效果更好。其他选项与层归一化的有效性没有直接关联。</li>
</ul>
<h3 id="q3-dropout" class="heading-element"><span>Q3: Dropout</span>
  <a href="#q3-dropout" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p><a href="https://arxiv.org/pdf/1207.0580"target="_blank" rel="external nofollow noopener noreferrer">参考论文</a></p>
<p>简单来说就是前向传播的时候会随机把一些神经元的值变为 0，可以缓解过拟合。</p>
<p><img loading="lazy" src="/image/ML/CS231n/dropout.jpeg" alt="/image/ML/CS231n/dropout.jpeg" srcset="/image/ML/CS231n/dropout.jpeg?size=small, /image/ML/CS231n/dropout.jpeg?size=medium 1.5x, /image/ML/CS231n/dropout.jpeg?size=large 2x" data-title="/image/ML/CS231n/dropout.jpeg" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-dropout_forward" class="heading-element"><span>TODO: dropout_forward</span>
  <a href="#todo-dropout_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>参考官网讲义：<a href="https://cs231n.github.io/neural-networks-2/#reg"target="_blank" rel="external nofollow noopener noreferrer">官网讲义</a></p>
<p>生成一个 0/1 概率向量，其中概率为 $p$，如果概率小于 $p$ 则<strong>不会</strong>被置为 $0$，为了最后输出期望统一要乘上 $\dfrac{1}{p}$ 放缩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dropout_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dropout_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Performs the forward pass for (inverted) dropout.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data, of any shape
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dropout_param: A dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - p: Dropout parameter. We keep each neuron output with probability p.
</span></span></span><span class="line"><span class="cl"><span class="s2">      - mode: &#39;test&#39; or &#39;train&#39;. If the mode is train, then perform dropout;
</span></span></span><span class="line"><span class="cl"><span class="s2">        if the mode is test, then just return the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">      - seed: Seed for the random number generator. Passing seed makes this
</span></span></span><span class="line"><span class="cl"><span class="s2">        function deterministic, which is needed for gradient checking but not
</span></span></span><span class="line"><span class="cl"><span class="s2">        in real networks.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Outputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Array of the same shape as x.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout
</span></span></span><span class="line"><span class="cl"><span class="s2">      mask that was used to multiply the input; in test mode, mask is None.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.
</span></span></span><span class="line"><span class="cl"><span class="s2">    See http://cs231n.github.io/neural-networks-2/#reg for more details.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    NOTE 2: Keep in mind that p is the probability of **keep** a neuron
</span></span></span><span class="line"><span class="cl"><span class="s2">    output; this might be contrary to some sources, where it is referred to
</span></span></span><span class="line"><span class="cl"><span class="s2">    as the probability of dropping a neuron output.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">p</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="n">dropout_param</span><span class="p">[</span><span class="s2">&#34;p&#34;</span><span class="p">],</span> <span class="n">dropout_param</span><span class="p">[</span><span class="s2">&#34;mode&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s2">&#34;seed&#34;</span> <span class="ow">in</span> <span class="n">dropout_param</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">dropout_param</span><span class="p">[</span><span class="s2">&#34;seed&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;train&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement training phase forward pass for inverted dropout.   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Store the dropout mask in the mask variable.                        #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                           END OF YOUR CODE                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;test&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the test phase forward pass for inverted dropout.   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                            END OF YOUR CODE                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">dropout_param</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Running tests with p =  0.25
</span></span><span class="line"><span class="cl">Mean of input:  10.000207878477502
</span></span><span class="line"><span class="cl">Mean of train-time output:  10.014059116977283
</span></span><span class="line"><span class="cl">Mean of test-time output:  10.000207878477502
</span></span><span class="line"><span class="cl">Fraction of train-time output set to zero:  0.749784
</span></span><span class="line"><span class="cl">Fraction of test-time output set to zero:  0.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Running tests with p =  0.4
</span></span><span class="line"><span class="cl">Mean of input:  10.000207878477502
</span></span><span class="line"><span class="cl">Mean of train-time output:  9.977917658761159
</span></span><span class="line"><span class="cl">Mean of test-time output:  10.000207878477502
</span></span><span class="line"><span class="cl">Fraction of train-time output set to zero:  0.600796
</span></span><span class="line"><span class="cl">Fraction of test-time output set to zero:  0.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Running tests with p =  0.7
</span></span><span class="line"><span class="cl">Mean of input:  10.000207878477502
</span></span><span class="line"><span class="cl">Mean of train-time output:  9.987811912159426
</span></span><span class="line"><span class="cl">Mean of test-time output:  10.000207878477502
</span></span><span class="line"><span class="cl">Fraction of train-time output set to zero:  0.30074
</span></span><span class="line"><span class="cl">Fraction of test-time output set to zero:  0.0</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-dropout_backward" class="heading-element"><span>TODO: dropout_backward</span>
  <a href="#todo-dropout_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dropout_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Perform the backward pass for (inverted) dropout.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of any shape
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: (dropout_param, mask) from dropout_forward.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dropout_param</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="n">dropout_param</span><span class="p">[</span><span class="s2">&#34;mode&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;train&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement training phase backward pass for inverted dropout   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                          END OF YOUR CODE                           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#######################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&#34;test&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">dx relative error:  5.44560814873387e-11</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="inline-question-1-7" class="heading-element"><span>Inline Question 1:</span>
  <a href="#inline-question-1-7" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>What happens if we do not divide the values being passed through inverse dropout by <code>p</code> in the dropout layer? Why does that happen?</p>
<p>如果我们在 dropout 层中没有将通过反向 dropout 的值除以 $p$，会发生什么？为什么会发生这种情况？</p>
<p>Answer:
如果在 dropout 层中没有通过 $p$ 来除以传递的值，那么在训练和测试阶段的输出分布将不一致。在训练阶段，dropout 会随机将一些神经元的输出设置为零，并通过 $p$ 来缩放剩余的输出，以保持激活的期望值不变。然而，如果不进行这种缩放，训练阶段的输出将会比测试阶段的输出小 $p$ 倍。这会导致模型在训练和测试阶段表现不一致，从而影响模型的泛化能力。</p>
<h4 id="todo-给-fc_net-加上-dropout" class="heading-element"><span>TODO: 给 fc_net 加上 dropout</span>
  <a href="#todo-%e7%bb%99-fc_net-%e5%8a%a0%e4%b8%8a-dropout" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">231</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">H1</span><span class="p">,</span> <span class="n">H2</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">dropout_keep_ratio</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running check with dropout = &#39;</span><span class="p">,</span> <span class="n">dropout_keep_ratio</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">FullyConnectedNet</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">H1</span><span class="p">,</span> <span class="n">H2</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_dim</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_classes</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight_scale</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dropout_keep_ratio</span><span class="o">=</span><span class="n">dropout_keep_ratio</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">seed</span><span class="o">=</span><span class="mi">123</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Initial loss: &#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Relative errors should be around e-6 or less.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Note that it&#39;s fine if for dropout_keep_ratio=1 you have W2 error be on the order of e-5.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad_num</span> <span class="o">=</span> <span class="n">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> relative error: </span><span class="si">%.2e</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">rel_error</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="n">name</span><span class="p">])))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Running check with dropout =  1
</span></span><span class="line"><span class="cl">Initial loss:  2.3004790897684924
</span></span><span class="line"><span class="cl">W1 relative error: 1.48e-07
</span></span><span class="line"><span class="cl">W2 relative error: 2.21e-05
</span></span><span class="line"><span class="cl">W3 relative error: 3.53e-07
</span></span><span class="line"><span class="cl">b1 relative error: 5.38e-09
</span></span><span class="line"><span class="cl">b2 relative error: 2.09e-09
</span></span><span class="line"><span class="cl">b3 relative error: 5.80e-11
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Running check with dropout =  0.75
</span></span><span class="line"><span class="cl">Initial loss:  2.302371489704412
</span></span><span class="line"><span class="cl">W1 relative error: 1.90e-07
</span></span><span class="line"><span class="cl">W2 relative error: 4.76e-06
</span></span><span class="line"><span class="cl">W3 relative error: 2.60e-08
</span></span><span class="line"><span class="cl">b1 relative error: 4.73e-09
</span></span><span class="line"><span class="cl">b2 relative error: 1.82e-09
</span></span><span class="line"><span class="cl">b3 relative error: 1.70e-10
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Running check with dropout =  0.5
</span></span><span class="line"><span class="cl">Initial loss:  2.3042759220785896
</span></span><span class="line"><span class="cl">W1 relative error: 3.11e-07
</span></span><span class="line"><span class="cl">W2 relative error: 1.84e-08
</span></span><span class="line"><span class="cl">W3 relative error: 5.35e-08
</span></span><span class="line"><span class="cl">b1 relative error: 5.37e-09
</span></span><span class="line"><span class="cl">b2 relative error: 2.99e-09
</span></span><span class="line"><span class="cl">b3 relative error: 1.13e-10</span></span></code></pre></td></tr></table>
</div>
</div><p>训练对比：</p>
<p><img loading="lazy" src="/image/ML/CS231n/14.png" alt="/image/ML/CS231n/14.png" srcset="/image/ML/CS231n/14.png?size=small, /image/ML/CS231n/14.png?size=medium 1.5x, /image/ML/CS231n/14.png?size=large 2x" data-title="/image/ML/CS231n/14.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="inline-question-2-5" class="heading-element"><span>Inline Question 2:</span>
  <a href="#inline-question-2-5" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>Compare the validation and training accuracies with and without dropout &ndash; what do your results suggest about dropout as a regularizer?</p>
<p>比较使用和不使用 dropout 的验证和训练准确率——你的结果对 dropout 作为正则化器有什么建议？</p>
<p>Answer:</p>
<p>实验结果显示，使用dropout（keep_ratio=0.25）时：</p>
<ol>
<li>训练准确率略低于不使用dropout的情况，说明dropout通过随机失活神经元降低了模型对训练数据的过拟合能力</li>
<li>验证准确率显著高于不使用dropout的情况，且与训练准确率的差距更小，表明模型泛化能力更好</li>
<li>验证曲线更平滑稳定，说明dropout起到了正则化作用，有效抑制了过拟合现象</li>
</ol>
<p>这说明dropout通过阻止神经元间的协同适应（co-adaptation），迫使网络学习更鲁棒的特征，从而提升模型在未见数据上的表现。</p>
<h3 id="q4-convolutional-neural-networks" class="heading-element"><span>Q4: Convolutional Neural Networks</span>
  <a href="#q4-convolutional-neural-networks" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><h4 id="todo-conv_forward_naive" class="heading-element"><span>TODO: conv_forward_naive</span>
  <a href="#todo-conv_forward_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">conv_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A naive implementation of the forward pass for a convolutional layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The input consists of N data points, each with C channels, height H and
</span></span></span><span class="line"><span class="cl"><span class="s2">    width W. We convolve each input with F different filters, where each filter
</span></span></span><span class="line"><span class="cl"><span class="s2">    spans all C channels and has height HH and width WW.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Input:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - w: Filter weights of shape (F, C, HH, WW)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - b: Biases, of shape (F,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - conv_param: A dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - &#39;stride&#39;: The number of pixels between adjacent receptive fields in the
</span></span></span><span class="line"><span class="cl"><span class="s2">        horizontal and vertical directions.
</span></span></span><span class="line"><span class="cl"><span class="s2">      - &#39;pad&#39;: The number of pixels that will be used to zero-pad the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    During padding, &#39;pad&#39; zeros should be placed symmetrically (i.e equally on both sides)
</span></span></span><span class="line"><span class="cl"><span class="s2">    along the height and width axes of the input. Be careful not to modfiy the original
</span></span></span><span class="line"><span class="cl"><span class="s2">    input x directly.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Output data, of shape (N, F, H&#39;, W&#39;) where H&#39; and W&#39; are given by
</span></span></span><span class="line"><span class="cl"><span class="s2">      H&#39; = 1 + (H + 2 * pad - HH) / stride
</span></span></span><span class="line"><span class="cl"><span class="s2">      W&#39; = 1 + (W + 2 * pad - WW) / stride
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: (x, w, b, conv_param)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the convolutional forward pass.                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Hint: you can use the function np.pad for padding.                      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 先获取一些需要用到的数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H_input</span><span class="p">,</span> <span class="n">W_input</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H_input高，W_input宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">F</span><span class="p">,</span> <span class="n">C_w_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># F个卷积核, C_w_个通道，HH高，WW宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s2">&#34;stride&#34;</span><span class="p">]</span>  <span class="c1"># 步长</span>
</span></span><span class="line"><span class="cl">    <span class="n">pad</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s2">&#34;pad&#34;</span><span class="p">]</span>  <span class="c1"># 填充数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算卷积后的高和宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H_input</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_W</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_input</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给x的上下左右填充上pad个0</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_pad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="s2">&#34;constant&#34;</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 将卷积核w转换成F * (C * HH * WW)的矩阵 (便于使用矩阵乘法)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w_row</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 生成空白输出便于后续循环填充</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">out_H</span><span class="p">,</span> <span class="n">out_W</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 开始卷积</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>  <span class="c1"># 遍历样本</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>  <span class="c1"># 遍历卷积核</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_H</span><span class="p">):</span>  <span class="c1"># 遍历高</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_W</span><span class="p">):</span>  <span class="c1"># 遍历宽</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 获取当前卷积窗口</span>
</span></span><span class="line"><span class="cl">                    <span class="n">window</span> <span class="o">=</span> <span class="n">x_pad</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">HH</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">WW</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 将卷积窗口拉成一行</span>
</span></span><span class="line"><span class="cl">                    <span class="n">window_row</span> <span class="o">=</span> <span class="n">window</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算当前卷积窗口和卷积核的卷积结果</span>
</span></span><span class="line"><span class="cl">                    <span class="n">out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">window_row</span> <span class="o">*</span> <span class="n">w_row</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="p">:])</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">f</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">	  <span class="c1"># 将pad后的x存入cache (省的反向传播的时候在计算一次)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x_pad</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing conv_forward_naive
</span></span><span class="line"><span class="cl">difference:  2.2121476417505994e-08</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/15.png" alt="/image/ML/CS231n/15.png" srcset="/image/ML/CS231n/15.png?size=small, /image/ML/CS231n/15.png?size=medium 1.5x, /image/ML/CS231n/15.png?size=large 2x" data-title="/image/ML/CS231n/15.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<h4 id="todo-conv_backward_naive" class="heading-element"><span>TODO: conv_backward_naive</span>
  <a href="#todo-conv_backward_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">conv_backward_naive</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A naive implementation of the backward pass for a convolutional layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to x
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dw: Gradient with respect to w
</span></span></span><span class="line"><span class="cl"><span class="s2">    - db: Gradient with respect to b
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the convolutional backward pass.                        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取一些需要用到的数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H_input</span><span class="p">,</span> <span class="n">W_input</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H_input高，W_input宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">F</span><span class="p">,</span> <span class="n">C_w_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># F个卷积核, C_w_个通道，HH高，WW宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s2">&#34;stride&#34;</span><span class="p">]</span>  <span class="c1"># 步长</span>
</span></span><span class="line"><span class="cl">    <span class="n">pad</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s2">&#34;pad&#34;</span><span class="p">]</span>  <span class="c1"># 填充数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算卷积后的高和宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H_input</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_W</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_input</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给dx,dw,db分配空间</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_H</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_W</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 获取当前卷积窗口</span>
</span></span><span class="line"><span class="cl">                    <span class="n">window</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">HH</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">WW</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算db</span>
</span></span><span class="line"><span class="cl">                    <span class="n">db</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dout</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算dw</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dw</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">+=</span> <span class="n">window</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算dx</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dx</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">HH</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">WW</span><span class="p">]</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 去掉dx的pad</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">pad</span><span class="p">:</span><span class="n">H_input</span> <span class="o">-</span> <span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">:</span><span class="n">W_input</span> <span class="o">-</span> <span class="n">pad</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing conv_backward_naive function
</span></span><span class="line"><span class="cl">dx error:  1.159803161159293e-08
</span></span><span class="line"><span class="cl">dw error:  2.2471264748452487e-10
</span></span><span class="line"><span class="cl">db error:  3.37264006649648e-11</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-max_pool_forward_naive" class="heading-element"><span>TODO: max_pool_forward_naive</span>
  <a href="#todo-max_pool_forward_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">max_pool_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A naive implementation of the forward pass for a max-pooling layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - pool_param: dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - &#39;pool_height&#39;: The height of each pooling region
</span></span></span><span class="line"><span class="cl"><span class="s2">      - &#39;pool_width&#39;: The width of each pooling region
</span></span></span><span class="line"><span class="cl"><span class="s2">      - &#39;stride&#39;: The distance between adjacent pooling regions
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    No padding is necessary here, eg you can assume:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - (H - pool_height) </span><span class="si">% s</span><span class="s2">tride == 0
</span></span></span><span class="line"><span class="cl"><span class="s2">      - (W - pool_width) </span><span class="si">% s</span><span class="s2">tride == 0
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Output data, of shape (N, C, H&#39;, W&#39;) where H&#39; and W&#39; are given by
</span></span></span><span class="line"><span class="cl"><span class="s2">      H&#39; = 1 + (H - pool_height) / stride
</span></span></span><span class="line"><span class="cl"><span class="s2">      W&#39; = 1 + (W - pool_width) / stride
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: (x, pool_param)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the max-pooling forward pass                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取一些需要用到的数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">pool_height</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;pool_height&#34;</span><span class="p">]</span>  <span class="c1"># 池化核高</span>
</span></span><span class="line"><span class="cl">    <span class="n">pool_width</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;pool_width&#34;</span><span class="p">]</span>  <span class="c1"># 池化核宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;stride&#34;</span><span class="p">]</span>  <span class="c1"># 步长</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算池化后的高和宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pool_height</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_W</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pool_width</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给out分配空间</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">out_H</span><span class="p">,</span> <span class="n">out_W</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_H</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_W</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 获取当前池化窗口</span>
</span></span><span class="line"><span class="cl">                    <span class="n">window</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">pool_height</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">pool_width</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算当前池化窗口的最大值</span>
</span></span><span class="line"><span class="cl">                    <span class="n">out</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing max_pool_forward_naive function:
</span></span><span class="line"><span class="cl">difference:  4.1666665157267834e-08</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="tood-max_pool_backward_naive" class="heading-element"><span>TOOD: max_pool_backward_naive</span>
  <a href="#tood-max_pool_backward_naive" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">max_pool_backward_naive</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A naive implementation of the backward pass for a max-pooling layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: A tuple of (x, pool_param) as in the forward pass.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the max-pooling backward pass                           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取一些需要用到的数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">pool_height</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;pool_height&#34;</span><span class="p">]</span>  <span class="c1"># 池化核高</span>
</span></span><span class="line"><span class="cl">    <span class="n">pool_width</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;pool_width&#34;</span><span class="p">]</span>  <span class="c1"># 池化核宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s2">&#34;stride&#34;</span><span class="p">]</span>  <span class="c1"># 步长</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算池化后的高和宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pool_height</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_W</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pool_width</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 给dx分配空间</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_H</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_W</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 获取当前池化窗口</span>
</span></span><span class="line"><span class="cl">                    <span class="n">window</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">pool_height</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span><span class="p">:</span><span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">pool_width</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算当前池化窗口的最大值</span>
</span></span><span class="line"><span class="cl">                    <span class="n">max_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># 计算dx</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dx</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">max_index</span> <span class="o">//</span> <span class="n">pool_width</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">max_index</span> <span class="o">%</span> <span class="n">pool_width</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dout</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing max_pool_backward_naive function:
</span></span><span class="line"><span class="cl">dx error:  3.27562514223145e-12</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="fast-layers" class="heading-element"><span>Fast Layers</span>
  <a href="#fast-layers" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><code>im2col_cython.pyx</code> 最上方加一行代码： <code>#cython: language_level=2</code></p>
<p>再改一下启动脚本，运行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Remember to restart the runtime after executing this cell!</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">cd</span> <span class="o">./</span><span class="n">cs231n</span>
</span></span><span class="line"><span class="cl"><span class="err">!</span><span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">build_ext</span> <span class="o">--</span><span class="n">inplace</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">cd</span> <span class="o">../</span></span></span></code></pre></td></tr></table>
</div>
</div><p>中途报了 <code>nameerror: name 'col2im_6d_cython' is not defined</code> 的错误，重启一下笔记本就好了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing conv_forward_fast:
</span></span><span class="line"><span class="cl">Naive: 4.879596s
</span></span><span class="line"><span class="cl">Fast: 0.010973s
</span></span><span class="line"><span class="cl">Speedup: 444.682390x
</span></span><span class="line"><span class="cl">Difference:  1.970563140655889e-11
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Testing conv_backward_fast:
</span></span><span class="line"><span class="cl">Naive: 6.945447s
</span></span><span class="line"><span class="cl">Fast: 0.002007s
</span></span><span class="line"><span class="cl">Speedup: 3459.776366x
</span></span><span class="line"><span class="cl">dx difference:  9.43434568725122e-12
</span></span><span class="line"><span class="cl">dw difference:  4.420587653909754e-13
</span></span><span class="line"><span class="cl">db difference:  3.481354613192702e-14
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Testing pool_forward_fast:
</span></span><span class="line"><span class="cl">Naive: 0.302344s
</span></span><span class="line"><span class="cl">fast: 0.004001s
</span></span><span class="line"><span class="cl">speedup: 75.559852x
</span></span><span class="line"><span class="cl">difference:  0.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Testing pool_backward_fast:
</span></span><span class="line"><span class="cl">Naive: 0.315859s
</span></span><span class="line"><span class="cl">fast: 0.010774s
</span></span><span class="line"><span class="cl">speedup: 29.317090x
</span></span><span class="line"><span class="cl">dx difference:  0.0</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Testing conv_relu_pool
</span></span><span class="line"><span class="cl">dx error:  4.397502834267091e-09
</span></span><span class="line"><span class="cl">dw error:  3.651699397290073e-09
</span></span><span class="line"><span class="cl">db error:  7.054812624223923e-10
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Testing conv_relu:
</span></span><span class="line"><span class="cl">dx error:  8.03522627181292e-09
</span></span><span class="line"><span class="cl">dw error:  2.0902405745264502e-10
</span></span><span class="line"><span class="cl">db error:  3.287958402642519e-10</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-three-layer-convolutional-network" class="heading-element"><span>TODO: Three-Layer Convolutional Network</span>
  <a href="#todo-three-layer-convolutional-network" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ThreeLayerConvNet</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A three-layer convolutional network with the following architecture:
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    conv - relu - 2x2 max pool - affine - relu - affine - softmax
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The network operates on minibatches of data that have shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    consisting of N images, each with height H and width W and with C input
</span></span></span><span class="line"><span class="cl"><span class="s2">    channels.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_dim</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">filter_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight_scale</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Initialize a new network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - input_dim: Tuple (C, H, W) giving size of input data
</span></span></span><span class="line"><span class="cl"><span class="s2">        - num_filters: Number of filters to use in the convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">        - filter_size: Width/height of filters to use in the convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">        - hidden_dim: Number of units to use in the fully-connected hidden layer
</span></span></span><span class="line"><span class="cl"><span class="s2">        - num_classes: Number of scores to produce from the final affine layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - weight_scale: Scalar giving standard deviation for random initialization
</span></span></span><span class="line"><span class="cl"><span class="s2">          of weights.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - reg: Scalar giving L2 regularization strength
</span></span></span><span class="line"><span class="cl"><span class="s2">        - dtype: numpy datatype to use for computation.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Initialize weights and biases for the three-layer convolutional    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># network. Weights should be initialized from a Gaussian centered at 0.0   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with standard deviation equal to weight_scale; biases should be          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># initialized to zero. All weights and biases should be stored in the      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  dictionary self.params. Store weights and biases for the convolutional  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># layer using the keys &#39;W1&#39; and &#39;b1&#39;; use keys &#39;W2&#39; and &#39;b2&#39; for the       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># weights and biases of the hidden affine layer, and keys &#39;W3&#39; and &#39;b3&#39;    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># for the weights and biases of the output affine layer.                   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># IMPORTANT: For this assignment, you can assume that the padding          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># and stride of the first convolutional layer are chosen so that           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># **the width and height of the input are preserved**. Take a look at      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the start of the loss() function to see how that happens.                #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># conv - relu - 2x2 max pool - affine - relu - affine - softmax</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">input_dim</span>  <span class="c1"># 获取输入数据的通道数，高度，宽度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 卷积层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_filters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">num_filters</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b2&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W3&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b3&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Evaluate loss and gradient for the three-layer convolutional network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Input / output: Same API as TwoLayerNet in fc_net.py.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W3&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b3&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># pass conv_param to the forward pass for the convolutional layer</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Padding and stride chosen to preserve the input spatial size</span>
</span></span><span class="line"><span class="cl">        <span class="n">filter_size</span> <span class="o">=</span> <span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_param</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;stride&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;pad&#34;</span><span class="p">:</span> <span class="p">(</span><span class="n">filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># pass pool_param to the forward pass for the max-pooling layer</span>
</span></span><span class="line"><span class="cl">        <span class="n">pool_param</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;pool_height&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;pool_width&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&#34;stride&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the forward pass for the three-layer convolutional net,  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># computing the class scores for X and storing them in the scores          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variable.                                                                #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Remember you can use the functions defined in cs231n/fast_layers.py and  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># cs231n/layer_utils.py in your implementation (already imported).         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># conv - relu - 2x2 max pool - affine - relu - affine - softmax</span>
</span></span><span class="line"><span class="cl">        <span class="n">out1</span><span class="p">,</span> <span class="n">cache1</span> <span class="o">=</span> <span class="n">conv_relu_pool_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">conv_param</span><span class="p">,</span> <span class="n">pool_param</span><span class="p">)</span>  <span class="c1"># 卷积层</span>
</span></span><span class="line"><span class="cl">        <span class="n">out2</span><span class="p">,</span> <span class="n">cache2</span> <span class="o">=</span> <span class="n">affine_relu_forward</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span><span class="p">,</span> <span class="n">cache3</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">scores</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the backward pass for the three-layer convolutional net, #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># storing the loss and gradients in the loss and grads variables. Compute  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># data loss using softmax, and make sure that grads[k] holds the gradients #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># for self.params[k]. Don&#39;t forget to add L2 regularization!               #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># NOTE: To ensure that your implementation matches ours and you pass the   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># automated tests, make sure that your L2 regularization includes a factor #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># of 0.5 to simplify the expression for the gradient.                      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算损失</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">dout</span> <span class="o">=</span> <span class="n">softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W3</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># L2正则化</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">dout</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W3&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b3&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache3</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="n">dout</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b2&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine_relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache2</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="n">dout</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv_relu_pool_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache1</span><span class="p">)</span>  <span class="c1"># 卷积层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 加上正则化项的梯度</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W3&#34;</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W3</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Sanity Check Loss</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Initial loss (no regularization):  2.302586071243987
</span></span><span class="line"><span class="cl">Initial loss (with regularization):  2.508255638232932</span></span></code></pre></td></tr></table>
</div>
</div><p>Gradient Check</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">W1 max relative error: 1.380104e-04
</span></span><span class="line"><span class="cl">W2 max relative error: 1.822723e-02
</span></span><span class="line"><span class="cl">W3 max relative error: 3.064049e-04
</span></span><span class="line"><span class="cl">b1 max relative error: 3.477652e-05
</span></span><span class="line"><span class="cl">b2 max relative error: 2.516375e-03
</span></span><span class="line"><span class="cl">b3 max relative error: 7.945660e-10</span></span></code></pre></td></tr></table>
</div>
</div><p>Overfit Small Data</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Small data training accuracy: 0.82
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Small data validation accuracy: 0.252</span></span></code></pre></td></tr></table>
</div>
</div><p><img loading="lazy" src="/image/ML/CS231n/16.png" alt="/image/ML/CS231n/16.png" srcset="/image/ML/CS231n/16.png?size=small, /image/ML/CS231n/16.png?size=medium 1.5x, /image/ML/CS231n/16.png?size=large 2x" data-title="/image/ML/CS231n/16.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>Train the Network</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Full data training accuracy: 0.4761836734693878
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Full data validation accuracy: 0.499</span></span></code></pre></td></tr></table>
</div>
</div><p>Visualize Filters</p>
<p><img loading="lazy" src="/image/ML/CS231n/17.png" alt="/image/ML/CS231n/17.png" srcset="/image/ML/CS231n/17.png?size=small, /image/ML/CS231n/17.png?size=medium 1.5x, /image/ML/CS231n/17.png?size=large 2x" data-title="/image/ML/CS231n/17.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>spatial batchnorm 和 spatial groupnorm 不太会，抄个代码鸽一下。</p>
<h4 id="todo-spatial_batchnorm_forward" class="heading-element"><span>TODO: spatial_batchnorm_forward</span>
  <a href="#todo-spatial_batchnorm_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spatial_batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the forward pass for spatial batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gamma: Scale parameter, of shape (C,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta: Shift parameter, of shape (C,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - bn_param: Dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - mode: &#39;train&#39; or &#39;test&#39;; required
</span></span></span><span class="line"><span class="cl"><span class="s2">      - eps: Constant for numeric stability
</span></span></span><span class="line"><span class="cl"><span class="s2">      - momentum: Constant for running mean / variance. momentum=0 means that
</span></span></span><span class="line"><span class="cl"><span class="s2">        old information is discarded completely at every time step, while
</span></span></span><span class="line"><span class="cl"><span class="s2">        momentum=1 means that new information is never incorporated. The
</span></span></span><span class="line"><span class="cl"><span class="s2">        default of momentum=0.9 should work well in most situations.
</span></span></span><span class="line"><span class="cl"><span class="s2">      - running_mean: Array of shape (D,) giving running mean of features
</span></span></span><span class="line"><span class="cl"><span class="s2">      - running_var Array of shape (D,) giving running variance of features
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Output data, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values needed for the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the forward pass for spatial batch normalization.       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: You can implement spatial batch normalization by calling the      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># vanilla version of batch normalization you implemented above.           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Your implementation should be very short; ours is less than five lines. #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>  <span class="c1"># 将C通道放到最后，然后reshape成二维数组</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">)</span>  <span class="c1"># 调用batchnorm_forward</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 将C通道放到第二维，然后reshape成四维数组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Before spatial batch normalization:
</span></span><span class="line"><span class="cl">  shape:  (2, 3, 4, 5)
</span></span><span class="line"><span class="cl">  means:  [9.33463814 8.90909116 9.11056338]
</span></span><span class="line"><span class="cl">  stds:  [3.61447857 3.19347686 3.5168142 ]
</span></span><span class="line"><span class="cl">After spatial batch normalization:
</span></span><span class="line"><span class="cl">  shape:  (2, 3, 4, 5)
</span></span><span class="line"><span class="cl">  means:  [ 6.18949336e-16  5.99520433e-16 -1.22124533e-16]
</span></span><span class="line"><span class="cl">  stds:  [0.99999962 0.99999951 0.9999996 ]
</span></span><span class="line"><span class="cl">After spatial batch normalization (nontrivial gamma, beta):
</span></span><span class="line"><span class="cl">  shape:  (2, 3, 4, 5)
</span></span><span class="line"><span class="cl">  means:  [6. 7. 8.]
</span></span><span class="line"><span class="cl">  stds:  [2.99999885 3.99999804 4.99999798]</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">After spatial batch normalization (test-time):
</span></span><span class="line"><span class="cl">  means:  [-0.08034406  0.07562881  0.05716371  0.04378383]
</span></span><span class="line"><span class="cl">  stds:  [0.96718744 1.0299714  1.02887624 1.00585577]</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-spatial_batchnorm_backward" class="heading-element"><span>TODO: spatial_batchnorm_backward</span>
  <a href="#todo-spatial_batchnorm_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spatial_batchnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the backward pass for spatial batch normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values from the forward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dgamma: Gradient with respect to scale parameter, of shape (C,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dbeta: Gradient with respect to shift parameter, of shape (C,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for spatial batch normalization.      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: You can implement spatial batch normalization by calling the      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># vanilla version of batch normalization you implemented above.           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Your implementation should be very short; ours is less than five lines. #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">    <span class="n">dout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>  <span class="c1"># 将C通道放到最后，然后reshape成二维数组</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="n">batchnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>  <span class="c1"># 调用batchnorm_backward</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">dx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 将C通道放到第二维，然后reshape成四维数组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">dx error:  2.786648197756335e-07
</span></span><span class="line"><span class="cl">dgamma error:  7.0974817113608705e-12
</span></span><span class="line"><span class="cl">dbeta error:  3.275608725278405e-12</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-spatial_groupnorm_forward" class="heading-element"><span>TODO: spatial_groupnorm_forward</span>
  <a href="#todo-spatial_groupnorm_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spatial_groupnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">gn_param</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the forward pass for spatial group normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">    In contrast to layer normalization, group normalization splits each entry
</span></span></span><span class="line"><span class="cl"><span class="s2">    in the data into G contiguous pieces, which it then normalizes independently.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Per feature shifting and scaling are then applied to the data, in a manner identical to that of batch normalization and layer normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gamma: Scale parameter, of shape (1, C, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - beta: Shift parameter, of shape (1, C, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - G: Integer mumber of groups to split into, should be a divisor of C
</span></span></span><span class="line"><span class="cl"><span class="s2">    - gn_param: Dictionary with the following keys:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - eps: Constant for numeric stability
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Output data, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values needed for the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">eps</span> <span class="o">=</span> <span class="n">gn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;eps&#34;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the forward pass for spatial group normalization.       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># This will be extremely similar to the layer norm implementation.        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># In particular, think about how you could transform the matrix so that   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># the bulk of the code is similar to both train-time batch normalization  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># and layer normalization!                                                #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将C通道分成G组，每组有C//G个通道</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># reshape成五维数组</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 求均值</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 求方差</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># 归一化</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x_norm</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># reshape成四维数组</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>  <span class="c1"># 伸缩平移</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">,</span> <span class="n">x_var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>  <span class="c1"># 缓存变量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Before spatial group normalization:
</span></span><span class="line"><span class="cl">  shape:  (2, 6, 4, 5)
</span></span><span class="line"><span class="cl">  means:  [9.72505327 8.51114185 8.9147544  9.43448077]
</span></span><span class="line"><span class="cl">  stds:  [3.67070958 3.09892597 4.27043622 3.97521327]
</span></span><span class="line"><span class="cl">After spatial group normalization:
</span></span><span class="line"><span class="cl">  shape:  (2, 6, 4, 5)
</span></span><span class="line"><span class="cl">  means:  [-2.14643118e-16  5.25505565e-16  2.65528340e-16 -3.38618023e-16]
</span></span><span class="line"><span class="cl">  stds:  [0.99999963 0.99999948 0.99999973 0.99999968]</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-spatial_groupnorm_backward" class="heading-element"><span>TODO: spatial_groupnorm_backward</span>
  <a href="#todo-spatial_groupnorm_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">spatial_groupnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the backward pass for spatial group normalization.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream derivatives, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values from the forward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dgamma: Gradient with respect to scale parameter, of shape (1, C, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dbeta: Gradient with respect to shift parameter, of shape (1, C, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for spatial group normalization.      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># This will be extremely similar to the layer norm implementation.        #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">,</span> <span class="n">x_var</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="n">cache</span>  <span class="c1"># 从缓存中取出变量</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># N个样本，C个通道，H高，W宽</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算dgamma和dbeta</span>
</span></span><span class="line"><span class="cl">    <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 求dgamma</span>
</span></span><span class="line"><span class="cl">    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 求dbeta</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 准备数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>  <span class="c1"># reshape成五维数组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="n">C</span> <span class="o">//</span> <span class="n">G</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">W</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">G</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">((</span><span class="n">x_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">),</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_norm</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">dx_var</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                                                                                             <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx_norm</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">dx_var</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span> <span class="o">+</span> <span class="n">dx_mean</span> <span class="o">/</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                             END OF YOUR CODE                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">dx error:  7.413109648400194e-08
</span></span><span class="line"><span class="cl">dgamma error:  9.468195772749234e-12
</span></span><span class="line"><span class="cl">dbeta error:  3.354494437653335e-12</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="q5-pytorch-on-cifar-10" class="heading-element"><span>Q5: PyTorch on CIFAR-10</span>
  <a href="#q5-pytorch-on-cifar-10" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>使用 PyTorch 来实现一些神经网络。</p>
<h4 id="barebones-pytorch-two-layer-network" class="heading-element"><span>Barebones PyTorch: Two-Layer Network</span>
  <a href="#barebones-pytorch-two-layer-network" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>实现一个两层 ReLU 的全连接神经网络，主要是对 PyTorch 的基本语法熟悉一下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>  <span class="c1"># useful stateless functions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">two_layer_fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A fully-connected neural networks; the architecture is:
</span></span></span><span class="line"><span class="cl"><span class="s2">    NN is fully connected -&gt; ReLU -&gt; fully connected layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note that this function only defines the forward pass; 
</span></span></span><span class="line"><span class="cl"><span class="s2">    PyTorch will take care of the backward pass for us.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    The input to the network will be a minibatch of data, of shape
</span></span></span><span class="line"><span class="cl"><span class="s2">    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,
</span></span></span><span class="line"><span class="cl"><span class="s2">    and the output layer will produce scores for C classes.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of
</span></span></span><span class="line"><span class="cl"><span class="s2">      input data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;
</span></span></span><span class="line"><span class="cl"><span class="s2">      w1 has shape (D, H) and w2 has shape (H, C).
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for
</span></span></span><span class="line"><span class="cl"><span class="s2">      the input data x.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># first we flatten the image</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: [batch_size, C x H x W]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">params</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Forward pass: compute predicted y using operations on Tensors. Since w1 and</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># w2 have requires_grad=True, operations involving these Tensors will cause</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># PyTorch to build a computational graph, allowing automatic computation of</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># gradients. Since we are no longer implementing the backward pass by hand we</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># don&#39;t need to keep references to intermediate values.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># you can also use `.clamp(min=0)`, equivalent to F.relu()</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">two_layer_fc_test</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_layer_size</span> <span class="o">=</span> <span class="mi">42</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># minibatch size 64, feature dimension 50</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="n">hidden_layer_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">two_layer_fc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># you should see [64, 10]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">two_layer_fc_test</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>torch.nn.functional</code> 是定义了一个无状态函数，他提供了一系列的常用函数。</p>
<p><code>torch.zeros</code> 是 PyTorch 中的一个函数，用于创建一个全为零的张量。</p>
<p><code>flatten</code> 是将输入的图像数据扁平化为一维向量，以便于后续的全连接层处理。</p>
<p><code>mm()</code> 矩阵乘法。</p>
<h4 id="todo-barebones-pytorch-three-layer-convnet" class="heading-element"><span>TODO: Barebones PyTorch: Three-Layer ConvNet</span>
  <a href="#todo-barebones-pytorch-three-layer-convnet" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>完成函数 three_layer_convnet 的实现，该函数将执行三层卷积网络的前向传播。网络应具有以下架构：</p>
<ol>
<li>一个卷积层（带偏置），具有 <code>channel_1</code> 个滤波器，每个滤波器的形状为 <code>KW1 x KH1</code>，并且有零填充为2。</li>
<li>ReLU 非线性激活。</li>
<li>一个卷积层（带偏置），具有 <code>channel_2</code> 个滤波器，每个滤波器的形状为 <code>KW2 x KH2</code>，并且有零填充为1。</li>
<li>ReLU 非线性激活。</li>
<li>一个全连接层（带偏置），生成 C 类的分数。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">three_layer_convnet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Performs the forward pass of a three-layer convolutional network with the
</span></span></span><span class="line"><span class="cl"><span class="s2">    architecture defined above.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images
</span></span></span><span class="line"><span class="cl"><span class="s2">    - params: A list of PyTorch Tensors giving the weights and biases for the
</span></span></span><span class="line"><span class="cl"><span class="s2">      network; should contain the following:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights
</span></span></span><span class="line"><span class="cl"><span class="s2">        for the first convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">      - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first
</span></span></span><span class="line"><span class="cl"><span class="s2">        convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">      - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving
</span></span></span><span class="line"><span class="cl"><span class="s2">        weights for the second convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">      - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second
</span></span></span><span class="line"><span class="cl"><span class="s2">        convolutional layer
</span></span></span><span class="line"><span class="cl"><span class="s2">      - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you
</span></span></span><span class="line"><span class="cl"><span class="s2">        figure out what the shape should be?
</span></span></span><span class="line"><span class="cl"><span class="s2">      - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you
</span></span></span><span class="line"><span class="cl"><span class="s2">        figure out what the shape should be?
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - scores: PyTorch Tensor of shape (N, C) giving classification scores for x
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">conv_w1</span><span class="p">,</span> <span class="n">conv_b1</span><span class="p">,</span> <span class="n">conv_w2</span><span class="p">,</span> <span class="n">conv_b2</span><span class="p">,</span> <span class="n">fc_w</span><span class="p">,</span> <span class="n">fc_b</span> <span class="o">=</span> <span class="n">params</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the forward pass for the three-layer ConvNet.                #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 第一层卷积</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">conv_w1</span><span class="p">,</span> <span class="n">conv_b1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 使用零填充</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ReLU 激活</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 第二层卷积</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">conv_w2</span><span class="p">,</span> <span class="n">conv_b2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 使用零填充</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ReLU 激活</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 展平</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">fc_w</span><span class="p">)</span> <span class="o">+</span> <span class="n">fc_b</span>  <span class="c1"># 计算分数</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                 END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">scores</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="barebones-pytorch-initialization" class="heading-element"><span>Barebones PyTorch: Initialization</span>
  <a href="#barebones-pytorch-initialization" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p><code>random_weight(shape)</code> 使用 Kaiming 正规化方法初始化权重张量。</p>
<p><code>zero_weight(shape)</code> 初始化一个全为零的权重张量。对于实例化偏置参数非常有用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">random_weight</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Create random Tensors for weights; setting requires_grad=True means that we
</span></span></span><span class="line"><span class="cl"><span class="s2">    want to compute gradients for these Tensors during the backward pass.
</span></span></span><span class="line"><span class="cl"><span class="s2">    We use Kaiming normalization: sqrt(2 / fan_in)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># FC weight</span>
</span></span><span class="line"><span class="cl">        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="c1"># conv weight [out_channel, in_channel, kH, kW]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># randn is standard normal distribution generator. </span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">fan_in</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">zero_weight</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create a weight of shape [3 x 5]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># you should see the type `torch.cuda.FloatTensor` if you use GPU. </span>
</span></span><span class="line"><span class="cl"><span class="c1"># Otherwise it should be `torch.FloatTensor`</span>
</span></span><span class="line"><span class="cl"><span class="n">random_weight</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-barebones-pytorch-training-a-convnet" class="heading-element"><span>TODO: BareBones PyTorch: Training a ConvNet</span>
  <a href="#todo-barebones-pytorch-training-a-convnet" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><h5 id="barebones-pytorch-训练卷积神经网络" class="heading-element"><span>BareBones PyTorch: 训练卷积神经网络</span>
  <a href="#barebones-pytorch-%e8%ae%ad%e7%bb%83%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h5><p>训练一个三层卷积网络。网络应具有以下架构：</p>
<ol>
<li>卷积层（带偏置），使用32个5x5的滤波器，零填充为2</li>
<li>ReLU激活</li>
<li>卷积层（带偏置），使用16个3x3的滤波器，零填充为1</li>
<li>ReLU激活</li>
<li>全连接层（带偏置），用于计算10个类别的分数</li>
</ol>
<p>您应该使用上面定义的<code>random_weight</code>函数来初始化权重矩阵，并使用<code>zero_weight</code>函数来初始化偏置向量。</p>
<p>您不需要调整任何超参数，但如果一切正常，您应该在一个epoch后达到超过42%的准确率。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">channel_1</span> <span class="o">=</span> <span class="mi">32</span>
</span></span><span class="line"><span class="cl"><span class="n">channel_2</span> <span class="o">=</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">conv_w1</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_b1</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_w2</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_b2</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">fc_w</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">fc_b</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Initialize the parameters of a three-layer ConvNet.                    #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">conv_w1</span> <span class="o">=</span> <span class="n">random_weight</span><span class="p">((</span><span class="n">channel_1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_b1</span> <span class="o">=</span> <span class="n">zero_weight</span><span class="p">(</span><span class="n">channel_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_w2</span> <span class="o">=</span> <span class="n">random_weight</span><span class="p">((</span><span class="n">channel_2</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">conv_b2</span> <span class="o">=</span> <span class="n">zero_weight</span><span class="p">(</span><span class="n">channel_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fc_w</span> <span class="o">=</span> <span class="n">random_weight</span><span class="p">((</span><span class="n">channel_2</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">fc_b</span> <span class="o">=</span> <span class="n">zero_weight</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv_w1</span><span class="p">,</span> <span class="n">conv_b1</span><span class="p">,</span> <span class="n">conv_w2</span><span class="p">,</span> <span class="n">conv_b2</span><span class="p">,</span> <span class="n">fc_w</span><span class="p">,</span> <span class="n">fc_b</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">train_part2</span><span class="p">(</span><span class="n">three_layer_convnet</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Iteration 0, loss = 3.8646
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 107 / 1000 correct (10.70%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 100, loss = 1.8770
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 317 / 1000 correct (31.70%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 200, loss = 1.8188
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 375 / 1000 correct (37.50%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 300, loss = 1.6608
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 397 / 1000 correct (39.70%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 400, loss = 1.7184
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 437 / 1000 correct (43.70%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 500, loss = 1.6697
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 450 / 1000 correct (45.00%)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 600, loss = 1.6106
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Iteration 700, loss = 1.4275
</span></span><span class="line"><span class="cl">Checking accuracy on the val set
</span></span><span class="line"><span class="cl">Got 438 / 1000 correct (43.80%)</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-module-api-three-layer-convnet" class="heading-element"><span>TODO: Module API: Three-Layer ConvNet</span>
  <a href="#todo-module-api-three-layer-convnet" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><h5 id="模块-api三层卷积网络" class="heading-element"><span>模块 API：三层卷积网络</span>
  <a href="#%e6%a8%a1%e5%9d%97-api%e4%b8%89%e5%b1%82%e5%8d%b7%e7%a7%af%e7%bd%91%e7%bb%9c" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h5><p>实现一个三层卷积网络，后面跟一个全连接层。网络架构应该与第二部分相同：</p>
<ol>
<li>使用 <code>channel_1</code> 个 5x5 的卷积层，零填充为 2</li>
<li>ReLU 激活</li>
<li>使用 <code>channel_2</code> 个 3x3 的卷积层，零填充为 1</li>
<li>ReLU 激活</li>
<li>全连接层，输出 <code>num_classes</code> 个类别</li>
</ol>
<p>你应该使用 Kaiming 正态初始化方法来初始化模型的权重矩阵。</p>
<p><strong>提示</strong>: <a href="http://pytorch.org/docs/stable/nn.html#conv2d"target="_blank" rel="external nofollow noopener noreferrer">PyTorch 文档</a></p>
<p>在你实现三层卷积网络后，<code>test_ThreeLayerConvNet</code> 函数将运行你的实现；它应该打印输出分数的形状为 <code>(64, 10)</code>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ThreeLayerConvNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Set up the layers you need for a three-layer ConvNet with the  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># architecture defined above.                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义卷积层和全连接层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 第一层卷积</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 第二层卷积</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel_2</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Kaiming 正态初始化</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                          END OF YOUR CODE                            #       </span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the forward function for a 3-layer ConvNet. you      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># should use the layers you defined in __init__ and specify the        #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># connectivity of those layers in forward()                            #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 定义前向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># 第一层卷积 + ReLU</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># 第二层卷积 + ReLU</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 展平</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">scores</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">test_ThreeLayerConvNet</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># minibatch size 64, image size [3, 32, 32]</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">ThreeLayerConvNet</span><span class="p">(</span><span class="n">in_channel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">channel_1</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">channel_2</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># you should see [64, 10]</span>
</span></span><span class="line"><span class="cl"><span class="n">test_ThreeLayerConvNet</span><span class="p">()</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-module-api-train-a-three-layer-convnet" class="heading-element"><span>TODO: Module API: Train a Three-Layer ConvNet</span>
  <a href="#todo-module-api-train-a-three-layer-convnet" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-3</span>
</span></span><span class="line"><span class="cl"><span class="n">channel_1</span> <span class="o">=</span> <span class="mi">32</span>
</span></span><span class="line"><span class="cl"><span class="n">channel_2</span> <span class="o">=</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Instantiate your ThreeLayerConvNet model and a corresponding optimizer #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 实例化三层卷积神经网络模型</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">ThreeLayerConvNet</span><span class="p">(</span><span class="n">in_channel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">channel_1</span><span class="o">=</span><span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="o">=</span><span class="n">channel_2</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义优化器</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_part34</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Iteration 0, loss = 2.8420
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 120 / 1000 correct (12.00)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 100, loss = 1.8090
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 336 / 1000 correct (33.60)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 200, loss = 1.8638
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 385 / 1000 correct (38.50)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 300, loss = 1.5297
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 402 / 1000 correct (40.20)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 400, loss = 1.5403
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 430 / 1000 correct (43.00)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 500, loss = 1.5430
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 450 / 1000 correct (45.00)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 600, loss = 1.5708
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Iteration 700, loss = 1.6809
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 466 / 1000 correct (46.60)</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-sequential-api-three-layer-convnet" class="heading-element"><span>TODO: Sequential API: Three-Layer ConvNet</span>
  <a href="#todo-sequential-api-three-layer-convnet" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><h5 id="顺序api三层卷积神经网络" class="heading-element"><span>顺序API：三层卷积神经网络</span>
  <a href="#%e9%a1%ba%e5%ba%8fapi%e4%b8%89%e5%b1%82%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h5><p>使用 <code>nn.Sequential</code> 来定义和训练一个三层卷积神经网络，其架构与我们在第三部分中使用的相同：</p>
<ol>
<li>卷积层（带偏置）使用32个5x5的滤波器，零填充为2</li>
<li>ReLU</li>
<li>卷积层（带偏置）使用16个3x3的滤波器，零填充为1</li>
<li>ReLU</li>
<li>全连接层（带偏置）计算10个类别的分数</li>
</ol>
<p>您可以使用默认的PyTorch权重初始化。</p>
<p>您应该使用带有Nesterov动量0.9的随机梯度下降来优化您的模型。</p>
<p>同样，您不需要调整任何超参数，但您应该在训练一个周期后看到准确率超过55%。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">channel_1</span> <span class="o">=</span> <span class="mi">32</span>
</span></span><span class="line"><span class="cl"><span class="n">channel_2</span> <span class="o">=</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl"><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO: Rewrite the 3-layer ConvNet with bias from Part III with the           #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Sequential API.                                                              #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">channel_1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>  <span class="c1"># 第一层卷积</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>                                           <span class="c1"># ReLU 激活</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channel_1</span><span class="p">,</span> <span class="n">channel_2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># 第二层卷积</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>                                           <span class="c1"># ReLU 激活</span>
</span></span><span class="line"><span class="cl">    <span class="n">Flatten</span><span class="p">(),</span>                                          <span class="c1"># 展平</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channel_2</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                 <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_part34</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Iteration 0, loss = 2.2989
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 116 / 1000 correct (11.60)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 100, loss = 1.6947
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 457 / 1000 correct (45.70)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 200, loss = 1.7063
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 476 / 1000 correct (47.60)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 300, loss = 1.1563
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 495 / 1000 correct (49.50)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 400, loss = 1.3090
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 553 / 1000 correct (55.30)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 500, loss = 1.0429
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 539 / 1000 correct (53.90)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Iteration 600, loss = 1.4302
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Iteration 700, loss = 1.3101
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 579 / 1000 correct (57.90)</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-part-v-cifar-10-open-ended-challenge" class="heading-element"><span>TODO: Part V. CIFAR-10 open-ended challenge</span>
  <a href="#todo-part-v-cifar-10-open-ended-challenge" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>在这一部分中，您可以尝试在 CIFAR-10 上实验任何卷积神经网络架构。</p>
<p>现在轮到您实验架构、超参数、损失函数和优化器，以训练一个在 CIFAR-10 <strong>验证集</strong>上达到 <strong>至少 70%</strong> 准确率的模型，训练时间不超过 10 个周期。您可以使用上面的 check_accuracy 和 train 函数。您可以使用 <code>nn.Module</code> 或 <code>nn.Sequential</code> API。</p>
<p>使用 ResNet 模型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #         </span>
</span></span><span class="line"><span class="cl"><span class="c1"># Experiment with any architectures, optimizers, and hyperparameters.          #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                              #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Note that you can use the check_accuracy function to evaluate on either      #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the test set or the validation set, by passing either loader_test or         #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># loader_val as the second argument to check_accuracy. You should not touch    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the test set until you have finished your architecture and  hyperparameter   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tuning, and only run the test set once at the end to report a final value.   #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># TODO:                                                                        #         </span>
</span></span><span class="line"><span class="cl"><span class="c1"># Experiment with any architectures, optimizers, and hyperparameters.          #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                                                              #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Note that you can use the check_accuracy function to evaluate on either      #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the test set or the validation set, by passing either loader_test or         #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># loader_val as the second argument to check_accuracy. You should not touch    #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the test set until you have finished your architecture and  hyperparameter   #</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tuning, and only run the test set once at the end to report a final value.   #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用ResNet-18架构</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 修改第一层卷积（原ImageNet输入为3通道224x224，CIFAR-10为3通道32x32）</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 移除原最大池化层（因图像尺寸较小）</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 修改全连接层输出为10类</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用Adam优化器，加入学习率调度</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl"><span class="c1">################################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># You should get at least 70% accuracy.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># You may modify the number of epochs to any number below 15.</span>
</span></span><span class="line"><span class="cl"><span class="n">train_part34</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span></span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight" title="Output"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Iteration 300, loss = 0.0958
</span></span><span class="line"><span class="cl">Checking accuracy on validation set
</span></span><span class="line"><span class="cl">Got 803 / 1000 correct (80.30)</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="assignment-3" class="heading-element"><span>Assignment 3</span>
  <a href="#assignment-3" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><h3 id="q1-image-captioning-with-vanilla-rnns" class="heading-element"><span>Q1: Image Captioning with Vanilla RNNs</span>
  <a href="#q1-image-captioning-with-vanilla-rnns" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>RNN 结构模型图如下：</p>
<p><img loading="lazy" src="/image/ML/CS231n/rnn1.webp" alt="/image/ML/CS231n/rnn1.webp" srcset="/image/ML/CS231n/rnn1.webp?size=small, /image/ML/CS231n/rnn1.webp?size=medium 1.5x, /image/ML/CS231n/rnn1.webp?size=large 2x" data-title="/image/ML/CS231n/rnn1.webp" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>特点就是可以保留历史信息，其中 $x$ 可以代表一个单词向量，$x_t$ 是第 $t$ 个单词向量 (也叫做 $t$ 时刻)，图中的 $W, U, V$ 是每个时刻共用的参数。
模型公式：</p>
<p>$$
o_t = g(V \cdot s_t) \\
s_t = f(U \cdot x_t + W \cdot s_{t - 1})
$$</p>
<p>在 CS231n 中，基本模型如下：</p>
<p><img loading="lazy" src="/image/ML/CS231n/rnn2.png" alt="/image/ML/CS231n/rnn2.png" srcset="/image/ML/CS231n/rnn2.png?size=small, /image/ML/CS231n/rnn2.png?size=medium 1.5x, /image/ML/CS231n/rnn2.png?size=large 2x" data-title="/image/ML/CS231n/rnn2.png" style="background: url(/images/loading.min.svg) no-repeat center;" onload="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}this.dataset.lazyloaded='';" onerror="this.title=this.dataset.title;for(const i of ['style', 'data-title','onerror','onload']){this.removeAttribute(i);}"/></p>
<p>先从单步模型看起：</p>
<h4 id="todo-rnn_step_forward" class="heading-element"><span>TODO: rnn_step_forward</span>
  <a href="#todo-rnn_step_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>单步的前向传播直接套用公式，这里激活函数一般是 $\tanh$</p>
<p>$$
\text{next}_h = \tanh(\text{prev}_h \cdot W_h + x \cdot W_x + b)
$$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rnn_step_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Run the forward pass for a single timestep of a vanilla RNN using a tanh activation function.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The input data has dimension D, the hidden state has dimension H,
</span></span></span><span class="line"><span class="cl"><span class="s2">    and the minibatch is of size N.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data for this timestep, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - prev_h: Hidden state from previous timestep, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - b: Biases of shape (H,)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - next_h: Next hidden state, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Tuple of values needed for the backward pass.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_h</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement a single forward step for the vanilla RNN. Store the next  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># hidden state and any values you need for the backward pass in the next_h   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># and cache variables respectively.                                          #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">next_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">prev_h</span><span class="p">,</span> <span class="n">Wh</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Wx</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">next_h</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">next_h</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-rnn_step_backward" class="heading-element"><span>TODO: rnn_step_backward</span>
  <a href="#todo-rnn_step_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>反向传播我们先来看一下 $\mathrm{d}x$</p>
<p>根据公式 $\text{next}_h = \tanh(\text{prev}_h \cdot W_h + x \cdot W_x + b)$</p>
<p>我们先设 $z = \text{prev}_h \cdot W_h + x \cdot W_x + b$</p>
<p>也就是 $\text{next}_h = \tanh(z)$</p>
<p>那么 $x$ 的梯度就是
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{ \partial z} \cdot \frac{\partial z}{ \partial x} = \frac{\partial L}{ \partial z} \cdot W_x ^ {\top}
$$</p>
<p>那来看一下 $\dfrac{\partial L}{\partial z}$ 这是什么：</p>
<p>$$
\frac{\partial L}{\partial z} = \frac{\partial L}{\partial \text{next}_h} \cdot \frac{\partial \text{next}_h}{\partial z} = \mathrm{d} \text{next}_h \cdot \frac{\partial \text{next}_h}{\partial z}
$$</p>
<p>那么 $\dfrac{\partial \text{next}_h}{\partial z}$ 就是对 $\tanh(z)$ 求导，导数为 $1 - \tanh^2(z)$</p>
<p>所以 $\dfrac{\partial L}{\partial z} = \mathrm{d} \text{next}_h (1 - \tanh^2(z))$，把这个记为 $\mathrm{d}\tanh$</p>
<p>所以 $\mathrm{d}x = \mathrm{d}\tanh \cdot W_x ^ {\top}$</p>
<p>其他参数都是同理的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rnn_step_backward</span><span class="p">(</span><span class="n">dnext_h</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Backward pass for a single timestep of a vanilla RNN.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Cache object from the forward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradients of input data, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dprev_h: Gradients of previous hidden state, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dWx: Gradients of input-to-hidden weights, of shape (D, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - db: Gradients of bias vector, of shape (H,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for a single step of a vanilla RNN.      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: For the tanh function, you can compute the local derivative in terms #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># of the output value from tanh.                                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">next_h</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">dtanh</span> <span class="o">=</span> <span class="n">dnext_h</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">next_h</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dtanh</span><span class="p">,</span> <span class="n">Wx</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dprev_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dtanh</span><span class="p">,</span> <span class="n">Wh</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dWx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dtanh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dWh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">prev_h</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dtanh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dtanh</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-rnn_forward" class="heading-element"><span>TODO: rnn_forward</span>
  <a href="#todo-rnn_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>整体的前向传播只需要把 $\text{prev}_h$ 代入就可以了，注意保留过程。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rnn_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Run a vanilla RNN forward on an entire sequence of data.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    We assume an input sequence composed of T vectors, each of dimension D. The RNN uses a hidden
</span></span></span><span class="line"><span class="cl"><span class="s2">    size of H, and we work over a minibatch containing N sequences. After running the RNN forward,
</span></span></span><span class="line"><span class="cl"><span class="s2">    we return the hidden states for all timesteps.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Input data for the entire timeseries, of shape (N, T, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - h0: Initial hidden state, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - b: Biases of shape (H,)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - h: Hidden states for the entire timeseries, of shape (N, T, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values needed in the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement forward pass for a vanilla RNN running on a sequence of    #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input data. You should use the rnn_step_forward function that you defined  #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># above. You can use a for loop to help compute the forward pass.            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">H</span> <span class="o">=</span> <span class="n">h0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">prev_h</span> <span class="o">=</span> <span class="n">h0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">prev_h</span><span class="p">,</span> <span class="n">cache_t</span> <span class="o">=</span> <span class="n">rnn_step_forward</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:],</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">h</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">prev_h</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-rnn_backward" class="heading-element"><span>TODO: rnn_backward</span>
  <a href="#todo-rnn_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>整体反向传播需要注意的第一个是逆序的，还要注意梯度传播有两个方向：一个是当前传播下来的梯度，另一个是由下一个时间节点传播下来的梯度，加起来就好。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">rnn_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Compute the backward pass for a vanilla RNN over an entire sequence of data.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dh: Upstream gradients of all hidden states, of shape (N, T, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    NOTE: &#39;dh&#39; contains the upstream gradients produced by the 
</span></span></span><span class="line"><span class="cl"><span class="s2">    individual loss functions at each timestep, *not* the gradients
</span></span></span><span class="line"><span class="cl"><span class="s2">    being passed between timesteps (which you&#39;ll have to compute yourself
</span></span></span><span class="line"><span class="cl"><span class="s2">    by calling rnn_step_backward in a loop).
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dx: Gradient of inputs, of shape (N, T, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dh0: Gradient of initial hidden state, of shape (N, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - db: Gradient of biases, of shape (H,)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for a vanilla RNN running an entire      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># sequence of data. You should use the rnn_step_backward function that you   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># defined above. You can use a for loop to help compute the backward pass.   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="n">dh</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">D</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dh0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dWx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dWh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">H</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dprev_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">dx_t</span><span class="p">,</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">dWx_t</span><span class="p">,</span> <span class="n">dWh_t</span><span class="p">,</span> <span class="n">db_t</span> <span class="o">=</span> <span class="n">rnn_step_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">cache</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">dx</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">dx_t</span>
</span></span><span class="line"><span class="cl">        <span class="n">dWx</span> <span class="o">+=</span> <span class="n">dWx_t</span>
</span></span><span class="line"><span class="cl">        <span class="n">dWh</span> <span class="o">+=</span> <span class="n">dWh_t</span>
</span></span><span class="line"><span class="cl">        <span class="n">db</span> <span class="o">+=</span> <span class="n">db_t</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dh0</span> <span class="o">=</span> <span class="n">dprev_h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-word_embedding_forward" class="heading-element"><span>TODO: word_embedding_forward</span>
  <a href="#todo-word_embedding_forward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>word embedding 就是词嵌入，简单来说就是把单词表示成向量的形式，假设用最简单的独热编码 (one-hot)，这个在之前 softmax 里提到过，就是每个不同种类的单词构成的单位矩阵。</p>
<p>在这里，单词矩阵 $W$ 是一个 $V \times D$ 的矩阵，其中 $V$ 代表单词个数，$D$ 代表维度。$X$ 是一个 $N \times T$ 的矩阵，$N$ 代表 batch，$T$ 代表这句话有 $T$ 个词，$T$ 序列中的每个值是 $W$ 中的索引。</p>
<p>所以前向传播直接 W[x] 自动索引就行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">word_embedding_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Forward pass for word embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    We operate on minibatches of size N where
</span></span></span><span class="line"><span class="cl"><span class="s2">    each sequence has length T. We assume a vocabulary of V words, assigning each
</span></span></span><span class="line"><span class="cl"><span class="s2">    word to a vector of dimension D.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - x: Integer array of shape (N, T) giving indices of words. Each element idx
</span></span></span><span class="line"><span class="cl"><span class="s2">      of x muxt be in the range 0 &lt;= idx &lt; V.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - W: Weight matrix of shape (V, D) giving word vectors for all words.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - out: Array of shape (N, T, D) giving word vectors for all input words.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values needed for the backward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the forward pass for word embeddings.                      #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: This can be done in one line using NumPy&#39;s array indexing.           #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-word_embedding_backward" class="heading-element"><span>TODO: word_embedding_backward</span>
  <a href="#todo-word_embedding_backward" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><p>首先注意提示，先学习 np.add.at()</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建一个数组</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在指定索引处增加值</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 输出结果</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># 结果为 [2, 3, 5, 4]</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这个就是给数组一个列表，在指定索引处增加值。</p>
<p>dout 是大小 $N \times T \times D$ 的上游梯度，在 dW 矩阵上根据 $x$ 矩阵作为下标加上 dout 的值，因为 out 只依赖于 $W$ 在特定位置（即 $x$ 的元素所表示的 $W$ 的下标）的值， out 对 $W$ 求导之后系数是 $1$，所以只要在特定位置加上 dout 的值就行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">word_embedding_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Backward pass for word embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    We cannot back-propagate into the words
</span></span></span><span class="line"><span class="cl"><span class="s2">    since they are integers, so we only return gradient for the word embedding
</span></span></span><span class="line"><span class="cl"><span class="s2">    matrix.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    HINT: Look up the function np.add.at
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dout: Upstream gradients of shape (N, T, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    - cache: Values from the forward pass
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - dW: Gradient of word embedding matrix, of shape (V, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># TODO: Implement the backward pass for word embeddings.                     #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                                                                            #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Note that words can appear more than once in a sequence.                   #</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># HINT: Look up the function np.add.at                                       #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">cache</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">dW</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#                               END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">    <span class="c1">##############################################################################</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dW</span></span></span></code></pre></td></tr></table>
</div>
</div><h4 id="todo-captioningrnnloss-captioningrnnsample" class="heading-element"><span>TODO: CaptioningRNN.loss CaptioningRNN.sample</span>
  <a href="#todo-captioningrnnloss-captioningrnnsample" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">..rnn_layers</span> <span class="kn">import</span> <span class="o">*</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CaptioningRNN</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A CaptioningRNN produces captions from image features using a recurrent
</span></span></span><span class="line"><span class="cl"><span class="s2">    neural network.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    The RNN receives input vectors of size D, has a vocab size of V, works on
</span></span></span><span class="line"><span class="cl"><span class="s2">    sequences of length T, has an RNN hidden dimension of H, uses word vectors
</span></span></span><span class="line"><span class="cl"><span class="s2">    of dimension W, and operates on minibatches of size N.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Note that we don&#39;t use any regularization for the CaptioningRNN.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">word_to_idx</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">wordvec_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cell_type</span><span class="o">=</span><span class="s2">&#34;rnn&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Construct a new CaptioningRNN instance.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,
</span></span></span><span class="line"><span class="cl"><span class="s2">          and maps each string to a unique integer in the range [0, V).
</span></span></span><span class="line"><span class="cl"><span class="s2">        - input_dim: Dimension D of input image feature vectors.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - wordvec_dim: Dimension W of word vectors.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - hidden_dim: Dimension H for the hidden state of the RNN.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - cell_type: What type of RNN to use; either &#39;rnn&#39; or &#39;lstm&#39;.
</span></span></span><span class="line"><span class="cl"><span class="s2">        - dtype: numpy datatype to use; use float32 for training and float64 for
</span></span></span><span class="line"><span class="cl"><span class="s2">          numeric gradient checking.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">cell_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&#34;rnn&#34;</span><span class="p">,</span> <span class="s2">&#34;lstm&#34;</span><span class="p">}:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid cell_type &#34;</span><span class="si">%s</span><span class="s1">&#34;&#39;</span> <span class="o">%</span> <span class="n">cell_type</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">=</span> <span class="n">cell_type</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">word_to_idx</span> <span class="o">=</span> <span class="n">word_to_idx</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_to_idx</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_null</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="p">[</span><span class="s2">&#34;&lt;NULL&gt;&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_start</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;&lt;START&gt;&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_end</span> <span class="o">=</span> <span class="n">word_to_idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;&lt;END&gt;&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize word vectors</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_embed&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">wordvec_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_embed&#34;</span><span class="p">]</span> <span class="o">/=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize CNN -&gt; hidden state projection parameters</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_proj&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_proj&#34;</span><span class="p">]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_proj&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize parameters for the RNN</span>
</span></span><span class="line"><span class="cl">        <span class="n">dim_mul</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;lstm&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&#34;rnn&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}[</span><span class="n">cell_type</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">wordvec_dim</span><span class="p">,</span> <span class="n">dim_mul</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">wordvec_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim_mul</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dim_mul</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Initialize output to vocab weights</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_vocab&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_vocab&#34;</span><span class="p">]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_vocab&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Cast parameters to correct dtype</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">captions</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Compute training-time loss for the RNN. We input image features and
</span></span></span><span class="line"><span class="cl"><span class="s2">        ground-truth captions for those images, and use an RNN (or LSTM) to compute
</span></span></span><span class="line"><span class="cl"><span class="s2">        loss and gradients on all parameters.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - features: Input image features, of shape (N, D)
</span></span></span><span class="line"><span class="cl"><span class="s2">        - captions: Ground-truth captions; an integer array of shape (N, T + 1) where
</span></span></span><span class="line"><span class="cl"><span class="s2">          each element is in the range 0 &lt;= y[i, t] &lt; V
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns a tuple of:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - loss: Scalar loss
</span></span></span><span class="line"><span class="cl"><span class="s2">        - grads: Dictionary of gradients parallel to self.params
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Cut captions into two pieces: captions_in has everything but the last word</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># and will be input to the RNN; captions_out has everything but the first</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># word and this is what we will expect the RNN to generate. These are offset</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># by one relative to each other because the RNN should produce word (t+1)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># after receiving word t. The first element of captions_in will be the START</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># token, and the first element of captions_out will be the first word.</span>
</span></span><span class="line"><span class="cl">        <span class="n">captions_in</span> <span class="o">=</span> <span class="n">captions</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">captions_out</span> <span class="o">=</span> <span class="n">captions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># You&#39;ll need this</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">captions_out</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_null</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Weight and bias for the affine transform from image features to initial</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># hidden state</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_proj</span><span class="p">,</span> <span class="n">b_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_proj&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_proj&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Word embedding matrix</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_embed&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Input-to-hidden, hidden-to-hidden, and biases for the RNN</span>
</span></span><span class="line"><span class="cl">        <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Weight and bias for the hidden-to-vocab transformation.</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_vocab&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_vocab&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement the forward and backward passes for the CaptioningRNN.   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># In the forward pass you will need to do the following:                   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (1) Use an affine transformation to compute the initial hidden state     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     from the image features. This should produce an array of shape (N, H)#</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (2) Use a word embedding layer to transform the words in captions_in     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     from indices to vectors, giving an array of shape (N, T, W).         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     process the sequence of input word vectors and produce hidden state  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     vectors for all timesteps, producing an array of shape (N, T, H).    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (4) Use a (temporal) affine transformation to compute scores over the    #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     vocabulary at every timestep using the hidden states, giving an      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     array of shape (N, T, V).                                            #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     the points where the output word is &lt;NULL&gt; using the mask above.     #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Do not worry about regularizing the weights or their gradients!          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># In the backward pass you will need to compute the gradient of the loss   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with respect to all model parameters. Use the loss and grads variables   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># defined above to store loss and gradients; grads[k] should give the      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># gradients for self.params[k].                                            #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Note also that you are allowed to make use of functions from layers.py   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in your implementation, if needed.                                       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h0</span><span class="p">,</span> <span class="n">cache_affine</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">W_proj</span><span class="p">,</span> <span class="n">b_proj</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">word_vectors</span><span class="p">,</span> <span class="n">cache_embed</span> <span class="o">=</span> <span class="n">word_embedding_forward</span><span class="p">(</span><span class="n">captions_in</span><span class="p">,</span> <span class="n">W_embed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;rnn&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h</span><span class="p">,</span> <span class="n">cache_rnn</span> <span class="o">=</span> <span class="n">rnn_forward</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;lstm&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">h</span><span class="p">,</span> <span class="n">cache_lstm</span> <span class="o">=</span> <span class="n">lstm_forward</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid cell_type&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span><span class="p">,</span> <span class="n">cache_temporal_affine</span> <span class="o">=</span> <span class="n">temporal_affine_forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="p">,</span> <span class="n">dscores</span> <span class="o">=</span> <span class="n">temporal_softmax_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">captions_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">dh</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W_vocab&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b_vocab&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">temporal_affine_backward</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">cache_temporal_affine</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;rnn&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dword_vectors</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rnn_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">cache_rnn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;lstm&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">dword_vectors</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lstm_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">cache_lstm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid cell_type&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W_embed&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">word_embedding_backward</span><span class="p">(</span><span class="n">dword_vectors</span><span class="p">,</span> <span class="n">cache_embed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;W_proj&#34;</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;b_proj&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine_backward</span><span class="p">(</span><span class="n">dh0</span><span class="p">,</span> <span class="n">cache_affine</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Run a test-time forward pass for the model, sampling captions for input
</span></span></span><span class="line"><span class="cl"><span class="s2">        feature vectors.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        At each timestep, we embed the current word, pass it and the previous hidden
</span></span></span><span class="line"><span class="cl"><span class="s2">        state to the RNN to get the next hidden state, use the hidden state to get
</span></span></span><span class="line"><span class="cl"><span class="s2">        scores for all vocab words, and choose the word with the highest score as
</span></span></span><span class="line"><span class="cl"><span class="s2">        the next word. The initial hidden state is computed by applying an affine
</span></span></span><span class="line"><span class="cl"><span class="s2">        transform to the input image features, and the initial word is the &lt;START&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">        token.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        For LSTMs you will also have to keep track of the cell state; in that case
</span></span></span><span class="line"><span class="cl"><span class="s2">        the initial cell state should be zero.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Inputs:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - features: Array of input image features of shape (N, D).
</span></span></span><span class="line"><span class="cl"><span class="s2">        - max_length: Maximum length T of generated captions.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        - captions: Array of shape (N, max_length) giving sampled captions,
</span></span></span><span class="line"><span class="cl"><span class="s2">          where each element is an integer in the range [0, V). The first element
</span></span></span><span class="line"><span class="cl"><span class="s2">          of captions should be the first sampled word, not the &lt;START&gt; token.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">N</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">captions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_null</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Unpack parameters</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_proj</span><span class="p">,</span> <span class="n">b_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_proj&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_proj&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_embed&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wx&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;Wh&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;W_vocab&#34;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;b_vocab&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># TODO: Implement test-time sampling for the model. You will need to      #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># initialize the hidden state of the RNN by applying the learned affine   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># transform to the input image features. The first word that you feed to  #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the RNN should be the &lt;START&gt; token; its value is stored in the         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># variable self._start. At each timestep you will need to do to:          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (1) Embed the previous word using the learned word embeddings           #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (2) Make an RNN step using the previous hidden state and the embedded   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     current word to get the next hidden state.                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (3) Apply the learned affine transformation to the next hidden state to #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     get scores for all words in the vocabulary                          #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (4) Select the word with the highest score as the next word, writing it #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#     (the word index) to the appropriate slot in the captions variable   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># For simplicity, you do not need to stop generating after an &lt;END&gt; token #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is sampled, but you can if you want to.                                 #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># HINT: You will not be able to use the rnn_forward or lstm_forward       #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># functions; you&#39;ll need to call rnn_step_forward or lstm_step_forward in #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># a loop.                                                                 #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                                                                         #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># NOTE: we are still working over minibatches in this function. Also if   #</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># you are using an LSTM, initialize the first cell state to zeros.        #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">###########################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">h</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">W_proj</span><span class="p">,</span> <span class="n">b_proj</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;lstm&#34;</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">captions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">word_vectors</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">word_embedding_forward</span><span class="p">(</span><span class="n">captions</span><span class="p">[:,</span> <span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">W_embed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;rnn&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">h</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rnn_step_forward</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">==</span> <span class="s2">&#34;lstm&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lstm_step_forward</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid cell_type&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">affine_forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">captions</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#                             END OF YOUR CODE                             #</span>
</span></span><span class="line"><span class="cl">        <span class="c1">############################################################################</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">captions</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="参考" class="heading-element"><span>参考</span>
  <a href="#%e5%8f%82%e8%80%83" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p><a href="https://github.com/Divsigma/2020-cs213n/tree/master/cs231n"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Divsigma/2020-cs213n/tree/master/cs231n</a></p>
<p><a href="https://github.com/Na-moe/CS231n-2024/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Na-moe/CS231n-2024/tree/main</a></p>
<p><a href="https://github.com/Chia202/CS231n/tree/main"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/Chia202/CS231n/tree/main</a></p>
<p><a href="https://blog.csdn.net/leezed525/category_12388436.html"target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/leezed525/category_12388436.html</a></p>
</div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title="更新于 2025-05-08 03:37:34">更新于 2025-05-08&nbsp;</span>
      </div></div><div class="post-info-line">
        <div class="post-info-md"><span><a href="/posts/mlcs231n/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span></div>
        <div class="post-info-share">
          <span></span>
        </div>
      </div></div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw me-1" aria-hidden="true"></i><a href="/tags/knn/" class="post-tag" title="标签 - KNN">KNN</a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-tag" title="标签 - 神经网络">神经网络</a><a href="/tags/svm/" class="post-tag" title="标签 - SVM">SVM</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div><div class="post-nav"><a href="/posts/mlknn/" class="post-nav-item" rel="prev" title="KNN"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>KNN</a><a href="/posts/misc2024%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" class="post-nav-item" rel="next" title="2024 年度总结">2024 年度总结<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article>

  <aside class="toc" id="toc-auto" aria-label="目录"><h2 class="toc-title">目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.139.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.3.15">FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2024 - 2025</span><span class="author" itemprop="copyrightHolder">
              <a href="https://github.com/messywind"target="_blank" rel="external nofollow noopener noreferrer">凌乱之风</a></span></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">该网站在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="preload" href="/lib/katex/katex.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/katex/katex.min.css"></noscript><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/fuse/fuse.min.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":11},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"distance":100,"findAllMatches":false,"fuseIndexURL":"/search.json","highlightTag":"em","ignoreFieldNorm":false,"ignoreLocation":false,"isCaseSensitive":false,"location":0,"maxResultLength":10,"minMatchCharLength":2,"noResultsFound":"没有找到结果","snippetLength":30,"threshold":0.3,"type":"fuse","useExtendedSearch":false},"version":"v0.3.15"};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>

# 逻辑回归

## 分类

有些数据例如：客户是否会购买某个商品，他的结果只有 0 与 1，如果使用线性回归，效果会非常差。所以我们考虑引入新的模型来拟合这些数据。

## sigmoid 函数

逻辑回归的拟合函数是 sigmoid 函数：$f(z) = \dfrac{1}{1&#43;e^{-z}}$

![1](/image/ML/1.png)

通过函数图像的观察，我们可以发现 $\lim\limits_{x \rightarrow \infty} f(x) = 1, \lim\limits_{x \rightarrow -\infty} f(x) = 0, f(0) = \dfrac{1}{2}$

这样就很巧妙又很光滑的(指函数图像)建立了一个 0/1 的函数模型。

他表示这个事件预测的**概率**，如果 $\dfrac{1}{2} \le f(x)$，则事件预测发生，相反 $f(x) &lt; \dfrac{1}{2}$，事件预测不发生。

## 决策边界

已经有了假设函数了，现在考虑什么时候将某个样本预测为正类，什么时候预测为负类。也就是说用一个方程来把数据划分为两类，在一侧是一类，在另一侧是另一类。

### 线性决策边界

![1](/image/ML/2.png)

如上图所示，此线性数据的决策边界为 $x_1 &#43; x_2 - 3 = 0$，如果 $x_1 &#43; x_2 - 3 &gt; 0$ 时，就会在这条直线的上方判定为属于 1 类，那 sigmoid 函数值就会大于等于 $0.5$，反之。

### 非线性决策边界

![1](/image/ML/3.png)

这里的决策边界为 $x_1^2 &#43; x_2^2 - 1 = 0$，如果在圆内是一类，在圆外是另一类。

## 损失函数

如果按照线性回归的计算方式，直接用 y 和 sigmoid 函数做差，会导致损失函数不是凸函数，不容易优化，容易陷入局部最小值。所以逻辑回归要用对数函数来做损失函数。
$$
J(\theta)=-\frac{1}{m}\sum_{i = 1} ^ {n}(y_i\log(h(x_i))&#43;(1-y_i)\log(1-h(x_i)))
$$
对于梯度下降，也是 $\theta_i - \alpha \dfrac{\partial J}{\partial\theta_i}$

### 多元分类

![1](/image/ML/4.webp)

假设我们的训练集中有三种物品，我们可以训练三个分类器如上图，分别将其中的一类当正类，其他都当负类，然后进行一次二元分类。计算完三个分类器后，在预测时，向三个分类器中输入一个数据，将会分别返回三个概率，选择概率最大的那个当做预测类别。



---

> 作者: [凌乱之风](https://github.com/messywind)  
> URL: https://blog.messywind.top/posts/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/  

